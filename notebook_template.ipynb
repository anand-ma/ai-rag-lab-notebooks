{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mongodb-developer/ai-agents-lab-notebooks/blob/main/notebook_template.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Lab Documentation and Solutions](https://img.shields.io/badge/Lab%20Documentation%20and%20Solutions-purple)](https://mongodb-developer.github.io/rag-lab/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Install libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "! pip install -qU pymongo langchain langchain-community langchain-mongodb fireworks-ai bs4 tiktoken sentence_transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Setup prerequisites\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "MONGODB_URI = \"<CODE_BLOCK_1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"FIREWORKS_API_KEY\"] = \"CODE_BLOCK_2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Prepare the dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\n",
    "    [\n",
    "        \"https://www.mongodb.com/developer/products/atlas/choose-embedding-model-rag/\",\n",
    "        \"https://www.mongodb.com/developer/products/atlas/evaluate-llm-applications-rag/\",\n",
    "        \"https://www.mongodb.com/developer/products/atlas/choosing-chunking-strategy-rag/\",\n",
    "        \"https://www.mongodb.com/developer/products/atlas/gemma-mongodb-huggingface-rag/\",\n",
    "    ]\n",
    ")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'https://www.mongodb.com/developer/products/atlas/choose-embedding-model-rag/', 'title': 'How to Choose the Right Embedding Model for Your LLM Application | MongoDB', 'description': 'In this tutorial, we will see why embeddings are important for RAG, and how to choose the right embedding model for your RAG application.', 'language': 'en'}, page_content='How to Choose the Right Embedding Model for Your LLM Application | MongoDBBlogAtlas Vector Search voted most loved vector database in 2024 Retool State of AI reportLearn more\\xa0>>Developer Articles & TopicsGeneral InformationDocumentationDeveloper Articles & TopicsCommunity ForumsBlogUniversityProductsPlatformAtlasBuild on a developer data platformPlatform ServicesDatabaseDeploy a multi-cloud databaseSearchDeliver engaging search experiencesVector SearchDesign intelligent apps with GenAIStream ProcessingUnify data in motion and data at restToolsCompassWork with MongoDB data in a GUIIntegrationsIntegrations with third-party servicesRelational MigratorMigrate to MongoDB with confidenceSelf ManagedEnterprise AdvancedRun and manage MongoDB yourselfCommunity EditionDevelop locally with MongoDBBuild with MongoDB AtlasGet started for free in minutesSign UpTest Enterprise AdvancedDevelop with MongoDB on-premisesDownloadTry Community EditionExplore the latest version of MongoDBDownloadResourcesDocumentationAtlas DocumentationGet started using AtlasServer DocumentationLearn to use MongoDBStart With GuidesGet step-by-step guidance for key tasks Tools and ConnectorsLearn how to connect to MongoDBMongoDB DriversUse drivers and libraries for MongoDBAI Resources HubGet help building the next big thing in AI with MongoDBarrow-rightConnectDeveloper CenterExplore a wide range of developer resourcesCommunityJoin a global community of developersCourses and CertificationLearn for free from MongoDBWebinars and EventsFind a webinar or event near youSolutionsUse casesArtificial IntelligenceEdge ComputingInternet of ThingsMobilePaymentsServerless DevelopmentIndustriesFinancial ServicesTelecommunicationsHealthcareRetailPublic SectorManufacturingSolutions LibraryOrganized and tailored solutions to kick-start projectsarrow-rightDeveloper Data PlatformAccelerate innovation at scaleLearn morearrow-rightStartups and AI InnovatorsFor world-changing ideas and AI pioneersLearn morearrow-rightCustomer Case StudiesHear directly from our usersSee Storiesarrow-rightCompanyCareersStart your next adventureBlogRead articles and announcementsNewsroomRead press releases and news storiesPartnersLearn about our partner ecosystemLeadershipMeet our executive teamCompanyLearn more about who we areContact UsReach out to MongoDBLet’s chatarrow-rightInvestorsVisit our investor portalLearn morearrow-rightPricingSupportSign InTry Freemenu-verticalMongoDB Developerchevron-downTopicsLanguagesplusTechnologiesplusProductsplusExpertise LevelsplusAll TopicsDocumentationArticlesTutorialsEventsCode ExamplesPodcastsMongoDB TVMongoDB DeveloperTopicschevron-downDocumentationArticlesTutorialsEventsCode ExamplesPodcastsMongoDB TVcloseAtlasplusSign in to follow topicsArticlesCode ExamplesDocumentationexternalEventsNews & AnnouncementsPodcastsQuickstartsTutorialsVideosMongoDB Developer Centerchevron-rightDeveloper Topicschevron-rightProductschevron-rightAtlaschevron-rightTutorialsHow to Choose the Right Embedding Model for Your LLM ApplicationApoorva Joshi16 min read • Published Jun 17, 2024 • Updated Jun 17, 2024AIPythonAtlasRate this tutorialIf you are building Generative AI (GenAI) applications in 2024, you’ve probably heard the term “embeddings” a few times by now and are seeing new embedding models hit the shelf every week. So why do so many people suddenly care about embeddings, a concept that has existed since the 1950s? And if embeddings are so important and you must use them, how do you choose among the vast number of options out there?This tutorial will cover the following:What are embeddings?Importance of embeddings in RAG applicationsHow to choose the right embedding model for your RAG applicationEvaluating embedding modelsThis tutorial is Part 1 of a multi-part series on Retrieval Augmented Generation (RAG), where we start with the fundamentals of building a RAG application, and work our way to more advanced techniques for RAG. The series will cover the following:Part 1: How to Choose the Right Embedding Model for Your LLM ApplicationPart 2: How to Evaluate Your LLM ApplicationPart 3: How to Choose the Right Chunking Strategy for Your LLM ApplicationPart 4: Improving RAG using metadata extraction and filteringWhat are embeddings and embedding models?An embedding is an array of numbers (a vector) representing a piece of information, such as text, images, audio, video, etc. Together, these numbers capture semantics and other important features of the data. The immediate consequence of doing this is that semantically similar entities map close to each other while dissimilar entities map farther apart in the vector space. For clarity, see the image below for a depiction of a high-dimensional vector space:In the context of natural language processing (NLP), embedding models are algorithms designed to learn and generate embeddings for a given piece of information. In today’s AI applications, embeddings are typically created using large language models (LLMs) that are trained on a massive corpus of data and use cutting-edge algorithms to learn complex semantic relationships in the data.What is RAG (briefly)Retrieval Augmented Generation, as the name suggests, aims to improve the quality of pre-trained LLM generation using data retrieved from a knowledge base. The success of RAG lies in retrieving the most relevant results from the knowledge base. This is where embeddings come into the picture. A RAG pipeline looks something like this:In the above pipeline, we see a common approach used for retrieval in GenAI applications — i.e., semantic search. In this technique, an embedding model is used to create vector representations of the user query and of information in the knowledge base. This way, given a user query and its embedding, we can retrieve the most relevant source documents from the knowledge base based on how similar their embeddings are to the query embedding. The retrieved documents, user query, and any user prompts are then passed as context to an LLM, to generate an answer to the user’s question.Choosing the right embedding model for your RAG applicationAs we have seen above, embeddings are central to RAG. But with so many models out there, how do we choose the best one for our use case?A good place to start when looking for embedding models to use is the MTEB Leaderboard on Hugging Face. It is the most up-to-date list of proprietary and open-source text embedding models, accompanied by statistics on how each model performs on various embedding tasks such as retrieval, summarization, etc.Evaluations of this magnitude for multimodal models are just emerging (see the MME benchmark) so we will only focus on text embedding models for this tutorial. However, all the guidance here on choosing an embedding model also applies to multimodal models.Benchmarks are a good place to begin but bear in mind that these results are self-reported and have been benchmarked on datasets that might not accurately represent the data you are dealing with. It is also possible that some models may include the MTEB datasets in their training data since they are publicly available. So even if you choose a model based on benchmark results, we recommend evaluating it on your dataset. We will see how to do this later in the tutorial, but first, let’s take a closer look at the leaderboard.Here’s a snapshot of the top 10 models on the leaderboard currently:Let’s look at the Overall tab since it provides a comprehensive summary of each model. However, note that we have sorted the leaderboard by the Retrieval Average column. This is because RAG is a retrieval\\xa0task and we want to see the best retrieval models at the top. We will ignore columns corresponding to other tasks, and focus on the following columns:Retrieval Average: Represents average Normalized Discounted Cumulative Gain (NDCG) @ 10 across several datasets. NDCG is a common metric to measure the performance of retrieval systems. A higher NDCG indicates a model that is better at ranking relevant items higher in the list of retrieved results.\\xa0Model Size: Size of the model (in GB). It gives an idea of the computational resources required to run the model. While retrieval performance scales with model size, it is important to note that model size also has a direct impact on latency. The latency-performance trade-off becomes especially important in a production setup.\\xa0\\xa0Max Tokens: Number of tokens that can be compressed into a single embedding. You typically don’t want to put more than a single paragraph of text (~100 tokens) into a single embedding. So even models with max tokens of 512 should be more than enough.Embedding Dimensions: Length of the embedding vector. Smaller embeddings offer faster inference and are more storage-efficient, while more dimensions can capture nuanced details and relationships in the data. Ultimately, we want a good trade-off between capturing the complexity of data and operational efficiency.The top 10 models on the leaderboard contain a mix of small vs large and proprietary vs open-source models. Let’s compare some of these to find the best embedding model for our dataset.Before we beginHere are some things to note about our evaluation experiment.DatasetMongoDB’s cosmopedia-wikihow-chunked dataset is available on Hugging Face, which consists of prechunked WikiHow-style articles.Models evaluatedvoyage-lite-02-instruct: A proprietary embedding model from VoyageAItext-embedding-3-large: One of OpenAI’s latest proprietary embedding modelsUAE-Large-V1: A small-ish (335M parameters) open-source embedding modelWe also attempted to evaluate SFR-Embedding-Mistral, currently the #1 model on the MTEB leaderboard, but the hardware below was not sufficient to run this model. This model and other 14+ GB models on the leaderboard will likely require a/multiple GPU(s) with at least 32 GB of total memory, which means higher costs and/or getting into distributed inference. While we haven’t evaluated this model in our experiment, this is already a good data point when thinking about cost and resources.Evaluation metricsWe used the following metrics to evaluate embedding performance:Embedding latency: Time taken to create embeddingsRetrieval quality: Relevance of retrieved documents to the user queryHardware used1 NVIDIA T4 GPU, 16GB MemoryWhere’s the code?Evaluation notebooks for each of the above models are available:voyage-lite-02-instructtext-embedding-3-largeUAE-Large-V1To run a notebook, click on the Open in Colab shield at the top of the notebook. The notebook will open in Google Colaboratory.Click the Connect button on the top right corner to connect to a hosted runtime environment.Once connected, you can also change the runtime type to use the T4 GPUs available for free on Google Colab.Step 1: Install the required librariesThe libraries required for each model differ slightly, but the common ones are as follows:datasets: Python library to get access to datasets available on Hugging Face Hubsentence-transformers: Framework for working with text and image embeddingsnumpy: Python library that provides tools to perform mathematical operations on arrayspandas: Python library for data analysis, exploration, and manipulationtdqm: Python module to show a progress meter for loopsCode Snippet! pip install -qU datasets sentence-transformers numpy pandas tqdmAdditionally for Voyage AI:\\nvoyageai: Python library to interact with OpenAI APIsCode Snippet! pip install -qU voyageaiAdditionally for OpenAI:\\nopenai: Python library to interact with OpenAI APIsCode Snippet! pip install -qU openaiAdditionally for UAE:\\ntransformers: Python library that provides APIs to interact with pre-trained models available on Hugging FaceCode Snippet! pip install -qU transformersStep 2: Setup pre-requisitesOpenAI and Voyage AI models are available via APIs. So you’ll need to obtain API keys and make them available to the respective clients.Code Snippetimport os\\nimport getpassInitialize Voyage AI client:Code Snippetimport voyageai\\nVOYAGE_API_KEY = getpass.getpass(\"Voyage API Key:\")\\nvoyage_client = voyageai.Client(api_key=VOYAGE_API_KEY)Initialize OpenAI client:Code Snippetfrom openai import OpenAI\\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\\nopenai_client = OpenAI()Step 3: Download the evaluation datasetAs mentioned previously, we will use MongoDB’s cosmopedia-wikihow-chunked dataset. The dataset is quite large (1M+ documents). So we will stream it and grab the first 25k records, instead of downloading the entire dataset to disk.Code Snippetfrom datasets import load_dataset\\nimport pandas as pd\\n\\n# Use streaming=True to load the dataset without downloading it fully\\ndata = load_dataset(\"MongoDB/cosmopedia-wikihow-chunked\", split=\"train\", streaming=True)\\n# Get first 25k records from the dataset\\ndata_head = data.take(25000)\\ndf = pd.DataFrame(data_head)\\n\\n# Use this if you want the full dataset\\n# data = load_dataset(\"MongoDB/cosmopedia-wikihow-chunked\", split=\"train\")\\n# df = pd.DataFrame(data)Step 4: Data analysisNow that we have our dataset, let’s perform some simple data analysis and run some sanity checks on our data to ensure that we don’t see any obvious errors:Code Snippet# Ensuring length of dataset is what we expect i.e. 25k\\nlen(df)\\n\\n# Previewing the contents of the data\\ndf.head()\\n\\n# Only keep records where the text field is not null\\ndf = df[df[\"text\"].notna()]\\n\\n# Number of unique documents in the dataset\\ndf.doc_id.nunique()Step 5: Create embeddingsNow, let’s create embedding functions for each of our models.For voyage-lite-02-instruct:Code Snippetdef get_embeddings(docs: List[str], input_type: str, model:str=\"voyage-lite-02-instruct\") -> List[List[float]]:\\n    \"\"\"\\n    Get embeddings using the Voyage AI API.\\n\\n    Args:\\n        docs (List[str]): List of texts to embed\\n        input_type (str): Type of input to embed. Can be \"document\" or \"query\".\\n        model (str, optional): Model name. Defaults to \"voyage-lite-02-instruct\".\\n\\n    Returns:\\n        List[List[float]]: Array of embedddings\\n    \"\"\"\\n    response = voyage_client.embed(docs, model=model, input_type=input_type)\\n    return response.embeddingsThe embedding function above takes a list of texts (docs) and an input_type as arguments and returns a list of embeddings. The input_type can be document or query depending on whether we are embedding a list of documents or user queries. Voyage uses this value to prepend the inputs with special prompts to enhance retrieval quality.For text-embedding-3-large:Code Snippetdef get_embeddings(docs: List[str], model: str=\"text-embedding-3-large\") -> List[List[float]]:\\n    \"\"\"\\n    Generate embeddings using the OpenAI API.\\n\\n    Args:\\n        docs (List[str]): List of texts to embed\\n        model (str, optional): Model name. Defaults to \"text-embedding-3-large\".\\n\\n    Returns:\\n        List[float]: Array of embeddings\\n    \"\"\"\\n    # replace newlines, which can negatively affect performance.\\n    docs = [doc.replace(\"\\\\n\", \" \") for doc in docs]\\n    response = openai_client.embeddings.create(input=docs, model=model)\\n    response = [r.embedding for r in response.data]\\n    return responseThe embedding function for the OpenAI model is similar to the previous one, with some key differences — there is no input_type argument, and the API returns a list of embedding objects, which need to be parsed to get the final list of embeddings. A sample response from the API looks as follows:Code Snippet{\\n  \"data\": [\\n    {\\n      \"embedding\": [\\n        0.018429679796099663,\\n        -0.009457024745643139\\n    .\\n    .\\n    .\\n      ],\\n      \"index\": 0,\\n      \"object\": \"embedding\"\\n    }\\n  ],\\n  \"model\": \"text-embedding-3-large\",\\n  \"object\": \"list\",\\n  \"usage\": {\\n    \"prompt_tokens\": 183,\\n    \"total_tokens\": 183\\n  }\\n}For UAE-large-V1:Code Snippetfrom typing import List\\nfrom transformers import AutoModel, AutoTokenizer\\nimport torch\\n\\n# Instruction to append to user queries, to improve retrieval\\nRETRIEVAL_INSTRUCT = \"Represent this sentence for searching relevant passages:\"\\n\\n# Check if CUDA (GPU support) is available, and set the device accordingly\\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\\n# Load the UAE-Large-V1 model from the Hugging Face \\nmodel = AutoModel.from_pretrained(\\'WhereIsAI/UAE-Large-V1\\').to(device)\\n# Load the tokenizer associated with the UAE-Large-V1 model\\ntokenizer = AutoTokenizer.from_pretrained(\\'WhereIsAI/UAE-Large-V1\\')\\n\\n# Decorator to disable gradient calculations\\n@torch.no_grad()\\ndef get_embeddings(docs: List[str], input_type: str) -> List[List[float]]:\\n    \"\"\"\\n    Get embeddings using the UAE-Large-V1 model.\\n\\n    Args:\\n        docs (List[str]): List of texts to embed\\n        input_type (str): Type of input to embed. Can be \"document\" or \"query\".\\n\\n    Returns:\\n        List[List[float]]: Array of embedddings\\n    \"\"\"\\n    # Prepend retrieval instruction to queries\\n    if input_type == \"query\":\\n        docs = [\"{}{}\".format(RETRIEVAL_INSTRUCT, q) for q in docs]\\n    # Tokenize input texts\\n    inputs = tokenizer(docs, padding=True, truncation=True, return_tensors=\\'pt\\', max_length=512).to(device)\\n    # Pass tokenized inputs to the model, and obtain the last hidden state\\n    last_hidden_state = model(**inputs, return_dict=True).last_hidden_state\\n    # Extract embeddings from the last hidden state\\n    embeddings = last_hidden_state[:, 0]\\n    return embeddings.cpu().numpy()The UAE-Large-V1 model is an open-source model available on Hugging Face Model Hub. First, we will need to download the model and its tokenizer from Hugging Face. We do this using the Auto classes — namely, AutoModel and AutoTokenizer from the Transformers library  — which automatically infers the underlying model architecture, in this case, BERT. Next, we load the model onto the GPU using .to(device) since we have one available.The embedding function for the UAE model, much like the Voyage model, takes a list of texts (docs) and an input_type as arguments and returns a list of embeddings. A special prompt is prepended to queries for better retrieval as well.The input texts are first tokenized, which includes padding (for short sequences) and truncation (for long sequences)  as needed to ensure that the length of inputs to the model is consistent — 512, in this case, defined by the max_length parameter. The pt value for return_tensors indicates that the output of tokenization should be PyTorch tensors.The tokenized texts are then passed to the model for inference and the last hidden layer (last_hidden_state) is extracted. This layer is the model’s final learned representation of the entire input sequence. The final embedding, however, is extracted only from the first token, which is often a special token ([CLS] in BERT) in transformer-based models. This token serves as an aggregate representation of the entire sequence due to the self-attention mechanism in transformers, where the representation of each token in a sequence is influenced by all other tokens. Finally, we move the embeddings back to CPU using .cpu() and convert the PyTorch tensors to numpy arrays using .numpy().Step 6: EvaluationAs mentioned previously, we will evaluate the models based on embedding latency and retrieval quality.Measuring embedding latencyTo measure embedding latency, we will create a local vector store, which is essentially a list of embeddings for the entire dataset. Latency here is defined as the time it takes to create embeddings for the full dataset.Code Snippetfrom tqdm.auto import tqdm\\n\\n# Get all the texts in the dataset\\ntexts = df[\"text\"].tolist()\\n\\n# Number of samples in a single batch\\nbatch_size = 128\\n\\nembeddings = []\\n# Generate embeddings in batches\\nfor i in tqdm(range(0, len(texts), batch_size)):\\n    end = min(len(texts), i+batch_size)\\n    batch = texts[i:end]\\n    # Generate embeddings for current batch\\n    batch_embeddings = get_embeddings(batch)\\n    # Add to the list of embeddings\\n    embeddings.extend(batch_embeddings)We first create a list of all the texts we want to embed and set the batch size. The voyage-lite-02-instruct model has a batch size limit of 128, so we use the same for all models, for consistency. We iterate through the list of texts, grabbing batch_size number of samples in each iteration, getting embeddings for the batch, and adding them to our \"vector store\".The time taken to generate embeddings on our hardware looked as follows:ModelBatch SizeDimensionsTimetext-embedding-3-large12830724m 17svoyage-lite-02-instruct128102411m 14sUAE-large-V1128102419m 50sThe OpenAI model has the lowest latency. However, note that it also has three times the number of embedding dimensions compared to the other two models. OpenAI also charges by tokens used, so both the storage and inference costs of this model can add up over time. While the UAE model is the slowest of the lot (despite running inference on a GPU), there is room for optimizations such as quantization, distillation, etc., since it is open-source.Measuring retrieval qualityTo evaluate retrieval quality, we use a set of questions based on themes seen in our dataset. For real applications, however, you will want to curate a set of \"cannot-miss\" questions — i.e. questions that you would typically expect users to ask from your data. For this tutorial, we will qualitatively evaluate the relevance of retrieved documents as a measure of quality, but we will explore metrics and techniques for quantitative evaluations in a following tutorial.Here are the main themes (generated using ChatGPT) covered by the top three documents retrieved by each model for our queries:😐 denotes documents that we felt weren’t as relevant to the question. Sentences that contributed to this verdict have been highlighted in bold.Query: Give me some tips to improve my mental health.voyage-lite-02-instructtext-embedding-3-largeUAE-large-V1😐 Regularly reassess treatment efficacy and modify plans as needed. Track mood, thoughts, and behaviors; share updates with therapists and support network. Use a multifaceted approach to manage suicidal thoughts, involving resources, skills, and connections.Eat balanced, exercise, sleep well. Cultivate relationships, engage socially, set boundaries. Manage stress with effective coping mechanisms.Prioritizing mental health is essential, not selfish. Practice mindfulness through meditation, journaling, and activities like yoga. Adopt healthy habits for better mood, less anxiety, and improved cognition.Recognize early signs of stress, share concerns, and develop coping mechanisms. Combat isolation by nurturing relationships and engaging in social activities. Set boundaries, communicate openly, and seek professional help for social anxiety.Prioritizing mental health is essential, not selfish. Practice mindfulness through meditation, journaling, and activities like yoga. Adopt healthy habits for better mood, less anxiety, and improved cognition.Eat balanced, exercise regularly, get 7-9 hours of sleep. Cultivate positive relationships, nurture friendships, and seek new social opportunities. Manage stress with effective coping mechanisms.Prioritizing mental health is essential, not selfish. Practice mindfulness through meditation, journaling, and activities like yoga. Adopt healthy habits for better mood, less anxiety, and improved cognition.Acknowledging feelings is a step to address them. Engage in self-care activities to boost mood and health. Make self-care consistent for lasting benefits.😐 Taking care of your mental health is crucial for a fulfilling life, productivity, and strong relationships. Recognize the importance of mental health in all aspects of life. Managing mental health reduces the risk of severe psychological conditions.While the results cover similar themes, the Voyage AI model keys in heavily on seeking professional help, while the UAE model covers slightly more about why taking care of your mental health is important. The OpenAI model is the one that consistently retrieves documents that cover general tips for improving mental health.Query: Give me some tips for writing good code.voyage-lite-02-instructtext-embedding-3-largeUAE-large-V1Strive for clean, maintainable code with consistent conventions and version control. Utilize linters, static analyzers, and document work for quality and collaboration. Embrace best practices like SOLID and TDD to enhance design, scalability, and extensibility.Strive for clean, maintainable code with consistent conventions and version control. Utilize linters, static analyzers, and document work for quality and collaboration. Embrace best practices like SOLID and TDD to enhance design, scalability, and extensibility.Strive for clean, maintainable code with consistent conventions and version control. Utilize linters, static analyzers, and document work for quality and collaboration. Embrace best practices like SOLID and TDD to enhance design, scalability, and extensibility.😐 Code and test core gameplay mechanics like combat and quest systems; debug and refine for stability. Use modular coding, version control, and object-oriented principles for effective game development. Playtest frequently to find and fix bugs, seek feedback, and prioritize significant improvements.😐 Good programming needs dedication, persistence, and patience. Master core concepts, practice diligently, and engage with peers for improvement. Every expert was once a beginner—keep pushing forward.Read programming books for comprehensive coverage and deep insights, choosing beginner-friendly texts with pathways to proficiency. Combine reading with coding to reinforce learning; take notes on critical points and unfamiliar terms. Engage with exercises and challenges in books to apply concepts and enhance skills.😐 Monitor social media and newsletters for current software testing insights. Participate in networks and forums to exchange knowledge with experienced testers. Regularly update your testing tools and methods for enhanced efficiency.Apply learning by working on real projects, starting small and progressing to larger ones. Participate in open-source projects or develop your applications to enhance problem-solving. Master debugging with IDEs, print statements, and understanding common errors for productivity.😐 Programming is key in various industries, offering diverse opportunities. This guide covers programming fundamentals, best practices, and improvement strategies. Choose a programming language based on interests, goals, and resources.All the models seem to struggle a bit with this question. They all retrieve at least one document that is not as relevant to the question. However, it is interesting to note that all the models retrieve the same document as their number one.Query: What are some environment-friendly practices I can incorporate in everyday life?voyage-lite-02-instructtext-embedding-3-largeUAE-large-V1😐 Conserve resources by reducing waste, reusing, and recycling, reflecting Jawa culture\\'s values due to their planet\\'s limited resources. Monitor consumption (e.g., water, electricity), repair goods, and join local environmental efforts. Eco-friendly practices enhance personal and global well-being, aligning with Jawa values.Carry reusable bags for shopping, keeping extras in your car or bag. Choose sustainable alternatives like reusable water bottles and eco-friendly cutlery. Support businesses that minimize packaging and use biodegradable materials.Educate others on eco-friendly practices; lead by example. Host workshops or discussion groups on sustainable living.Embody respect for the planet; every effort counts towards improvement.Learn and follow local recycling rules, rinse containers, and educate others on proper recycling. Opt for green transportation like walking, cycling, or electric vehicles, and check for incentives. Upgrade to energy-efficient options like LED lights, seal drafts, and consider renewable energy sources.Opt for sustainable transportation, energy-efficient appliances, solar panels, and eat less meat to reduce emissions. Conserve water by fixing leaks, taking shorter showers, and using low-flow fixtures. Water conservation protects ecosystems, ensures food security, and reduces infrastructure stress.Carry reusable bags for shopping, keeping extras in your car or bag. Choose sustainable alternatives like reusable water bottles and eco-friendly cutlery. Support businesses that minimize packaging and use biodegradable materials.😐 Consistently implement these steps. Actively contribute to a cleaner, greener world. Support resilience for future generations.Conserve water with low-flow fixtures, fix leaks, and use rainwater for gardening. Compost kitchen scraps to reduce waste and enrich soil, avoid meat and dairy. Shop locally at farmers markets and CSAs to lower emissions and support local economies.Join local tree-planting events and volunteer at community gardens or restoration projects. Integrate native plants into landscaping to support pollinators and remove invasive species. Adopt eco-friendly transportation methods to decrease fossil fuel consumption.We see a similar trend with this query as with the previous two examples — the OpenAI model consistently retrieves documents that provide the most actionable tips, followed by the UAE model. The Voyage model provides more high-level advice.Overall, based on our preliminary evaluation, OpenAI’s text-embedding-3-large model comes out on top. When working with real-world systems, however, a more rigorous evaluation of a larger dataset is recommended. Also, operational costs become an important consideration. More on evaluation coming in Part 2 of this series!ConclusionIn this tutorial, we looked into how to choose the right model to embed data for RAG. The MTEB leaderboard is a good place to start, especially for text embedding models, but evaluating them on your data is important to find the best one for your RAG application. Storage and inference costs, embedding latency, and retrieval quality are all important parameters to consider while evaluating embedding models. The best model is typically one that offers the best trade-off across these dimensions.Now that you have a good understanding of embedding models, here are some resources to get started with building RAG applications using MongoDB:Using Latest OpenAI Embeddings in a RAG System With MongoDBBuilding a RAG System With Google’s Gemma, Hugging Face, and MongoDBHow to Build a RAG System With LlamaIndex, OpenAI, and MongoDBFollow along with these by creating a free MongoDB Atlas cluster and reach out to us in our Generative AI community forums if you have any questions.Top Comments in ForumsThere are no comments on this article yet.Start the ConversationRate this tutorialRelatedTutorialHow to Use Custom Archival Rules and Partitioning on MongoDB Atlas Online Archive May 31, 2023 | 5 min readTutorialHow to Deploy MongoDB Atlas with AWS CDK in TypeScript Jan 23, 2024 | 5 min readArticleHow to work with Johns Hopkins University COVID-19 Data in MongoDB Atlas Nov 16, 2023 | 8 min readArticleTriggers Treats and Tricks - Auto-Increment a Running ID Field Sep 23, 2022 | 3 min readRequest a TutorialTable of ContentsWhat are embeddings and embedding models?What is RAG (briefly)Choosing the right embedding model for your RAG applicationStep 1: Install the required librariesStep 2: Setup pre-requisitesStep 3: Download the evaluation datasetStep 4: Data analysisStep 5: Create embeddingsStep 6: EvaluationConclusionEnglishEnglishPortuguêsEspañol한국어日本語ItalianoDeutschFrançais简体中文© 2024 MongoDB, Inc.AboutCareersInvestor RelationsLegal NoticesPrivacy NoticesSecurity InformationTrust CenterSupportContact UsCustomer PortalAtlas StatusCustomer SupportSocialGitHubStack OverflowLinkedInYouTubeXTwitchFacebook© 2024 MongoDB, Inc.')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview a document\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How to Choose the Right Embedding Model for Your LLM Application | MongoDBBlogAtlas Vector Search voted most loved vector database in 2024 Retool State of AI reportLearn more\\xa0>>Developer Articles & TopicsGeneral InformationDocumentationDeveloper Articles & TopicsCommunity ForumsBlogUniversityProductsPlatformAtlasBuild on a developer data platformPlatform ServicesDatabaseDeploy a multi-cloud databaseSearchDeliver engaging search experiencesVector SearchDesign intelligent apps with GenAIStream ProcessingUnify data in motion and data at restToolsCompassWork with MongoDB data in a GUIIntegrationsIntegrations with third-party servicesRelational MigratorMigrate to MongoDB with confidenceSelf ManagedEnterprise AdvancedRun and manage MongoDB yourselfCommunity EditionDevelop locally with MongoDBBuild with MongoDB AtlasGet started for free in minutesSign UpTest Enterprise AdvancedDevelop with MongoDB on-premisesDownloadTry Community EditionExplore the latest version of MongoDBDownloadResourcesDocumentationAtlas DocumentationGet started using AtlasServer DocumentationLearn to use MongoDBStart With GuidesGet step-by-step guidance for key tasks Tools and ConnectorsLearn how to connect to MongoDBMongoDB DriversUse drivers and libraries for MongoDBAI Resources HubGet help building the next big thing in AI with MongoDBarrow-rightConnectDeveloper CenterExplore a wide range of developer resourcesCommunityJoin a global community of developersCourses and CertificationLearn for free from MongoDBWebinars and EventsFind a webinar or event near youSolutionsUse casesArtificial IntelligenceEdge ComputingInternet of ThingsMobilePaymentsServerless DevelopmentIndustriesFinancial ServicesTelecommunicationsHealthcareRetailPublic SectorManufacturingSolutions LibraryOrganized and tailored solutions to kick-start projectsarrow-rightDeveloper Data PlatformAccelerate innovation at scaleLearn morearrow-rightStartups and AI InnovatorsFor world-changing ideas and AI pioneersLearn morearrow-rightCustomer Case StudiesHear directly from our usersSee Storiesarrow-rightCompanyCareersStart your next adventureBlogRead articles and announcementsNewsroomRead press releases and news storiesPartnersLearn about our partner ecosystemLeadershipMeet our executive teamCompanyLearn more about who we areContact UsReach out to MongoDBLet’s chatarrow-rightInvestorsVisit our investor portalLearn morearrow-rightPricingSupportSign InTry Freemenu-verticalMongoDB Developerchevron-downTopicsLanguagesplusTechnologiesplusProductsplusExpertise LevelsplusAll TopicsDocumentationArticlesTutorialsEventsCode ExamplesPodcastsMongoDB TVMongoDB DeveloperTopicschevron-downDocumentationArticlesTutorialsEventsCode ExamplesPodcastsMongoDB TVcloseAtlasplusSign in to follow topicsArticlesCode ExamplesDocumentationexternalEventsNews & AnnouncementsPodcastsQuickstartsTutorialsVideosMongoDB Developer Centerchevron-rightDeveloper Topicschevron-rightProductschevron-rightAtlaschevron-rightTutorialsHow to Choose the Right Embedding Model for Your LLM ApplicationApoorva Joshi16 min read • Published Jun 17, 2024 • Updated Jun 17, 2024AIPythonAtlasRate this tutorialIf you are building Generative AI (GenAI) applications in 2024, you’ve probably heard the term “embeddings” a few times by now and are seeing new embedding models hit the shelf every week. So why do so many people suddenly care about embeddings, a concept that has existed since the 1950s? And if embeddings are so important and you must use them, how do you choose among the vast number of options out there?This tutorial will cover the following:What are embeddings?Importance of embeddings in RAG applicationsHow to choose the right embedding model for your RAG applicationEvaluating embedding modelsThis tutorial is Part 1 of a multi-part series on Retrieval Augmented Generation (RAG), where we start with the fundamentals of building a RAG application, and work our way to more advanced techniques for RAG. The series will cover the following:Part 1: How to Choose the Right Embedding Model for Your LLM ApplicationPart 2: How to Evaluate Your LLM ApplicationPart 3: How to Choose the Right Chunking Strategy for Your LLM ApplicationPart 4: Improving RAG using metadata extraction and filteringWhat are embeddings and embedding models?An embedding is an array of numbers (a vector) representing a piece of information, such as text, images, audio, video, etc. Together, these numbers capture semantics and other important features of the data. The immediate consequence of doing this is that semantically similar entities map close to each other while dissimilar entities map farther apart in the vector space. For clarity, see the image below for a depiction of a high-dimensional vector space:In the context of natural language processing (NLP), embedding models are algorithms designed to learn and generate embeddings for a given piece of information. In today’s AI applications, embeddings are typically created using large language models (LLMs) that are trained on a massive corpus of data and use cutting-edge algorithms to learn complex semantic relationships in the data.What is RAG (briefly)Retrieval Augmented Generation, as the name suggests, aims to improve the quality of pre-trained LLM generation using data retrieved from a knowledge base. The success of RAG lies in retrieving the most relevant results from the knowledge base. This is where embeddings come into the picture. A RAG pipeline looks something like this:In the above pipeline, we see a common approach used for retrieval in GenAI applications — i.e., semantic search. In this technique, an embedding model is used to create vector representations of the user query and of information in the knowledge base. This way, given a user query and its embedding, we can retrieve the most relevant source documents from the knowledge base based on how similar their embeddings are to the query embedding. The retrieved documents, user query, and any user prompts are then passed as context to an LLM, to generate an answer to the user’s question.Choosing the right embedding model for your RAG applicationAs we have seen above, embeddings are central to RAG. But with so many models out there, how do we choose the best one for our use case?A good place to start when looking for embedding models to use is the MTEB Leaderboard on Hugging Face. It is the most up-to-date list of proprietary and open-source text embedding models, accompanied by statistics on how each model performs on various embedding tasks such as retrieval, summarization, etc.Evaluations of this magnitude for multimodal models are just emerging (see the MME benchmark) so we will only focus on text embedding models for this tutorial. However, all the guidance here on choosing an embedding model also applies to multimodal models.Benchmarks are a good place to begin but bear in mind that these results are self-reported and have been benchmarked on datasets that might not accurately represent the data you are dealing with. It is also possible that some models may include the MTEB datasets in their training data since they are publicly available. So even if you choose a model based on benchmark results, we recommend evaluating it on your dataset. We will see how to do this later in the tutorial, but first, let’s take a closer look at the leaderboard.Here’s a snapshot of the top 10 models on the leaderboard currently:Let’s look at the Overall tab since it provides a comprehensive summary of each model. However, note that we have sorted the leaderboard by the Retrieval Average column. This is because RAG is a retrieval\\xa0task and we want to see the best retrieval models at the top. We will ignore columns corresponding to other tasks, and focus on the following columns:Retrieval Average: Represents average Normalized Discounted Cumulative Gain (NDCG) @ 10 across several datasets. NDCG is a common metric to measure the performance of retrieval systems. A higher NDCG indicates a model that is better at ranking relevant items higher in the list of retrieved results.\\xa0Model Size: Size of the model (in GB). It gives an idea of the computational resources required to run the model. While retrieval performance scales with model size, it is important to note that model size also has a direct impact on latency. The latency-performance trade-off becomes especially important in a production setup.\\xa0\\xa0Max Tokens: Number of tokens that can be compressed into a single embedding. You typically don’t want to put more than a single paragraph of text (~100 tokens) into a single embedding. So even models with max tokens of 512 should be more than enough.Embedding Dimensions: Length of the embedding vector. Smaller embeddings offer faster inference and are more storage-efficient, while more dimensions can capture nuanced details and relationships in the data. Ultimately, we want a good trade-off between capturing the complexity of data and operational efficiency.The top 10 models on the leaderboard contain a mix of small vs large and proprietary vs open-source models. Let’s compare some of these to find the best embedding model for our dataset.Before we beginHere are some things to note about our evaluation experiment.DatasetMongoDB’s cosmopedia-wikihow-chunked dataset is available on Hugging Face, which consists of prechunked WikiHow-style articles.Models evaluatedvoyage-lite-02-instruct: A proprietary embedding model from VoyageAItext-embedding-3-large: One of OpenAI’s latest proprietary embedding modelsUAE-Large-V1: A small-ish (335M parameters) open-source embedding modelWe also attempted to evaluate SFR-Embedding-Mistral, currently the #1 model on the MTEB leaderboard, but the hardware below was not sufficient to run this model. This model and other 14+ GB models on the leaderboard will likely require a/multiple GPU(s) with at least 32 GB of total memory, which means higher costs and/or getting into distributed inference. While we haven’t evaluated this model in our experiment, this is already a good data point when thinking about cost and resources.Evaluation metricsWe used the following metrics to evaluate embedding performance:Embedding latency: Time taken to create embeddingsRetrieval quality: Relevance of retrieved documents to the user queryHardware used1 NVIDIA T4 GPU, 16GB MemoryWhere’s the code?Evaluation notebooks for each of the above models are available:voyage-lite-02-instructtext-embedding-3-largeUAE-Large-V1To run a notebook, click on the Open in Colab shield at the top of the notebook. The notebook will open in Google Colaboratory.Click the Connect button on the top right corner to connect to a hosted runtime environment.Once connected, you can also change the runtime type to use the T4 GPUs available for free on Google Colab.Step 1: Install the required librariesThe libraries required for each model differ slightly, but the common ones are as follows:datasets: Python library to get access to datasets available on Hugging Face Hubsentence-transformers: Framework for working with text and image embeddingsnumpy: Python library that provides tools to perform mathematical operations on arrayspandas: Python library for data analysis, exploration, and manipulationtdqm: Python module to show a progress meter for loopsCode Snippet! pip install -qU datasets sentence-transformers numpy pandas tqdmAdditionally for Voyage AI:\\nvoyageai: Python library to interact with OpenAI APIsCode Snippet! pip install -qU voyageaiAdditionally for OpenAI:\\nopenai: Python library to interact with OpenAI APIsCode Snippet! pip install -qU openaiAdditionally for UAE:\\ntransformers: Python library that provides APIs to interact with pre-trained models available on Hugging FaceCode Snippet! pip install -qU transformersStep 2: Setup pre-requisitesOpenAI and Voyage AI models are available via APIs. So you’ll need to obtain API keys and make them available to the respective clients.Code Snippetimport os\\nimport getpassInitialize Voyage AI client:Code Snippetimport voyageai\\nVOYAGE_API_KEY = getpass.getpass(\"Voyage API Key:\")\\nvoyage_client = voyageai.Client(api_key=VOYAGE_API_KEY)Initialize OpenAI client:Code Snippetfrom openai import OpenAI\\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\\nopenai_client = OpenAI()Step 3: Download the evaluation datasetAs mentioned previously, we will use MongoDB’s cosmopedia-wikihow-chunked dataset. The dataset is quite large (1M+ documents). So we will stream it and grab the first 25k records, instead of downloading the entire dataset to disk.Code Snippetfrom datasets import load_dataset\\nimport pandas as pd\\n\\n# Use streaming=True to load the dataset without downloading it fully\\ndata = load_dataset(\"MongoDB/cosmopedia-wikihow-chunked\", split=\"train\", streaming=True)\\n# Get first 25k records from the dataset\\ndata_head = data.take(25000)\\ndf = pd.DataFrame(data_head)\\n\\n# Use this if you want the full dataset\\n# data = load_dataset(\"MongoDB/cosmopedia-wikihow-chunked\", split=\"train\")\\n# df = pd.DataFrame(data)Step 4: Data analysisNow that we have our dataset, let’s perform some simple data analysis and run some sanity checks on our data to ensure that we don’t see any obvious errors:Code Snippet# Ensuring length of dataset is what we expect i.e. 25k\\nlen(df)\\n\\n# Previewing the contents of the data\\ndf.head()\\n\\n# Only keep records where the text field is not null\\ndf = df[df[\"text\"].notna()]\\n\\n# Number of unique documents in the dataset\\ndf.doc_id.nunique()Step 5: Create embeddingsNow, let’s create embedding functions for each of our models.For voyage-lite-02-instruct:Code Snippetdef get_embeddings(docs: List[str], input_type: str, model:str=\"voyage-lite-02-instruct\") -> List[List[float]]:\\n    \"\"\"\\n    Get embeddings using the Voyage AI API.\\n\\n    Args:\\n        docs (List[str]): List of texts to embed\\n        input_type (str): Type of input to embed. Can be \"document\" or \"query\".\\n        model (str, optional): Model name. Defaults to \"voyage-lite-02-instruct\".\\n\\n    Returns:\\n        List[List[float]]: Array of embedddings\\n    \"\"\"\\n    response = voyage_client.embed(docs, model=model, input_type=input_type)\\n    return response.embeddingsThe embedding function above takes a list of texts (docs) and an input_type as arguments and returns a list of embeddings. The input_type can be document or query depending on whether we are embedding a list of documents or user queries. Voyage uses this value to prepend the inputs with special prompts to enhance retrieval quality.For text-embedding-3-large:Code Snippetdef get_embeddings(docs: List[str], model: str=\"text-embedding-3-large\") -> List[List[float]]:\\n    \"\"\"\\n    Generate embeddings using the OpenAI API.\\n\\n    Args:\\n        docs (List[str]): List of texts to embed\\n        model (str, optional): Model name. Defaults to \"text-embedding-3-large\".\\n\\n    Returns:\\n        List[float]: Array of embeddings\\n    \"\"\"\\n    # replace newlines, which can negatively affect performance.\\n    docs = [doc.replace(\"\\\\n\", \" \") for doc in docs]\\n    response = openai_client.embeddings.create(input=docs, model=model)\\n    response = [r.embedding for r in response.data]\\n    return responseThe embedding function for the OpenAI model is similar to the previous one, with some key differences — there is no input_type argument, and the API returns a list of embedding objects, which need to be parsed to get the final list of embeddings. A sample response from the API looks as follows:Code Snippet{\\n  \"data\": [\\n    {\\n      \"embedding\": [\\n        0.018429679796099663,\\n        -0.009457024745643139\\n    .\\n    .\\n    .\\n      ],\\n      \"index\": 0,\\n      \"object\": \"embedding\"\\n    }\\n  ],\\n  \"model\": \"text-embedding-3-large\",\\n  \"object\": \"list\",\\n  \"usage\": {\\n    \"prompt_tokens\": 183,\\n    \"total_tokens\": 183\\n  }\\n}For UAE-large-V1:Code Snippetfrom typing import List\\nfrom transformers import AutoModel, AutoTokenizer\\nimport torch\\n\\n# Instruction to append to user queries, to improve retrieval\\nRETRIEVAL_INSTRUCT = \"Represent this sentence for searching relevant passages:\"\\n\\n# Check if CUDA (GPU support) is available, and set the device accordingly\\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\\n# Load the UAE-Large-V1 model from the Hugging Face \\nmodel = AutoModel.from_pretrained(\\'WhereIsAI/UAE-Large-V1\\').to(device)\\n# Load the tokenizer associated with the UAE-Large-V1 model\\ntokenizer = AutoTokenizer.from_pretrained(\\'WhereIsAI/UAE-Large-V1\\')\\n\\n# Decorator to disable gradient calculations\\n@torch.no_grad()\\ndef get_embeddings(docs: List[str], input_type: str) -> List[List[float]]:\\n    \"\"\"\\n    Get embeddings using the UAE-Large-V1 model.\\n\\n    Args:\\n        docs (List[str]): List of texts to embed\\n        input_type (str): Type of input to embed. Can be \"document\" or \"query\".\\n\\n    Returns:\\n        List[List[float]]: Array of embedddings\\n    \"\"\"\\n    # Prepend retrieval instruction to queries\\n    if input_type == \"query\":\\n        docs = [\"{}{}\".format(RETRIEVAL_INSTRUCT, q) for q in docs]\\n    # Tokenize input texts\\n    inputs = tokenizer(docs, padding=True, truncation=True, return_tensors=\\'pt\\', max_length=512).to(device)\\n    # Pass tokenized inputs to the model, and obtain the last hidden state\\n    last_hidden_state = model(**inputs, return_dict=True).last_hidden_state\\n    # Extract embeddings from the last hidden state\\n    embeddings = last_hidden_state[:, 0]\\n    return embeddings.cpu().numpy()The UAE-Large-V1 model is an open-source model available on Hugging Face Model Hub. First, we will need to download the model and its tokenizer from Hugging Face. We do this using the Auto classes — namely, AutoModel and AutoTokenizer from the Transformers library  — which automatically infers the underlying model architecture, in this case, BERT. Next, we load the model onto the GPU using .to(device) since we have one available.The embedding function for the UAE model, much like the Voyage model, takes a list of texts (docs) and an input_type as arguments and returns a list of embeddings. A special prompt is prepended to queries for better retrieval as well.The input texts are first tokenized, which includes padding (for short sequences) and truncation (for long sequences)  as needed to ensure that the length of inputs to the model is consistent — 512, in this case, defined by the max_length parameter. The pt value for return_tensors indicates that the output of tokenization should be PyTorch tensors.The tokenized texts are then passed to the model for inference and the last hidden layer (last_hidden_state) is extracted. This layer is the model’s final learned representation of the entire input sequence. The final embedding, however, is extracted only from the first token, which is often a special token ([CLS] in BERT) in transformer-based models. This token serves as an aggregate representation of the entire sequence due to the self-attention mechanism in transformers, where the representation of each token in a sequence is influenced by all other tokens. Finally, we move the embeddings back to CPU using .cpu() and convert the PyTorch tensors to numpy arrays using .numpy().Step 6: EvaluationAs mentioned previously, we will evaluate the models based on embedding latency and retrieval quality.Measuring embedding latencyTo measure embedding latency, we will create a local vector store, which is essentially a list of embeddings for the entire dataset. Latency here is defined as the time it takes to create embeddings for the full dataset.Code Snippetfrom tqdm.auto import tqdm\\n\\n# Get all the texts in the dataset\\ntexts = df[\"text\"].tolist()\\n\\n# Number of samples in a single batch\\nbatch_size = 128\\n\\nembeddings = []\\n# Generate embeddings in batches\\nfor i in tqdm(range(0, len(texts), batch_size)):\\n    end = min(len(texts), i+batch_size)\\n    batch = texts[i:end]\\n    # Generate embeddings for current batch\\n    batch_embeddings = get_embeddings(batch)\\n    # Add to the list of embeddings\\n    embeddings.extend(batch_embeddings)We first create a list of all the texts we want to embed and set the batch size. The voyage-lite-02-instruct model has a batch size limit of 128, so we use the same for all models, for consistency. We iterate through the list of texts, grabbing batch_size number of samples in each iteration, getting embeddings for the batch, and adding them to our \"vector store\".The time taken to generate embeddings on our hardware looked as follows:ModelBatch SizeDimensionsTimetext-embedding-3-large12830724m 17svoyage-lite-02-instruct128102411m 14sUAE-large-V1128102419m 50sThe OpenAI model has the lowest latency. However, note that it also has three times the number of embedding dimensions compared to the other two models. OpenAI also charges by tokens used, so both the storage and inference costs of this model can add up over time. While the UAE model is the slowest of the lot (despite running inference on a GPU), there is room for optimizations such as quantization, distillation, etc., since it is open-source.Measuring retrieval qualityTo evaluate retrieval quality, we use a set of questions based on themes seen in our dataset. For real applications, however, you will want to curate a set of \"cannot-miss\" questions — i.e. questions that you would typically expect users to ask from your data. For this tutorial, we will qualitatively evaluate the relevance of retrieved documents as a measure of quality, but we will explore metrics and techniques for quantitative evaluations in a following tutorial.Here are the main themes (generated using ChatGPT) covered by the top three documents retrieved by each model for our queries:😐 denotes documents that we felt weren’t as relevant to the question. Sentences that contributed to this verdict have been highlighted in bold.Query: Give me some tips to improve my mental health.voyage-lite-02-instructtext-embedding-3-largeUAE-large-V1😐 Regularly reassess treatment efficacy and modify plans as needed. Track mood, thoughts, and behaviors; share updates with therapists and support network. Use a multifaceted approach to manage suicidal thoughts, involving resources, skills, and connections.Eat balanced, exercise, sleep well. Cultivate relationships, engage socially, set boundaries. Manage stress with effective coping mechanisms.Prioritizing mental health is essential, not selfish. Practice mindfulness through meditation, journaling, and activities like yoga. Adopt healthy habits for better mood, less anxiety, and improved cognition.Recognize early signs of stress, share concerns, and develop coping mechanisms. Combat isolation by nurturing relationships and engaging in social activities. Set boundaries, communicate openly, and seek professional help for social anxiety.Prioritizing mental health is essential, not selfish. Practice mindfulness through meditation, journaling, and activities like yoga. Adopt healthy habits for better mood, less anxiety, and improved cognition.Eat balanced, exercise regularly, get 7-9 hours of sleep. Cultivate positive relationships, nurture friendships, and seek new social opportunities. Manage stress with effective coping mechanisms.Prioritizing mental health is essential, not selfish. Practice mindfulness through meditation, journaling, and activities like yoga. Adopt healthy habits for better mood, less anxiety, and improved cognition.Acknowledging feelings is a step to address them. Engage in self-care activities to boost mood and health. Make self-care consistent for lasting benefits.😐 Taking care of your mental health is crucial for a fulfilling life, productivity, and strong relationships. Recognize the importance of mental health in all aspects of life. Managing mental health reduces the risk of severe psychological conditions.While the results cover similar themes, the Voyage AI model keys in heavily on seeking professional help, while the UAE model covers slightly more about why taking care of your mental health is important. The OpenAI model is the one that consistently retrieves documents that cover general tips for improving mental health.Query: Give me some tips for writing good code.voyage-lite-02-instructtext-embedding-3-largeUAE-large-V1Strive for clean, maintainable code with consistent conventions and version control. Utilize linters, static analyzers, and document work for quality and collaboration. Embrace best practices like SOLID and TDD to enhance design, scalability, and extensibility.Strive for clean, maintainable code with consistent conventions and version control. Utilize linters, static analyzers, and document work for quality and collaboration. Embrace best practices like SOLID and TDD to enhance design, scalability, and extensibility.Strive for clean, maintainable code with consistent conventions and version control. Utilize linters, static analyzers, and document work for quality and collaboration. Embrace best practices like SOLID and TDD to enhance design, scalability, and extensibility.😐 Code and test core gameplay mechanics like combat and quest systems; debug and refine for stability. Use modular coding, version control, and object-oriented principles for effective game development. Playtest frequently to find and fix bugs, seek feedback, and prioritize significant improvements.😐 Good programming needs dedication, persistence, and patience. Master core concepts, practice diligently, and engage with peers for improvement. Every expert was once a beginner—keep pushing forward.Read programming books for comprehensive coverage and deep insights, choosing beginner-friendly texts with pathways to proficiency. Combine reading with coding to reinforce learning; take notes on critical points and unfamiliar terms. Engage with exercises and challenges in books to apply concepts and enhance skills.😐 Monitor social media and newsletters for current software testing insights. Participate in networks and forums to exchange knowledge with experienced testers. Regularly update your testing tools and methods for enhanced efficiency.Apply learning by working on real projects, starting small and progressing to larger ones. Participate in open-source projects or develop your applications to enhance problem-solving. Master debugging with IDEs, print statements, and understanding common errors for productivity.😐 Programming is key in various industries, offering diverse opportunities. This guide covers programming fundamentals, best practices, and improvement strategies. Choose a programming language based on interests, goals, and resources.All the models seem to struggle a bit with this question. They all retrieve at least one document that is not as relevant to the question. However, it is interesting to note that all the models retrieve the same document as their number one.Query: What are some environment-friendly practices I can incorporate in everyday life?voyage-lite-02-instructtext-embedding-3-largeUAE-large-V1😐 Conserve resources by reducing waste, reusing, and recycling, reflecting Jawa culture\\'s values due to their planet\\'s limited resources. Monitor consumption (e.g., water, electricity), repair goods, and join local environmental efforts. Eco-friendly practices enhance personal and global well-being, aligning with Jawa values.Carry reusable bags for shopping, keeping extras in your car or bag. Choose sustainable alternatives like reusable water bottles and eco-friendly cutlery. Support businesses that minimize packaging and use biodegradable materials.Educate others on eco-friendly practices; lead by example. Host workshops or discussion groups on sustainable living.Embody respect for the planet; every effort counts towards improvement.Learn and follow local recycling rules, rinse containers, and educate others on proper recycling. Opt for green transportation like walking, cycling, or electric vehicles, and check for incentives. Upgrade to energy-efficient options like LED lights, seal drafts, and consider renewable energy sources.Opt for sustainable transportation, energy-efficient appliances, solar panels, and eat less meat to reduce emissions. Conserve water by fixing leaks, taking shorter showers, and using low-flow fixtures. Water conservation protects ecosystems, ensures food security, and reduces infrastructure stress.Carry reusable bags for shopping, keeping extras in your car or bag. Choose sustainable alternatives like reusable water bottles and eco-friendly cutlery. Support businesses that minimize packaging and use biodegradable materials.😐 Consistently implement these steps. Actively contribute to a cleaner, greener world. Support resilience for future generations.Conserve water with low-flow fixtures, fix leaks, and use rainwater for gardening. Compost kitchen scraps to reduce waste and enrich soil, avoid meat and dairy. Shop locally at farmers markets and CSAs to lower emissions and support local economies.Join local tree-planting events and volunteer at community gardens or restoration projects. Integrate native plants into landscaping to support pollinators and remove invasive species. Adopt eco-friendly transportation methods to decrease fossil fuel consumption.We see a similar trend with this query as with the previous two examples — the OpenAI model consistently retrieves documents that provide the most actionable tips, followed by the UAE model. The Voyage model provides more high-level advice.Overall, based on our preliminary evaluation, OpenAI’s text-embedding-3-large model comes out on top. When working with real-world systems, however, a more rigorous evaluation of a larger dataset is recommended. Also, operational costs become an important consideration. More on evaluation coming in Part 2 of this series!ConclusionIn this tutorial, we looked into how to choose the right model to embed data for RAG. The MTEB leaderboard is a good place to start, especially for text embedding models, but evaluating them on your data is important to find the best one for your RAG application. Storage and inference costs, embedding latency, and retrieval quality are all important parameters to consider while evaluating embedding models. The best model is typically one that offers the best trade-off across these dimensions.Now that you have a good understanding of embedding models, here are some resources to get started with building RAG applications using MongoDB:Using Latest OpenAI Embeddings in a RAG System With MongoDBBuilding a RAG System With Google’s Gemma, Hugging Face, and MongoDBHow to Build a RAG System With LlamaIndex, OpenAI, and MongoDBFollow along with these by creating a free MongoDB Atlas cluster and reach out to us in our Generative AI community forums if you have any questions.Top Comments in ForumsThere are no comments on this article yet.Start the ConversationRate this tutorialRelatedTutorialHow to Use Custom Archival Rules and Partitioning on MongoDB Atlas Online Archive May 31, 2023 | 5 min readTutorialHow to Deploy MongoDB Atlas with AWS CDK in TypeScript Jan 23, 2024 | 5 min readArticleHow to work with Johns Hopkins University COVID-19 Data in MongoDB Atlas Nov 16, 2023 | 8 min readArticleTriggers Treats and Tricks - Auto-Increment a Running ID Field Sep 23, 2022 | 3 min readRequest a TutorialTable of ContentsWhat are embeddings and embedding models?What is RAG (briefly)Choosing the right embedding model for your RAG applicationStep 1: Install the required librariesStep 2: Setup pre-requisitesStep 3: Download the evaluation datasetStep 4: Data analysisStep 5: Create embeddingsStep 6: EvaluationConclusionEnglishEnglishPortuguêsEspañol한국어日本語ItalianoDeutschFrançais简体中文© 2024 MongoDB, Inc.AboutCareersInvestor RelationsLegal NoticesPrivacy NoticesSecurity InformationTrust CenterSupportContact UsCustomer PortalAtlas StatusCustomer SupportSocialGitHubStack OverflowLinkedInYouTubeXTwitchFacebook© 2024 MongoDB, Inc.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'https://www.mongodb.com/developer/products/atlas/choose-embedding-model-rag/',\n",
       " 'title': 'How to Choose the Right Embedding Model for Your LLM Application | MongoDB',\n",
       " 'description': 'In this tutorial, we will see why embeddings are important for RAG, and how to choose the right embedding model for your RAG application.',\n",
       " 'language': 'en'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunk up the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    encoding_name=\"cl100k_base\", chunk_size=200, chunk_overlap=30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_docs = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "202"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(split_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_docs = [doc.dict() for doc in split_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': None,\n",
       " 'metadata': {'source': 'https://www.mongodb.com/developer/products/atlas/choose-embedding-model-rag/',\n",
       "  'title': 'How to Choose the Right Embedding Model for Your LLM Application | MongoDB',\n",
       "  'description': 'In this tutorial, we will see why embeddings are important for RAG, and how to choose the right embedding model for your RAG application.',\n",
       "  'language': 'en'},\n",
       " 'page_content': 'How to Choose the Right Embedding Model for Your LLM Application | MongoDBBlogAtlas Vector Search voted most loved vector database in 2024 Retool State of AI reportLearn more\\xa0>>Developer Articles & TopicsGeneral InformationDocumentationDeveloper Articles & TopicsCommunity ForumsBlogUniversityProductsPlatformAtlasBuild on a developer data platformPlatform ServicesDatabaseDeploy a multi-cloud databaseSearchDeliver engaging search experiencesVector SearchDesign intelligent apps with GenAIStream ProcessingUnify data in motion and data at restToolsCompassWork with MongoDB data in a GUIIntegrationsIntegrations with third-party servicesRelational MigratorMigrate to MongoDB with confidenceSelf ManagedEnterprise AdvancedRun and manage MongoDB yourselfCommunity EditionDevelop locally with MongoDBBuild with MongoDB AtlasGet started for free in minutesSign UpTest Enterprise AdvancedDevelop with MongoDB on-premisesDownloadTry Community EditionExplore the latest version of MongoDBDownloadResourcesDocumentationAtlas DocumentationGet started using AtlasServer DocumentationLearn to use MongoDBStart With GuidesGet step-by-step guidance for key tasks Tools',\n",
       " 'type': 'Document'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = SentenceTransformer(\"mixedbread-ai/mxbai-embed-large-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text: str):\n",
    "    embedding = embedding_model.encode(text)\n",
    "    return embedding.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_docs = [\n",
    "    {**d, \"embedding\": get_embedding(d[\"page_content\"])} for d in split_docs\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_docs = []\n",
    "for doc in split_docs:\n",
    "    temp = doc.copy()\n",
    "    temp[\"embedding\"] = get_embedding(temp[\"page_content\"])\n",
    "    embedded_docs.append(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Perform Semantic Search on Your Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "from typing import List, Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingest documents into MongoDB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a MongoDB Python client\n",
    "mongo_client = MongoClient(MONGODB_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of the database -- Change if needed or leave as is\n",
    "DB_NAME = \"mongodb_rag_lab\"\n",
    "# Name of the collection -- Change if needed or leave as is\n",
    "COLLECTION_NAME = \"knowledge_base\"\n",
    "# Name of the vector search index -- Change if needed or leave as is\n",
    "ATLAS_VECTOR_SEARCH_INDEX_NAME = \"vector_index\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the collection defined above using the MongoDB client\n",
    "collection = mongo_client[DB_NAME][COLLECTION_NAME]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeleteResult({'n': 202, 'electionId': ObjectId('7fffffff000000000000000c'), 'opTime': {'ts': Timestamp(1720559750, 201), 't': 12}, 'ok': 1.0, '$clusterTime': {'clusterTime': Timestamp(1720559750, 202), 'signature': {'hash': b'|\\xdd\\xc0\\xedw\\xcd\\xfd?\\xc7k\\xd22\\xff\\xc3\\nC\\x0b\\x17\\xa16', 'keyId': 7353010953081847814}}, 'operationTime': Timestamp(1720559750, 201)}, acknowledged=True)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bulk delete all existing records from the collection defined above -- should be a one-liner\n",
    "collection.delete_many({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data ingestion into MongoDB completed\n"
     ]
    }
   ],
   "source": [
    "# Bulk insert `records` into the collection defined above -- should be a one-liner\n",
    "collection.insert_many(embedded_docs)\n",
    "\n",
    "print(\"Data ingestion into MongoDB completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a vector search index\n",
    "\n",
    "Follow the instructions in the documentation to create a Vector Search index in the Atlas UI.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a vector search function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_search(user_query: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Perform a vector search on a MongoDB collection based on the user query.\n",
    "\n",
    "    Args:\n",
    "    user_query (str): The user's query string.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of matching documents.\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate embedding for the user query\n",
    "    query_embedding = get_embedding(user_query)\n",
    "\n",
    "    # Define the vector search pipeline\n",
    "    pipeline = [\n",
    "        {\n",
    "            \"$vectorSearch\": {\n",
    "                \"index\": ATLAS_VECTOR_SEARCH_INDEX_NAME,\n",
    "                \"queryVector\": query_embedding,\n",
    "                \"path\": \"embedding\",\n",
    "                \"numCandidates\": 150,\n",
    "                \"limit\": 5,\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"$project\": {\n",
    "                \"_id\": 0,\n",
    "                \"page_content\": 1,\n",
    "                \"score\": {\"$meta\": \"vectorSearchScore\"},\n",
    "            }\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    # Execute the search\n",
    "    results = collection.aggregate(pipeline)\n",
    "    return list(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run vector search queries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page_content': 'models out there, how do we choose the best one for our use case?A good place to start when looking for embedding models to use is the MTEB Leaderboard on Hugging Face. It is the most up-to-date list of proprietary and open-source text embedding models, accompanied by statistics on how each model performs on various embedding tasks such as retrieval, summarization, etc.Evaluations of this magnitude for multimodal models are just emerging (see the MME benchmark) so we will only focus on text embedding models for this tutorial. However, all the guidance here on choosing an embedding model also applies to multimodal models.Benchmarks are a good place to begin but bear in mind that these results are self-reported and have been benchmarked on datasets that might not accurately represent the data you are dealing with. It is also possible that some models may include the MTEB datasets in their training data since they are publicly available. So even if you choose a model based on benchmark',\n",
       "  'score': 0.8928182125091553},\n",
       " {'page_content': 'Topicschevron-rightProductschevron-rightAtlaschevron-rightTutorialsHow to Choose the Right Embedding Model for Your LLM ApplicationApoorva Joshi16 min read • Published Jun 17, 2024 • Updated Jun 17, 2024AIPythonAtlasRate this tutorialIf you are building Generative AI (GenAI) applications in 2024, you’ve probably heard the term “embeddings” a few times by now and are seeing new embedding models hit the shelf every week. So why do so many people suddenly care about embeddings, a concept that has existed since the 1950s? And if embeddings are so important and you must use them, how do you choose among the vast number of options out there?This tutorial will cover the following:What are embeddings?Importance of embeddings in RAG applicationsHow to choose the right embedding model for your RAG applicationEvaluating embedding modelsThis tutorial is Part 1 of a multi-part series',\n",
       "  'score': 0.8831697702407837},\n",
       " {'page_content': 'vector. Smaller embeddings offer faster inference and are more storage-efficient, while more dimensions can capture nuanced details and relationships in the data. Ultimately, we want a good trade-off between capturing the complexity of data and operational efficiency.The top 10 models on the leaderboard contain a mix of small vs large and proprietary vs open-source models. Let’s compare some of these to find the best embedding model for our dataset.Before we beginHere are some things to note about our evaluation experiment.DatasetMongoDB’s cosmopedia-wikihow-chunked dataset is available on Hugging Face, which consists of prechunked WikiHow-style articles.Models evaluatedvoyage-lite-02-instruct: A proprietary embedding model from VoyageAItext-embedding-3-large: One of OpenAI’s latest proprietary embedding modelsUAE-Large-V1: A small-ish (335M parameters) open-source embedding modelWe also attempted to evaluate SFR-Embedding-Mistral, currently the #1 model on the',\n",
       "  'score': 0.8781559467315674},\n",
       " {'page_content': 'applicationsHow to choose the right embedding model for your RAG applicationEvaluating embedding modelsThis tutorial is Part 1 of a multi-part series on Retrieval Augmented Generation (RAG), where we start with the fundamentals of building a RAG application, and work our way to more advanced techniques for RAG. The series will cover the following:Part 1: How to Choose the Right Embedding Model for Your LLM ApplicationPart 2: How to Evaluate Your LLM ApplicationPart 3: How to Choose the Right Chunking Strategy for Your LLM ApplicationPart 4: Improving RAG using metadata extraction and filteringWhat are embeddings and embedding models?An embedding is an array of numbers (a vector) representing a piece of information, such as text, images, audio, video, etc. Together, these numbers capture semantics and other important features of the data. The immediate consequence of doing this is that semantically similar entities map close to each other while dissimilar entities',\n",
       "  'score': 0.8729569911956787},\n",
       " {'page_content': 'How to Choose the Right Embedding Model for Your LLM Application | MongoDBBlogAtlas Vector Search voted most loved vector database in 2024 Retool State of AI reportLearn more\\xa0>>Developer Articles & TopicsGeneral InformationDocumentationDeveloper Articles & TopicsCommunity ForumsBlogUniversityProductsPlatformAtlasBuild on a developer data platformPlatform ServicesDatabaseDeploy a multi-cloud databaseSearchDeliver engaging search experiencesVector SearchDesign intelligent apps with GenAIStream ProcessingUnify data in motion and data at restToolsCompassWork with MongoDB data in a GUIIntegrationsIntegrations with third-party servicesRelational MigratorMigrate to MongoDB with confidenceSelf ManagedEnterprise AdvancedRun and manage MongoDB yourselfCommunity EditionDevelop locally with MongoDBBuild with MongoDB AtlasGet started for free in minutesSign UpTest Enterprise AdvancedDevelop with MongoDB on-premisesDownloadTry Community EditionExplore the latest version of MongoDBDownloadResourcesDocumentationAtlas DocumentationGet started using AtlasServer DocumentationLearn to use MongoDBStart With GuidesGet step-by-step guidance for key tasks Tools',\n",
       "  'score': 0.853762149810791}]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_search(\n",
    "    \"What are the important considerations while choosing an embedding model?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page_content': 'breaking down large pieces of text into smaller segments or chunks. In the context of RAG, embedding smaller chunks instead of entire documents to create the knowledge base means that given a user query, you only have to retrieve the most relevant document chunks, resulting in fewer input tokens and more targeted context for the LLM to work with.Choosing the right chunking strategy for your RAG applicationThere is no “one size fits all” solution when it comes to choosing a chunking strategy for RAG — it depends on the structure of the documents being used to create the knowledge base and will look different depending on whether you are working with well-formatted text documents or documents with code snippets, tables, images, etc. The three key components of a chunking strategy are as follows:Splitting technique: Determines where the chunk boundaries will be placed — based on paragraph boundaries, programming language-specific separators, tokens, or even semantic boundariesChunk size: The maximum number of characters or tokens allowed for each chunkChunk',\n",
       "  'score': 0.9121989011764526},\n",
       " {'page_content': 'Topicschevron-rightProductschevron-rightAtlaschevron-rightTutorialsHow to Choose the Right Chunking Strategy for Your LLM ApplicationApoorva Joshi15 min read • Published Jun 17, 2024 • Updated Jun 17, 2024AIPythonAtlasRate this tutorialIn Part 1 of this series on Retrieval Augmented Generation (RAG), we looked into choosing the right embedding model for your RAG application. While the choice of embedding model is an important consideration to ensure good quality retrieval for RAG, there is one key decision to be made before the embedding stage that can have a significant downstream impact — choosing the right chunking strategy for your data.In this tutorial, we will cover the following:What is chunking and why is it important for RAG?Choosing the right chunking strategy for your RAG applicationEvaluating different chunking methodologies on a datasetRAG — a very quick refresherIn a RAG application, the goal',\n",
       "  'score': 0.9081633687019348},\n",
       " {'page_content': 'a very small evaluation dataset of 10 samples for demonstration so the above results cannot be treated as fully conclusive. In reality, you will want to use a larger, more representative evaluation dataset.ConclusionIn this tutorial, we learned how to choose the right chunking strategy for RAG. Breaking up large documents into smaller chunks for RAG results in fewer tokens passed as input to LLMs, and a more targeted context for LLMs to work with. Chunking strategies are composed of three key components — splitting technique, chunk size, and chunk overlap. Picking the right strategy involves experimenting with different combinations of the three components.Now that you have a good understanding of chunking for RAG, take it up as a challenge to evaluate different chunking strategies on datasets that were used in some of our previous tutorials:Building an AI Agent with Memory Using MongoDB, Fireworks AI, and LangChainHow to Build a RAG System Using Claude 3 Opus and MongoDBIf you have further questions',\n",
       "  'score': 0.9075931310653687},\n",
       " {'page_content': 'use LangChain for text splitting and for creating components of a RAG application as needed. We will use the Ragas framework which we introduced in Part 2 of this tutorial series to evaluate the best chunking strategy for our data.Where’s the code?The Jupyter Notebook for this tutorial can be found on GitHub.Chunking strategiesLooking at the PEP documents, we note that they contain mostly text and some Python code snippets. With this in mind, we will test out the following chunking strategies:Fixed token without overlapIn this technique, we split the documents into chunks with a fixed number of tokens, with no token overlap between chunks. This can work well if there are hard contextual boundaries between chunks — i.e., the context varies drastically between adjacent chunks. In reality, this is rarely ever the case, but we will keep this as a baseline.A visual representation of this strategy is as follows:Fixed token with overlapIn this technique, we split documents into chunks with a fixed number of',\n",
       "  'score': 0.891015887260437},\n",
       " {'page_content': 'your RAG applicationEvaluating different chunking methodologies on a datasetRAG — a very quick refresherIn a RAG application, the goal is to get more accurate responses from a large language model (LLM) on subjects that might not be well-represented in its parametric knowledge — for example, your organization’s data. This is typically done by retrieving relevant information from a knowledge base using semantic search. This technique uses an embedding model to create vector representations of the user query and information in the knowledge base.Given a user query and its embedding, it retrieves the most relevant documents from the knowledge base using vector similarity search. The retrieved documents, user query, and any user prompts are then passed as context to an LLM, to generate an answer to the user’s question.What is chunking and why is it important for RAG?Most use cases for RAG currently are centered around using LLMs to answer questions about large repositories of data such as technical documentation,',\n",
       "  'score': 0.8862432837486267}]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_search(\"How to choose a chunking strategy for RAG?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Build a RAG Application\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate a chat model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fireworks.client import Fireworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "fw_client = Fireworks()\n",
    "model = \"accounts/fireworks/models/llama-v2-7b-chat\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function to create the chat prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(user_query: str) -> str:\n",
    "    \"\"\"\n",
    "    Create a chat prompt that includes the user query and retrieved context.\n",
    "\n",
    "    Args:\n",
    "        user_query (str): The user's query string.\n",
    "\n",
    "    Returns:\n",
    "        str: The chat prompt string.\n",
    "    \"\"\"\n",
    "    context = vector_search(user_query)\n",
    "    context = \"\\n\\n\".join([d.get(\"page_content\", \"\") for d in context])\n",
    "    prompt = f\"Answer the question based only on the following context. If the context is empty, say I DON'T KNOW\\n\\nContext:\\n{context}\\n\\nQuestion:{user_query}\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function to answer user queries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(user_query: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate an answer to the user query.\n",
    "\n",
    "    Args:\n",
    "        user_query (str): The user's query string.\n",
    "\n",
    "    Returns:\n",
    "        str: The answer string.\n",
    "    \"\"\"\n",
    "    response = fw_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": create_prompt(user_query),\n",
    "            }\n",
    "        ],\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query the RAG application\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Based on the provided context, the important considerations while choosing an embedding model are:\n",
      "\n",
      "1. Dataset: The dataset you are working with and the type of data it contains, such as text, images, or audio, can influence the choice of embedding model.\n",
      "2. Dimensions: The number of dimensions in the embedding model, which can impact the model's ability to capture complexity and nuanced details in the data.\n",
      "3. Training data: The embedding model may have been trained on a dataset that includes the MTEB datasets, which could affect the model's performance on unseen data.\n",
      "4. Operational efficiency: The embedding model's size and computational requirements can impact the model's operational efficiency, so it's important to choose a model that balances complexity and efficiency.\n",
      "5. Trade-off: The choice of embedding model should be based on a trade-off between capturing the complexity of the data and operational efficiency.\n",
      "6. Self-reported benchmarks: The benchmarks provided by the model authors may not accurately represent the model's performance on unseen data, so it's important to evaluate the model using other metrics as well.\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    generate_answer(\n",
    "        \"What are the important considerations while choosing an embedding model?\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Based on the given context, I cannot answer the question as the context does not provide any information to answer the question. Therefore, I DON'T KNOW.\n"
     ]
    }
   ],
   "source": [
    "print(generate_answer(\"What did I just ask you?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Add memory to the RAG Application\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'session_id_1'"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_collection = mongo_client[DB_NAME][\"chat_history\"]\n",
    "history_collection.create_index(\"session_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function to store chat messages in MongoDB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_chat_message(session_id, role, content):\n",
    "    message = {\n",
    "        \"session_id\": session_id,\n",
    "        \"role\": role,\n",
    "        \"content\": content,\n",
    "        \"timestamp\": datetime.now(),\n",
    "    }\n",
    "    history_collection.insert_one(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function to retrieve chat history from MongoDB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_session_history(session_id):\n",
    "    cursor = history_collection.find({\"session_id\": session_id}).sort(\"timestamp\", 1)\n",
    "\n",
    "    if cursor:\n",
    "        messages = [{\"role\": msg[\"role\"], \"content\": msg[\"content\"]} for msg in cursor]\n",
    "    else:\n",
    "        messages = []\n",
    "\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle chat history in the `generate_answer` function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(session_id: str, user_query: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate an answer to the user query.\n",
    "\n",
    "    Args:\n",
    "        user_query (str): The user's query string.\n",
    "\n",
    "    Returns:\n",
    "        str: The answer string.\n",
    "    \"\"\"\n",
    "    context = vector_search(user_query)\n",
    "    context = \"\\n\\n\".join([d.get(\"page_content\", \"\") for d in context])\n",
    "\n",
    "    messages = []\n",
    "\n",
    "    system_message = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": f\"Answer the question based only on the following context. If the context is empty, say I DON'T KNOW\\n\\nContext:\\n{context}\",\n",
    "    }\n",
    "    messages.append(system_message)\n",
    "\n",
    "    message_history = retrieve_session_history(session_id)\n",
    "    messages += message_history\n",
    "\n",
    "    user_message = {\"role\": \"user\", \"content\": user_query}\n",
    "    messages.append(user_message)\n",
    "    print(messages)\n",
    "\n",
    "    response = fw_client.chat.completions.create(model=model, messages=messages)\n",
    "\n",
    "    answer = response.choices[0].message.content\n",
    "\n",
    "    store_chat_message(session_id, \"user\", user_query)\n",
    "    store_chat_message(session_id, \"assistant\", answer)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': \"Answer the question based only on the following context. If the context is empty, say I DON'T KNOW\\n\\nContext:\\nmodels out there, how do we choose the best one for our use case?A good place to start when looking for embedding models to use is the MTEB Leaderboard on Hugging Face. It is the most up-to-date list of proprietary and open-source text embedding models, accompanied by statistics on how each model performs on various embedding tasks such as retrieval, summarization, etc.Evaluations of this magnitude for multimodal models are just emerging (see the MME benchmark) so we will only focus on text embedding models for this tutorial. However, all the guidance here on choosing an embedding model also applies to multimodal models.Benchmarks are a good place to begin but bear in mind that these results are self-reported and have been benchmarked on datasets that might not accurately represent the data you are dealing with. It is also possible that some models may include the MTEB datasets in their training data since they are publicly available. So even if you choose a model based on benchmark\\n\\nTopicschevron-rightProductschevron-rightAtlaschevron-rightTutorialsHow to Choose the Right Embedding Model for Your LLM ApplicationApoorva Joshi16 min read • Published Jun 17, 2024 • Updated Jun 17, 2024AIPythonAtlasRate this tutorialIf you are building Generative AI (GenAI) applications in 2024, you’ve probably heard the term “embeddings” a few times by now and are seeing new embedding models hit the shelf every week. So why do so many people suddenly care about embeddings, a concept that has existed since the 1950s? And if embeddings are so important and you must use them, how do you choose among the vast number of options out there?This tutorial will cover the following:What are embeddings?Importance of embeddings in RAG applicationsHow to choose the right embedding model for your RAG applicationEvaluating embedding modelsThis tutorial is Part 1 of a multi-part series\\n\\nvector. Smaller embeddings offer faster inference and are more storage-efficient, while more dimensions can capture nuanced details and relationships in the data. Ultimately, we want a good trade-off between capturing the complexity of data and operational efficiency.The top 10 models on the leaderboard contain a mix of small vs large and proprietary vs open-source models. Let’s compare some of these to find the best embedding model for our dataset.Before we beginHere are some things to note about our evaluation experiment.DatasetMongoDB’s cosmopedia-wikihow-chunked dataset is available on Hugging Face, which consists of prechunked WikiHow-style articles.Models evaluatedvoyage-lite-02-instruct: A proprietary embedding model from VoyageAItext-embedding-3-large: One of OpenAI’s latest proprietary embedding modelsUAE-Large-V1: A small-ish (335M parameters) open-source embedding modelWe also attempted to evaluate SFR-Embedding-Mistral, currently the #1 model on the\\n\\napplicationsHow to choose the right embedding model for your RAG applicationEvaluating embedding modelsThis tutorial is Part 1 of a multi-part series on Retrieval Augmented Generation (RAG), where we start with the fundamentals of building a RAG application, and work our way to more advanced techniques for RAG. The series will cover the following:Part 1: How to Choose the Right Embedding Model for Your LLM ApplicationPart 2: How to Evaluate Your LLM ApplicationPart 3: How to Choose the Right Chunking Strategy for Your LLM ApplicationPart 4: Improving RAG using metadata extraction and filteringWhat are embeddings and embedding models?An embedding is an array of numbers (a vector) representing a piece of information, such as text, images, audio, video, etc. Together, these numbers capture semantics and other important features of the data. The immediate consequence of doing this is that semantically similar entities map close to each other while dissimilar entities\\n\\nHow to Choose the Right Embedding Model for Your LLM Application | MongoDBBlogAtlas Vector Search voted most loved vector database in 2024 Retool State of AI reportLearn more\\xa0>>Developer Articles & TopicsGeneral InformationDocumentationDeveloper Articles & TopicsCommunity ForumsBlogUniversityProductsPlatformAtlasBuild on a developer data platformPlatform ServicesDatabaseDeploy a multi-cloud databaseSearchDeliver engaging search experiencesVector SearchDesign intelligent apps with GenAIStream ProcessingUnify data in motion and data at restToolsCompassWork with MongoDB data in a GUIIntegrationsIntegrations with third-party servicesRelational MigratorMigrate to MongoDB with confidenceSelf ManagedEnterprise AdvancedRun and manage MongoDB yourselfCommunity EditionDevelop locally with MongoDBBuild with MongoDB AtlasGet started for free in minutesSign UpTest Enterprise AdvancedDevelop with MongoDB on-premisesDownloadTry Community EditionExplore the latest version of MongoDBDownloadResourcesDocumentationAtlas DocumentationGet started using AtlasServer DocumentationLearn to use MongoDBStart With GuidesGet step-by-step guidance for key tasks Tools\"}, {'role': 'user', 'content': 'What are the important considerations while choosing an embedding model?'}]\n",
      "  According to the provided context, the important considerations while choosing an embedding model are:\n",
      "\n",
      "1. Size: The model's size (i.e., number of parameters) can affect both inference speed and storage efficiency. A smaller model may be faster but may not capture nuanced details, while a larger model may capture more details but be slower and more resource-intensive.\n",
      "2. Dataset: The choice of embedding model should be informed by the specific dataset being used. Different models may perform better or worse on different datasets, so it's important to evaluate the performance of various models on the dataset at hand.\n",
      "3. Evaluation: It's important to evaluate the performance of the various embedding models being considered using metrics such as retrieval, summarization, and other applicable tasks. This can help identify the best-performing model for the specific use case.\n",
      "4. Trade-off: The choice of embedding model should involve a balance between capturing the complexity of the data and operational efficiency. A model that captures too much detail may be slower and more resource-intensive, while a model that doesn't capture enough detail may not perform well in certain tasks.\n",
      "5. Storage-efficiency: Smaller embeddings are generally more storage-efficient, but may not capture as much detail as larger embeddings.\n",
      "6. Self-reported Benchmarks: The results of benchmarks provided by the model developers should be taken with caution, as they might not accurately reflect the model's performance on the specific dataset being used.\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    generate_answer(\n",
    "        \"2\",\n",
    "        user_query=\"What are the important considerations while choosing an embedding model?\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': 'Answer the question based only on the following context. If the context is empty, say I DON\\'T KNOW\\n\\nContext:\\nvector. Smaller embeddings offer faster inference and are more storage-efficient, while more dimensions can capture nuanced details and relationships in the data. Ultimately, we want a good trade-off between capturing the complexity of data and operational efficiency.The top 10 models on the leaderboard contain a mix of small vs large and proprietary vs open-source models. Let’s compare some of these to find the best embedding model for our dataset.Before we beginHere are some things to note about our evaluation experiment.DatasetMongoDB’s cosmopedia-wikihow-chunked dataset is available on Hugging Face, which consists of prechunked WikiHow-style articles.Models evaluatedvoyage-lite-02-instruct: A proprietary embedding model from VoyageAItext-embedding-3-large: One of OpenAI’s latest proprietary embedding modelsUAE-Large-V1: A small-ish (335M parameters) open-source embedding modelWe also attempted to evaluate SFR-Embedding-Mistral, currently the #1 model on the\\n\\n@ 10 across several datasets. NDCG is a common metric to measure the performance of retrieval systems. A higher NDCG indicates a model that is better at ranking relevant items higher in the list of retrieved results.\\xa0Model Size: Size of the model (in GB). It gives an idea of the computational resources required to run the model. While retrieval performance scales with model size, it is important to note that model size also has a direct impact on latency. The latency-performance trade-off becomes especially important in a production setup.\\xa0\\xa0Max Tokens: Number of tokens that can be compressed into a single embedding. You typically don’t want to put more than a single paragraph of text (~100 tokens) into a single embedding. So even models with max tokens of 512 should be more than enough.Embedding Dimensions: Length of the embedding vector. Smaller embeddings offer faster inference and are more storage-efficient, while more dimensions can capture nuanced details and relationships in the data. Ultimately, we\\n\\nembeddings.extend(batch_embeddings)We first create a list of all the texts we want to embed and set the batch size. The voyage-lite-02-instruct model has a batch size limit of 128, so we use the same for all models, for consistency. We iterate through the list of texts, grabbing batch_size number of samples in each iteration, getting embeddings for the batch, and adding them to our \"vector store\".The time taken to generate embeddings on our hardware looked as follows:ModelBatch SizeDimensionsTimetext-embedding-3-large12830724m 17svoyage-lite-02-instruct128102411m 14sUAE-large-V1128102419m 50sThe OpenAI model has the lowest latency. However, note that it also has three times the number of embedding dimensions compared to the other two models. OpenAI also charges by tokens used, so both the storage and inference costs of this model can add up over\\n\\n(335M parameters) open-source embedding modelWe also attempted to evaluate SFR-Embedding-Mistral, currently the #1 model on the MTEB leaderboard, but the hardware below was not sufficient to run this model. This model and other 14+ GB models on the leaderboard will likely require a/multiple GPU(s) with at least 32 GB of total memory, which means higher costs and/or getting into distributed inference. While we haven’t evaluated this model in our experiment, this is already a good data point when thinking about cost and resources.Evaluation metricsWe used the following metrics to evaluate embedding performance:Embedding latency: Time taken to create embeddingsRetrieval quality: Relevance of retrieved documents to the user queryHardware used1 NVIDIA T4 GPU, 16GB MemoryWhere’s the code?Evaluation notebooks for each of the above models are available:voyage-lite-02-instructtext-embedding-3-largeUAE-Large-V1To run a\\n\\n\"fields\": [\\n    {\\n      \"numDimensions\": 1536,\\n      \"path\": \"embedding\",\\n      \"similarity\": \"cosine\",\\n      \"type\": \"vector\"\\n    }\\n  ]\\n}The number of embedding dimensions in both index definitions is 1536 since ada-002 and 3-small have the same number of dimensions.Step 6: Compare embedding models for retrievalAs a first step in the evaluation process, we want to ensure that we are retrieving the right context for the LLM. While there are several factors (chunking, re-ranking, etc.) that can impact retrieval, in this tutorial, we will only experiment with different embedding models. We will use the same models that we used in Step 5. We will use LangChain to create a vector store using MongoDB Atlas and use it as a retriever in our RAG application.Code Snippetfrom langchain_openai import OpenAIEmbeddings'}, {'role': 'user', 'content': 'What are the important considerations while choosing an embedding model?'}, {'role': 'assistant', 'content': \"  According to the provided context, the important considerations while choosing an embedding model are:\\n\\n1. Size: The model's size (i.e., number of parameters) can affect both inference speed and storage efficiency. A smaller model may be faster but may not capture nuanced details, while a larger model may capture more details but be slower and more resource-intensive.\\n2. Dataset: The choice of embedding model should be informed by the specific dataset being used. Different models may perform better or worse on different datasets, so it's important to evaluate the performance of various models on the dataset at hand.\\n3. Evaluation: It's important to evaluate the performance of the various embedding models being considered using metrics such as retrieval, summarization, and other applicable tasks. This can help identify the best-performing model for the specific use case.\\n4. Trade-off: The choice of embedding model should involve a balance between capturing the complexity of the data and operational efficiency. A model that captures too much detail may be slower and more resource-intensive, while a model that doesn't capture enough detail may not perform well in certain tasks.\\n5. Storage-efficiency: Smaller embeddings are generally more storage-efficient, but may not capture as much detail as larger embeddings.\\n6. Self-reported Benchmarks: The results of benchmarks provided by the model developers should be taken with caution, as they might not accurately reflect the model's performance on the specific dataset being used.\"}, {'role': 'user', 'content': 'Tell me more about embedding size.'}]\n",
      "  In the context of natural language processing and text embedding, the embedding size refers to the number of dimensions or features in the vector representation of a text document. The embedding size is a hyperparameter that can be adjusted during the training process to optimize the performance of the model.\n",
      "\n",
      "A larger embedding size generally allows the model to capture more nuanced details in the text data, such as more subtle patterns and relationships between words. However, a larger embedding size also means that the model requires more computational resources and longer training times to train.\n",
      "\n",
      "On the other hand, a smaller embedding size can result in faster training times and reduced computational resources, but may not capture as much detail in the text data. The optimal embedding size depends on the specific use case and the trade-off between computational resources and model performance.\n",
      "\n",
      "In general, the embedding size is typically in the range of several hundred to a few thousand dimensions, such as 100-500 or 500-1000 dimensions. However, some state-of-the-art models, such as Transformers, use even larger embedding sizes, such as 2000-3000 dimensions or more.\n",
      "\n",
      "The choice of embedding size also has implications for the downstream tasks that use the text embeddings. For example, in language translation, a larger embedding size may be necessary to capture the complex relationships between words and phrases in the target language, while in text classification, a smaller embedding size may be more appropriate to reduce the dimensionality of the input data and improve the model's performance.\n",
      "\n",
      "In summary, the embedding size is a hyperparameter that can be adjusted during the training process to optimize the performance of the model, and the choice of embedding size depends on the specific use case and the trade-off between computational resources and model performance.\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    generate_answer(\n",
    "        \"2\",\n",
    "        user_query=\"Tell me more about embedding size.\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': 'Answer the question based only on the following context. If the context is empty, say I DON\\'T KNOW\\n\\nContext:\\nQuestion: {question}\\n    \"\"\"\\n    # Defining the chat prompt\\n    prompt = ChatPromptTemplate.from_template(template)\\n    # Defining the model to be used for chat completion\\n    llm = ChatOpenAI(temperature=0, model=model)\\n    # Parse output as a string\\n    parse_output = StrOutputParser()\\n\\ntypes of questions to include in the test set: simple corresponds to straightforward questions that can be easily answered using the source data. multi_context stands for questions that would require information from multiple related sections or chunks to formulate an answer. reasoning questions are those that would require an LLM to reason to effectively answer the question. You will want to set this distribution based on your best guess of the type of questions you would expect from your users.Generates a test set from the dataset (pages) we created in Step 3. We use the generate_with_langchain_docs method since our dataset is a list of LangChain documents.Ragas also supports test set generation using LlamaIndex. Refer to their documentation for more information.Step 6: Evaluate chunking strategiesNow that we have defined our chunking functions and created our evaluation dataset, we are ready to evaluate the different chunking strategies to find the one that gives us the best retrieval quality on our evaluation dataset.Let’s start by creating a MongoDB collection\\n\\ndef get_rag_chain(retriever: VectorStoreRetriever, model: str) -> RunnableSequence:\\n    \"\"\"\\n    Create a basic RAG chain\\n\\n    Args:\\n        retriever (VectorStoreRetriever): Vector store retriever object\\n        model (str): Chat completion model to use\\n\\n    Returns:\\n        RunnableSequence: A RAG chain\\n    \"\"\"\\n    # Generate context using the retriever, and pass the user question through\\n    retrieve = {\\n        \"context\": retriever\\n        | (lambda docs: \"\\\\n\\\\n\".join([d.page_content for d in docs])),\\n        \"question\": RunnablePassthrough(),\\n    }\\n    template = \"\"\"Answer the question based only on the following context: \\\\\\n    {context}\\n\\nThanks for making my work a lot easier. your post are appreciated.\\nBest Regard\\n\\nand weaknesses of your system. In addition to curating a dataset of questions, you may also want to write out ground truth answers to the questions. While these are especially important for tasks like query generation that have a definitive right or wrong answer, they can also be useful for grounding LLMs when using them as a judge for evaluation.As with any software, you will want to evaluate each component separately and the system as a whole. In RAG systems, for example, you will want to evaluate the retrieval and generation to ensure that you are retrieving the right context and generating suitable answers, whereas in tool-calling agents, you will want to validate the intermediate responses from each of the tools. You will also want to evaluate the overall system for correctness, typically done by comparing the final answer to the ground truth answer.Finally, think about how you will collect feedback from your users, incorporate it into your evaluation pipeline, and track the performance of your application over time.RAG — a very quick'}, {'role': 'user', 'content': 'What are the important considerations while choosing an embedding model?'}, {'role': 'assistant', 'content': \"  According to the provided context, the important considerations while choosing an embedding model are:\\n\\n1. Size: The model's size (i.e., number of parameters) can affect both inference speed and storage efficiency. A smaller model may be faster but may not capture nuanced details, while a larger model may capture more details but be slower and more resource-intensive.\\n2. Dataset: The choice of embedding model should be informed by the specific dataset being used. Different models may perform better or worse on different datasets, so it's important to evaluate the performance of various models on the dataset at hand.\\n3. Evaluation: It's important to evaluate the performance of the various embedding models being considered using metrics such as retrieval, summarization, and other applicable tasks. This can help identify the best-performing model for the specific use case.\\n4. Trade-off: The choice of embedding model should involve a balance between capturing the complexity of the data and operational efficiency. A model that captures too much detail may be slower and more resource-intensive, while a model that doesn't capture enough detail may not perform well in certain tasks.\\n5. Storage-efficiency: Smaller embeddings are generally more storage-efficient, but may not capture as much detail as larger embeddings.\\n6. Self-reported Benchmarks: The results of benchmarks provided by the model developers should be taken with caution, as they might not accurately reflect the model's performance on the specific dataset being used.\"}, {'role': 'user', 'content': 'Tell me more about embedding size.'}, {'role': 'assistant', 'content': \"  In the context of natural language processing and text embedding, the embedding size refers to the number of dimensions or features in the vector representation of a text document. The embedding size is a hyperparameter that can be adjusted during the training process to optimize the performance of the model.\\n\\nA larger embedding size generally allows the model to capture more nuanced details in the text data, such as more subtle patterns and relationships between words. However, a larger embedding size also means that the model requires more computational resources and longer training times to train.\\n\\nOn the other hand, a smaller embedding size can result in faster training times and reduced computational resources, but may not capture as much detail in the text data. The optimal embedding size depends on the specific use case and the trade-off between computational resources and model performance.\\n\\nIn general, the embedding size is typically in the range of several hundred to a few thousand dimensions, such as 100-500 or 500-1000 dimensions. However, some state-of-the-art models, such as Transformers, use even larger embedding sizes, such as 2000-3000 dimensions or more.\\n\\nThe choice of embedding size also has implications for the downstream tasks that use the text embeddings. For example, in language translation, a larger embedding size may be necessary to capture the complex relationships between words and phrases in the target language, while in text classification, a smaller embedding size may be more appropriate to reduce the dimensionality of the input data and improve the model's performance.\\n\\nIn summary, the embedding size is a hyperparameter that can be adjusted during the training process to optimize the performance of the model, and the choice of embedding size depends on the specific use case and the trade-off between computational resources and model performance.\"}, {'role': 'user', 'content': 'What did I just ask you?'}]\n",
      "  You asked me: \"Tell me more about embedding size.\"\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    generate_answer(\n",
    "        \"2\",\n",
    "        user_query=\"What did I just ask you?\",\n",
    "    )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
