{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mongodb-developer/ai-agents-lab-notebooks/blob/main/notebook_template.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Lab Documentation and Solutions](https://img.shields.io/badge/Lab%20Documentation%20and%20Solutions-purple)](https://mongodb-developer.github.io/rag-lab/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Install libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -qU pymongo langchain langchain-community fireworks-ai bs4 tiktoken sentence_transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Setup prerequisites\n",
    "\n",
    "Replace:\n",
    "\n",
    "- `<CODE_BLOCK_1>` with your **MongoDB connection string**\n",
    "- `<CODE_BLOCK_2>` with your **Fireworks API key**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retain the quotes (\"\") when pasting the URI\n",
    "MONGODB_URI = \"<CODE_BLOCK_1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retain the quotes (\"\") when pasting the API key\n",
    "os.environ[\"FIREWORKS_API_KEY\"] = \"<CODE_BLOCK_2>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Prepare the dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\n",
    "    [\n",
    "        \"https://www.mongodb.com/developer/products/atlas/choose-embedding-model-rag/\",\n",
    "        \"https://www.mongodb.com/developer/products/atlas/evaluate-llm-applications-rag/\",\n",
    "        \"https://www.mongodb.com/developer/products/atlas/choosing-chunking-strategy-rag/\",\n",
    "        \"https://www.mongodb.com/developer/products/atlas/gemma-mongodb-huggingface-rag/\",\n",
    "    ]\n",
    ")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the number of documents created\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'https://www.mongodb.com/developer/products/atlas/choose-embedding-model-rag/', 'title': 'How to Choose the Right Embedding Model for Your LLM Application | MongoDB', 'description': 'In this tutorial, we will see why embeddings are important for RAG, and how to choose the right embedding model for your RAG application.', 'language': 'en'}, page_content='How to Choose the Right Embedding Model for Your LLM Application | MongoDBBlogAtlas Vector Search voted most loved vector database in 2024 Retool State of AI reportLearn more\\xa0>>Developer Articles & TopicsGeneral InformationDocumentationDeveloper Articles & TopicsCommunity ForumsBlogUniversityProductsPlatformAtlasBuild on a developer data platformPlatform ServicesDatabaseDeploy a multi-cloud databaseSearchDeliver engaging search experiencesVector SearchDesign intelligent apps with GenAIStream ProcessingUnify data in motion and data at restToolsCompassWork with MongoDB data in a GUIIntegrationsIntegrations with third-party servicesRelational MigratorMigrate to MongoDB with confidenceSelf ManagedEnterprise AdvancedRun and manage MongoDB yourselfCommunity EditionDevelop locally with MongoDBBuild with MongoDB AtlasGet started for free in minutesSign UpTest Enterprise AdvancedDevelop with MongoDB on-premisesDownloadTry Community EditionExplore the latest version of MongoDBDownloadResourcesDocumentationAtlas DocumentationGet started using AtlasServer DocumentationLearn to use MongoDBStart With GuidesGet step-by-step guidance for key tasks Tools and ConnectorsLearn how to connect to MongoDBMongoDB DriversUse drivers and libraries for MongoDBAI Resources HubGet help building the next big thing in AI with MongoDBarrow-rightConnectDeveloper CenterExplore a wide range of developer resourcesCommunityJoin a global community of developersCourses and CertificationLearn for free from MongoDBWebinars and EventsFind a webinar or event near youSolutionsUse casesArtificial IntelligenceEdge ComputingInternet of ThingsMobilePaymentsServerless DevelopmentIndustriesFinancial ServicesTelecommunicationsHealthcareRetailPublic SectorManufacturingSolutions LibraryOrganized and tailored solutions to kick-start projectsarrow-rightDeveloper Data PlatformAccelerate innovation at scaleLearn morearrow-rightStartups and AI InnovatorsFor world-changing ideas and AI pioneersLearn morearrow-rightCustomer Case StudiesHear directly from our usersSee Storiesarrow-rightCompanyCareersStart your next adventureBlogRead articles and announcementsNewsroomRead press releases and news storiesPartnersLearn about our partner ecosystemLeadershipMeet our executive teamCompanyLearn more about who we areContact UsReach out to MongoDBLet’s chatarrow-rightInvestorsVisit our investor portalLearn morearrow-rightPricingSupportSign InTry Freemenu-verticalMongoDB Developerchevron-downTopicsLanguagesplusTechnologiesplusProductsplusExpertise LevelsplusAll TopicsDocumentationArticlesTutorialsEventsCode ExamplesPodcastsMongoDB TVMongoDB DeveloperTopicschevron-downDocumentationArticlesTutorialsEventsCode ExamplesPodcastsMongoDB TVcloseAtlasplusSign in to follow topicsArticlesCode ExamplesDocumentationexternalEventsNews & AnnouncementsPodcastsQuickstartsTutorialsVideosMongoDB Developer Centerchevron-rightDeveloper Topicschevron-rightProductschevron-rightAtlaschevron-rightTutorialsHow to Choose the Right Embedding Model for Your LLM ApplicationApoorva Joshi16 min read • Published Jun 17, 2024 • Updated Jun 17, 2024AIPythonAtlasRate this tutorialIf you are building Generative AI (GenAI) applications in 2024, you’ve probably heard the term “embeddings” a few times by now and are seeing new embedding models hit the shelf every week. So why do so many people suddenly care about embeddings, a concept that has existed since the 1950s? And if embeddings are so important and you must use them, how do you choose among the vast number of options out there?This tutorial will cover the following:What are embeddings?Importance of embeddings in RAG applicationsHow to choose the right embedding model for your RAG applicationEvaluating embedding modelsThis tutorial is Part 1 of a multi-part series on Retrieval Augmented Generation (RAG), where we start with the fundamentals of building a RAG application, and work our way to more advanced techniques for RAG. The series will cover the following:Part 1: How to Choose the Right Embedding Model for Your LLM ApplicationPart 2: How to Evaluate Your LLM ApplicationPart 3: How to Choose the Right Chunking Strategy for Your LLM ApplicationPart 4: Improving RAG using metadata extraction and filteringWhat are embeddings and embedding models?An embedding is an array of numbers (a vector) representing a piece of information, such as text, images, audio, video, etc. Together, these numbers capture semantics and other important features of the data. The immediate consequence of doing this is that semantically similar entities map close to each other while dissimilar entities map farther apart in the vector space. For clarity, see the image below for a depiction of a high-dimensional vector space:In the context of natural language processing (NLP), embedding models are algorithms designed to learn and generate embeddings for a given piece of information. In today’s AI applications, embeddings are typically created using large language models (LLMs) that are trained on a massive corpus of data and use cutting-edge algorithms to learn complex semantic relationships in the data.What is RAG (briefly)Retrieval Augmented Generation, as the name suggests, aims to improve the quality of pre-trained LLM generation using data retrieved from a knowledge base. The success of RAG lies in retrieving the most relevant results from the knowledge base. This is where embeddings come into the picture. A RAG pipeline looks something like this:In the above pipeline, we see a common approach used for retrieval in GenAI applications — i.e., semantic search. In this technique, an embedding model is used to create vector representations of the user query and of information in the knowledge base. This way, given a user query and its embedding, we can retrieve the most relevant source documents from the knowledge base based on how similar their embeddings are to the query embedding. The retrieved documents, user query, and any user prompts are then passed as context to an LLM, to generate an answer to the user’s question.Choosing the right embedding model for your RAG applicationAs we have seen above, embeddings are central to RAG. But with so many models out there, how do we choose the best one for our use case?A good place to start when looking for embedding models to use is the MTEB Leaderboard on Hugging Face. It is the most up-to-date list of proprietary and open-source text embedding models, accompanied by statistics on how each model performs on various embedding tasks such as retrieval, summarization, etc.Evaluations of this magnitude for multimodal models are just emerging (see the MME benchmark) so we will only focus on text embedding models for this tutorial. However, all the guidance here on choosing an embedding model also applies to multimodal models.Benchmarks are a good place to begin but bear in mind that these results are self-reported and have been benchmarked on datasets that might not accurately represent the data you are dealing with. It is also possible that some models may include the MTEB datasets in their training data since they are publicly available. So even if you choose a model based on benchmark results, we recommend evaluating it on your dataset. We will see how to do this later in the tutorial, but first, let’s take a closer look at the leaderboard.Here’s a snapshot of the top 10 models on the leaderboard currently:Let’s look at the Overall tab since it provides a comprehensive summary of each model. However, note that we have sorted the leaderboard by the Retrieval Average column. This is because RAG is a retrieval\\xa0task and we want to see the best retrieval models at the top. We will ignore columns corresponding to other tasks, and focus on the following columns:Retrieval Average: Represents average Normalized Discounted Cumulative Gain (NDCG) @ 10 across several datasets. NDCG is a common metric to measure the performance of retrieval systems. A higher NDCG indicates a model that is better at ranking relevant items higher in the list of retrieved results.\\xa0Model Size: Size of the model (in GB). It gives an idea of the computational resources required to run the model. While retrieval performance scales with model size, it is important to note that model size also has a direct impact on latency. The latency-performance trade-off becomes especially important in a production setup.\\xa0\\xa0Max Tokens: Number of tokens that can be compressed into a single embedding. You typically don’t want to put more than a single paragraph of text (~100 tokens) into a single embedding. So even models with max tokens of 512 should be more than enough.Embedding Dimensions: Length of the embedding vector. Smaller embeddings offer faster inference and are more storage-efficient, while more dimensions can capture nuanced details and relationships in the data. Ultimately, we want a good trade-off between capturing the complexity of data and operational efficiency.The top 10 models on the leaderboard contain a mix of small vs large and proprietary vs open-source models. Let’s compare some of these to find the best embedding model for our dataset.Before we beginHere are some things to note about our evaluation experiment.DatasetMongoDB’s cosmopedia-wikihow-chunked dataset is available on Hugging Face, which consists of prechunked WikiHow-style articles.Models evaluatedvoyage-lite-02-instruct: A proprietary embedding model from VoyageAItext-embedding-3-large: One of OpenAI’s latest proprietary embedding modelsUAE-Large-V1: A small-ish (335M parameters) open-source embedding modelWe also attempted to evaluate SFR-Embedding-Mistral, currently the #1 model on the MTEB leaderboard, but the hardware below was not sufficient to run this model. This model and other 14+ GB models on the leaderboard will likely require a/multiple GPU(s) with at least 32 GB of total memory, which means higher costs and/or getting into distributed inference. While we haven’t evaluated this model in our experiment, this is already a good data point when thinking about cost and resources.Evaluation metricsWe used the following metrics to evaluate embedding performance:Embedding latency: Time taken to create embeddingsRetrieval quality: Relevance of retrieved documents to the user queryHardware used1 NVIDIA T4 GPU, 16GB MemoryWhere’s the code?Evaluation notebooks for each of the above models are available:voyage-lite-02-instructtext-embedding-3-largeUAE-Large-V1To run a notebook, click on the Open in Colab shield at the top of the notebook. The notebook will open in Google Colaboratory.Click the Connect button on the top right corner to connect to a hosted runtime environment.Once connected, you can also change the runtime type to use the T4 GPUs available for free on Google Colab.Step 1: Install the required librariesThe libraries required for each model differ slightly, but the common ones are as follows:datasets: Python library to get access to datasets available on Hugging Face Hubsentence-transformers: Framework for working with text and image embeddingsnumpy: Python library that provides tools to perform mathematical operations on arrayspandas: Python library for data analysis, exploration, and manipulationtdqm: Python module to show a progress meter for loopsCode Snippet! pip install -qU datasets sentence-transformers numpy pandas tqdmAdditionally for Voyage AI:\\nvoyageai: Python library to interact with OpenAI APIsCode Snippet! pip install -qU voyageaiAdditionally for OpenAI:\\nopenai: Python library to interact with OpenAI APIsCode Snippet! pip install -qU openaiAdditionally for UAE:\\ntransformers: Python library that provides APIs to interact with pre-trained models available on Hugging FaceCode Snippet! pip install -qU transformersStep 2: Setup pre-requisitesOpenAI and Voyage AI models are available via APIs. So you’ll need to obtain API keys and make them available to the respective clients.Code Snippetimport os\\nimport getpassInitialize Voyage AI client:Code Snippetimport voyageai\\nVOYAGE_API_KEY = getpass.getpass(\"Voyage API Key:\")\\nvoyage_client = voyageai.Client(api_key=VOYAGE_API_KEY)Initialize OpenAI client:Code Snippetfrom openai import OpenAI\\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\\nopenai_client = OpenAI()Step 3: Download the evaluation datasetAs mentioned previously, we will use MongoDB’s cosmopedia-wikihow-chunked dataset. The dataset is quite large (1M+ documents). So we will stream it and grab the first 25k records, instead of downloading the entire dataset to disk.Code Snippetfrom datasets import load_dataset\\nimport pandas as pd\\n\\n# Use streaming=True to load the dataset without downloading it fully\\ndata = load_dataset(\"MongoDB/cosmopedia-wikihow-chunked\", split=\"train\", streaming=True)\\n# Get first 25k records from the dataset\\ndata_head = data.take(25000)\\ndf = pd.DataFrame(data_head)\\n\\n# Use this if you want the full dataset\\n# data = load_dataset(\"MongoDB/cosmopedia-wikihow-chunked\", split=\"train\")\\n# df = pd.DataFrame(data)Step 4: Data analysisNow that we have our dataset, let’s perform some simple data analysis and run some sanity checks on our data to ensure that we don’t see any obvious errors:Code Snippet# Ensuring length of dataset is what we expect i.e. 25k\\nlen(df)\\n\\n# Previewing the contents of the data\\ndf.head()\\n\\n# Only keep records where the text field is not null\\ndf = df[df[\"text\"].notna()]\\n\\n# Number of unique documents in the dataset\\ndf.doc_id.nunique()Step 5: Create embeddingsNow, let’s create embedding functions for each of our models.For voyage-lite-02-instruct:Code Snippetdef get_embeddings(docs: List[str], input_type: str, model:str=\"voyage-lite-02-instruct\") -> List[List[float]]:\\n    \"\"\"\\n    Get embeddings using the Voyage AI API.\\n\\n    Args:\\n        docs (List[str]): List of texts to embed\\n        input_type (str): Type of input to embed. Can be \"document\" or \"query\".\\n        model (str, optional): Model name. Defaults to \"voyage-lite-02-instruct\".\\n\\n    Returns:\\n        List[List[float]]: Array of embedddings\\n    \"\"\"\\n    response = voyage_client.embed(docs, model=model, input_type=input_type)\\n    return response.embeddingsThe embedding function above takes a list of texts (docs) and an input_type as arguments and returns a list of embeddings. The input_type can be document or query depending on whether we are embedding a list of documents or user queries. Voyage uses this value to prepend the inputs with special prompts to enhance retrieval quality.For text-embedding-3-large:Code Snippetdef get_embeddings(docs: List[str], model: str=\"text-embedding-3-large\") -> List[List[float]]:\\n    \"\"\"\\n    Generate embeddings using the OpenAI API.\\n\\n    Args:\\n        docs (List[str]): List of texts to embed\\n        model (str, optional): Model name. Defaults to \"text-embedding-3-large\".\\n\\n    Returns:\\n        List[float]: Array of embeddings\\n    \"\"\"\\n    # replace newlines, which can negatively affect performance.\\n    docs = [doc.replace(\"\\\\n\", \" \") for doc in docs]\\n    response = openai_client.embeddings.create(input=docs, model=model)\\n    response = [r.embedding for r in response.data]\\n    return responseThe embedding function for the OpenAI model is similar to the previous one, with some key differences — there is no input_type argument, and the API returns a list of embedding objects, which need to be parsed to get the final list of embeddings. A sample response from the API looks as follows:Code Snippet{\\n  \"data\": [\\n    {\\n      \"embedding\": [\\n        0.018429679796099663,\\n        -0.009457024745643139\\n    .\\n    .\\n    .\\n      ],\\n      \"index\": 0,\\n      \"object\": \"embedding\"\\n    }\\n  ],\\n  \"model\": \"text-embedding-3-large\",\\n  \"object\": \"list\",\\n  \"usage\": {\\n    \"prompt_tokens\": 183,\\n    \"total_tokens\": 183\\n  }\\n}For UAE-large-V1:Code Snippetfrom typing import List\\nfrom transformers import AutoModel, AutoTokenizer\\nimport torch\\n\\n# Instruction to append to user queries, to improve retrieval\\nRETRIEVAL_INSTRUCT = \"Represent this sentence for searching relevant passages:\"\\n\\n# Check if CUDA (GPU support) is available, and set the device accordingly\\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\\n# Load the UAE-Large-V1 model from the Hugging Face \\nmodel = AutoModel.from_pretrained(\\'WhereIsAI/UAE-Large-V1\\').to(device)\\n# Load the tokenizer associated with the UAE-Large-V1 model\\ntokenizer = AutoTokenizer.from_pretrained(\\'WhereIsAI/UAE-Large-V1\\')\\n\\n# Decorator to disable gradient calculations\\n@torch.no_grad()\\ndef get_embeddings(docs: List[str], input_type: str) -> List[List[float]]:\\n    \"\"\"\\n    Get embeddings using the UAE-Large-V1 model.\\n\\n    Args:\\n        docs (List[str]): List of texts to embed\\n        input_type (str): Type of input to embed. Can be \"document\" or \"query\".\\n\\n    Returns:\\n        List[List[float]]: Array of embedddings\\n    \"\"\"\\n    # Prepend retrieval instruction to queries\\n    if input_type == \"query\":\\n        docs = [\"{}{}\".format(RETRIEVAL_INSTRUCT, q) for q in docs]\\n    # Tokenize input texts\\n    inputs = tokenizer(docs, padding=True, truncation=True, return_tensors=\\'pt\\', max_length=512).to(device)\\n    # Pass tokenized inputs to the model, and obtain the last hidden state\\n    last_hidden_state = model(**inputs, return_dict=True).last_hidden_state\\n    # Extract embeddings from the last hidden state\\n    embeddings = last_hidden_state[:, 0]\\n    return embeddings.cpu().numpy()The UAE-Large-V1 model is an open-source model available on Hugging Face Model Hub. First, we will need to download the model and its tokenizer from Hugging Face. We do this using the Auto classes — namely, AutoModel and AutoTokenizer from the Transformers library  — which automatically infers the underlying model architecture, in this case, BERT. Next, we load the model onto the GPU using .to(device) since we have one available.The embedding function for the UAE model, much like the Voyage model, takes a list of texts (docs) and an input_type as arguments and returns a list of embeddings. A special prompt is prepended to queries for better retrieval as well.The input texts are first tokenized, which includes padding (for short sequences) and truncation (for long sequences)  as needed to ensure that the length of inputs to the model is consistent — 512, in this case, defined by the max_length parameter. The pt value for return_tensors indicates that the output of tokenization should be PyTorch tensors.The tokenized texts are then passed to the model for inference and the last hidden layer (last_hidden_state) is extracted. This layer is the model’s final learned representation of the entire input sequence. The final embedding, however, is extracted only from the first token, which is often a special token ([CLS] in BERT) in transformer-based models. This token serves as an aggregate representation of the entire sequence due to the self-attention mechanism in transformers, where the representation of each token in a sequence is influenced by all other tokens. Finally, we move the embeddings back to CPU using .cpu() and convert the PyTorch tensors to numpy arrays using .numpy().Step 6: EvaluationAs mentioned previously, we will evaluate the models based on embedding latency and retrieval quality.Measuring embedding latencyTo measure embedding latency, we will create a local vector store, which is essentially a list of embeddings for the entire dataset. Latency here is defined as the time it takes to create embeddings for the full dataset.Code Snippetfrom tqdm.auto import tqdm\\n\\n# Get all the texts in the dataset\\ntexts = df[\"text\"].tolist()\\n\\n# Number of samples in a single batch\\nbatch_size = 128\\n\\nembeddings = []\\n# Generate embeddings in batches\\nfor i in tqdm(range(0, len(texts), batch_size)):\\n    end = min(len(texts), i+batch_size)\\n    batch = texts[i:end]\\n    # Generate embeddings for current batch\\n    batch_embeddings = get_embeddings(batch)\\n    # Add to the list of embeddings\\n    embeddings.extend(batch_embeddings)We first create a list of all the texts we want to embed and set the batch size. The voyage-lite-02-instruct model has a batch size limit of 128, so we use the same for all models, for consistency. We iterate through the list of texts, grabbing batch_size number of samples in each iteration, getting embeddings for the batch, and adding them to our \"vector store\".The time taken to generate embeddings on our hardware looked as follows:ModelBatch SizeDimensionsTimetext-embedding-3-large12830724m 17svoyage-lite-02-instruct128102411m 14sUAE-large-V1128102419m 50sThe OpenAI model has the lowest latency. However, note that it also has three times the number of embedding dimensions compared to the other two models. OpenAI also charges by tokens used, so both the storage and inference costs of this model can add up over time. While the UAE model is the slowest of the lot (despite running inference on a GPU), there is room for optimizations such as quantization, distillation, etc., since it is open-source.Measuring retrieval qualityTo evaluate retrieval quality, we use a set of questions based on themes seen in our dataset. For real applications, however, you will want to curate a set of \"cannot-miss\" questions — i.e. questions that you would typically expect users to ask from your data. For this tutorial, we will qualitatively evaluate the relevance of retrieved documents as a measure of quality, but we will explore metrics and techniques for quantitative evaluations in a following tutorial.Here are the main themes (generated using ChatGPT) covered by the top three documents retrieved by each model for our queries:😐 denotes documents that we felt weren’t as relevant to the question. Sentences that contributed to this verdict have been highlighted in bold.Query: Give me some tips to improve my mental health.voyage-lite-02-instructtext-embedding-3-largeUAE-large-V1😐 Regularly reassess treatment efficacy and modify plans as needed. Track mood, thoughts, and behaviors; share updates with therapists and support network. Use a multifaceted approach to manage suicidal thoughts, involving resources, skills, and connections.Eat balanced, exercise, sleep well. Cultivate relationships, engage socially, set boundaries. Manage stress with effective coping mechanisms.Prioritizing mental health is essential, not selfish. Practice mindfulness through meditation, journaling, and activities like yoga. Adopt healthy habits for better mood, less anxiety, and improved cognition.Recognize early signs of stress, share concerns, and develop coping mechanisms. Combat isolation by nurturing relationships and engaging in social activities. Set boundaries, communicate openly, and seek professional help for social anxiety.Prioritizing mental health is essential, not selfish. Practice mindfulness through meditation, journaling, and activities like yoga. Adopt healthy habits for better mood, less anxiety, and improved cognition.Eat balanced, exercise regularly, get 7-9 hours of sleep. Cultivate positive relationships, nurture friendships, and seek new social opportunities. Manage stress with effective coping mechanisms.Prioritizing mental health is essential, not selfish. Practice mindfulness through meditation, journaling, and activities like yoga. Adopt healthy habits for better mood, less anxiety, and improved cognition.Acknowledging feelings is a step to address them. Engage in self-care activities to boost mood and health. Make self-care consistent for lasting benefits.😐 Taking care of your mental health is crucial for a fulfilling life, productivity, and strong relationships. Recognize the importance of mental health in all aspects of life. Managing mental health reduces the risk of severe psychological conditions.While the results cover similar themes, the Voyage AI model keys in heavily on seeking professional help, while the UAE model covers slightly more about why taking care of your mental health is important. The OpenAI model is the one that consistently retrieves documents that cover general tips for improving mental health.Query: Give me some tips for writing good code.voyage-lite-02-instructtext-embedding-3-largeUAE-large-V1Strive for clean, maintainable code with consistent conventions and version control. Utilize linters, static analyzers, and document work for quality and collaboration. Embrace best practices like SOLID and TDD to enhance design, scalability, and extensibility.Strive for clean, maintainable code with consistent conventions and version control. Utilize linters, static analyzers, and document work for quality and collaboration. Embrace best practices like SOLID and TDD to enhance design, scalability, and extensibility.Strive for clean, maintainable code with consistent conventions and version control. Utilize linters, static analyzers, and document work for quality and collaboration. Embrace best practices like SOLID and TDD to enhance design, scalability, and extensibility.😐 Code and test core gameplay mechanics like combat and quest systems; debug and refine for stability. Use modular coding, version control, and object-oriented principles for effective game development. Playtest frequently to find and fix bugs, seek feedback, and prioritize significant improvements.😐 Good programming needs dedication, persistence, and patience. Master core concepts, practice diligently, and engage with peers for improvement. Every expert was once a beginner—keep pushing forward.Read programming books for comprehensive coverage and deep insights, choosing beginner-friendly texts with pathways to proficiency. Combine reading with coding to reinforce learning; take notes on critical points and unfamiliar terms. Engage with exercises and challenges in books to apply concepts and enhance skills.😐 Monitor social media and newsletters for current software testing insights. Participate in networks and forums to exchange knowledge with experienced testers. Regularly update your testing tools and methods for enhanced efficiency.Apply learning by working on real projects, starting small and progressing to larger ones. Participate in open-source projects or develop your applications to enhance problem-solving. Master debugging with IDEs, print statements, and understanding common errors for productivity.😐 Programming is key in various industries, offering diverse opportunities. This guide covers programming fundamentals, best practices, and improvement strategies. Choose a programming language based on interests, goals, and resources.All the models seem to struggle a bit with this question. They all retrieve at least one document that is not as relevant to the question. However, it is interesting to note that all the models retrieve the same document as their number one.Query: What are some environment-friendly practices I can incorporate in everyday life?voyage-lite-02-instructtext-embedding-3-largeUAE-large-V1😐 Conserve resources by reducing waste, reusing, and recycling, reflecting Jawa culture\\'s values due to their planet\\'s limited resources. Monitor consumption (e.g., water, electricity), repair goods, and join local environmental efforts. Eco-friendly practices enhance personal and global well-being, aligning with Jawa values.Carry reusable bags for shopping, keeping extras in your car or bag. Choose sustainable alternatives like reusable water bottles and eco-friendly cutlery. Support businesses that minimize packaging and use biodegradable materials.Educate others on eco-friendly practices; lead by example. Host workshops or discussion groups on sustainable living.Embody respect for the planet; every effort counts towards improvement.Learn and follow local recycling rules, rinse containers, and educate others on proper recycling. Opt for green transportation like walking, cycling, or electric vehicles, and check for incentives. Upgrade to energy-efficient options like LED lights, seal drafts, and consider renewable energy sources.Opt for sustainable transportation, energy-efficient appliances, solar panels, and eat less meat to reduce emissions. Conserve water by fixing leaks, taking shorter showers, and using low-flow fixtures. Water conservation protects ecosystems, ensures food security, and reduces infrastructure stress.Carry reusable bags for shopping, keeping extras in your car or bag. Choose sustainable alternatives like reusable water bottles and eco-friendly cutlery. Support businesses that minimize packaging and use biodegradable materials.😐 Consistently implement these steps. Actively contribute to a cleaner, greener world. Support resilience for future generations.Conserve water with low-flow fixtures, fix leaks, and use rainwater for gardening. Compost kitchen scraps to reduce waste and enrich soil, avoid meat and dairy. Shop locally at farmers markets and CSAs to lower emissions and support local economies.Join local tree-planting events and volunteer at community gardens or restoration projects. Integrate native plants into landscaping to support pollinators and remove invasive species. Adopt eco-friendly transportation methods to decrease fossil fuel consumption.We see a similar trend with this query as with the previous two examples — the OpenAI model consistently retrieves documents that provide the most actionable tips, followed by the UAE model. The Voyage model provides more high-level advice.Overall, based on our preliminary evaluation, OpenAI’s text-embedding-3-large model comes out on top. When working with real-world systems, however, a more rigorous evaluation of a larger dataset is recommended. Also, operational costs become an important consideration. More on evaluation coming in Part 2 of this series!ConclusionIn this tutorial, we looked into how to choose the right model to embed data for RAG. The MTEB leaderboard is a good place to start, especially for text embedding models, but evaluating them on your data is important to find the best one for your RAG application. Storage and inference costs, embedding latency, and retrieval quality are all important parameters to consider while evaluating embedding models. The best model is typically one that offers the best trade-off across these dimensions.Now that you have a good understanding of embedding models, here are some resources to get started with building RAG applications using MongoDB:Using Latest OpenAI Embeddings in a RAG System With MongoDBBuilding a RAG System With Google’s Gemma, Hugging Face, and MongoDBHow to Build a RAG System With LlamaIndex, OpenAI, and MongoDBFollow along with these by creating a free MongoDB Atlas cluster and reach out to us in our Generative AI community forums if you have any questions.Top Comments in ForumsThere are no comments on this article yet.Start the ConversationRate this tutorialRelatedQuickstartGetting Started with Deno & MongoDB May 22, 2024 | 12 min readTutorialBeyond Basics: Enhancing Kotlin Ktor API With Vector Search Mar 25, 2024 | 9 min readTutorialNairobi Stock Exchange Web Scraper Apr 02, 2024 | 20 min readTutorialBuilding AI Applications with Microsoft Semantic Kernel and MongoDB Atlas Vector Search Nov 27, 2023 | 8 min readRequest a TutorialTable of ContentsWhat are embeddings and embedding models?What is RAG (briefly)Choosing the right embedding model for your RAG applicationStep 1: Install the required librariesStep 2: Setup pre-requisitesStep 3: Download the evaluation datasetStep 4: Data analysisStep 5: Create embeddingsStep 6: EvaluationConclusionEnglishEnglishPortuguêsEspañol한국어日本語ItalianoDeutschFrançais简体中文© 2024 MongoDB, Inc.AboutCareersInvestor RelationsLegal NoticesPrivacy NoticesSecurity InformationTrust CenterSupportContact UsCustomer PortalAtlas StatusCustomer SupportSocialGitHubStack OverflowLinkedInYouTubeXTwitchFacebook© 2024 MongoDB, Inc.')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview a document\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How to Choose the Right Embedding Model for Your LLM Application | MongoDBBlogAtlas Vector Search voted most loved vector database in 2024 Retool State of AI reportLearn more\\xa0>>Developer Articles & TopicsGeneral InformationDocumentationDeveloper Articles & TopicsCommunity ForumsBlogUniversityProductsPlatformAtlasBuild on a developer data platformPlatform ServicesDatabaseDeploy a multi-cloud databaseSearchDeliver engaging search experiencesVector SearchDesign intelligent apps with GenAIStream ProcessingUnify data in motion and data at restToolsCompassWork with MongoDB data in a GUIIntegrationsIntegrations with third-party servicesRelational MigratorMigrate to MongoDB with confidenceSelf ManagedEnterprise AdvancedRun and manage MongoDB yourselfCommunity EditionDevelop locally with MongoDBBuild with MongoDB AtlasGet started for free in minutesSign UpTest Enterprise AdvancedDevelop with MongoDB on-premisesDownloadTry Community EditionExplore the latest version of MongoDBDownloadResourcesDocumentationAtlas DocumentationGet started using AtlasServer DocumentationLearn to use MongoDBStart With GuidesGet step-by-step guidance for key tasks Tools and ConnectorsLearn how to connect to MongoDBMongoDB DriversUse drivers and libraries for MongoDBAI Resources HubGet help building the next big thing in AI with MongoDBarrow-rightConnectDeveloper CenterExplore a wide range of developer resourcesCommunityJoin a global community of developersCourses and CertificationLearn for free from MongoDBWebinars and EventsFind a webinar or event near youSolutionsUse casesArtificial IntelligenceEdge ComputingInternet of ThingsMobilePaymentsServerless DevelopmentIndustriesFinancial ServicesTelecommunicationsHealthcareRetailPublic SectorManufacturingSolutions LibraryOrganized and tailored solutions to kick-start projectsarrow-rightDeveloper Data PlatformAccelerate innovation at scaleLearn morearrow-rightStartups and AI InnovatorsFor world-changing ideas and AI pioneersLearn morearrow-rightCustomer Case StudiesHear directly from our usersSee Storiesarrow-rightCompanyCareersStart your next adventureBlogRead articles and announcementsNewsroomRead press releases and news storiesPartnersLearn about our partner ecosystemLeadershipMeet our executive teamCompanyLearn more about who we areContact UsReach out to MongoDBLet’s chatarrow-rightInvestorsVisit our investor portalLearn morearrow-rightPricingSupportSign InTry Freemenu-verticalMongoDB Developerchevron-downTopicsLanguagesplusTechnologiesplusProductsplusExpertise LevelsplusAll TopicsDocumentationArticlesTutorialsEventsCode ExamplesPodcastsMongoDB TVMongoDB DeveloperTopicschevron-downDocumentationArticlesTutorialsEventsCode ExamplesPodcastsMongoDB TVcloseAtlasplusSign in to follow topicsArticlesCode ExamplesDocumentationexternalEventsNews & AnnouncementsPodcastsQuickstartsTutorialsVideosMongoDB Developer Centerchevron-rightDeveloper Topicschevron-rightProductschevron-rightAtlaschevron-rightTutorialsHow to Choose the Right Embedding Model for Your LLM ApplicationApoorva Joshi16 min read • Published Jun 17, 2024 • Updated Jun 17, 2024AIPythonAtlasRate this tutorialIf you are building Generative AI (GenAI) applications in 2024, you’ve probably heard the term “embeddings” a few times by now and are seeing new embedding models hit the shelf every week. So why do so many people suddenly care about embeddings, a concept that has existed since the 1950s? And if embeddings are so important and you must use them, how do you choose among the vast number of options out there?This tutorial will cover the following:What are embeddings?Importance of embeddings in RAG applicationsHow to choose the right embedding model for your RAG applicationEvaluating embedding modelsThis tutorial is Part 1 of a multi-part series on Retrieval Augmented Generation (RAG), where we start with the fundamentals of building a RAG application, and work our way to more advanced techniques for RAG. The series will cover the following:Part 1: How to Choose the Right Embedding Model for Your LLM ApplicationPart 2: How to Evaluate Your LLM ApplicationPart 3: How to Choose the Right Chunking Strategy for Your LLM ApplicationPart 4: Improving RAG using metadata extraction and filteringWhat are embeddings and embedding models?An embedding is an array of numbers (a vector) representing a piece of information, such as text, images, audio, video, etc. Together, these numbers capture semantics and other important features of the data. The immediate consequence of doing this is that semantically similar entities map close to each other while dissimilar entities map farther apart in the vector space. For clarity, see the image below for a depiction of a high-dimensional vector space:In the context of natural language processing (NLP), embedding models are algorithms designed to learn and generate embeddings for a given piece of information. In today’s AI applications, embeddings are typically created using large language models (LLMs) that are trained on a massive corpus of data and use cutting-edge algorithms to learn complex semantic relationships in the data.What is RAG (briefly)Retrieval Augmented Generation, as the name suggests, aims to improve the quality of pre-trained LLM generation using data retrieved from a knowledge base. The success of RAG lies in retrieving the most relevant results from the knowledge base. This is where embeddings come into the picture. A RAG pipeline looks something like this:In the above pipeline, we see a common approach used for retrieval in GenAI applications — i.e., semantic search. In this technique, an embedding model is used to create vector representations of the user query and of information in the knowledge base. This way, given a user query and its embedding, we can retrieve the most relevant source documents from the knowledge base based on how similar their embeddings are to the query embedding. The retrieved documents, user query, and any user prompts are then passed as context to an LLM, to generate an answer to the user’s question.Choosing the right embedding model for your RAG applicationAs we have seen above, embeddings are central to RAG. But with so many models out there, how do we choose the best one for our use case?A good place to start when looking for embedding models to use is the MTEB Leaderboard on Hugging Face. It is the most up-to-date list of proprietary and open-source text embedding models, accompanied by statistics on how each model performs on various embedding tasks such as retrieval, summarization, etc.Evaluations of this magnitude for multimodal models are just emerging (see the MME benchmark) so we will only focus on text embedding models for this tutorial. However, all the guidance here on choosing an embedding model also applies to multimodal models.Benchmarks are a good place to begin but bear in mind that these results are self-reported and have been benchmarked on datasets that might not accurately represent the data you are dealing with. It is also possible that some models may include the MTEB datasets in their training data since they are publicly available. So even if you choose a model based on benchmark results, we recommend evaluating it on your dataset. We will see how to do this later in the tutorial, but first, let’s take a closer look at the leaderboard.Here’s a snapshot of the top 10 models on the leaderboard currently:Let’s look at the Overall tab since it provides a comprehensive summary of each model. However, note that we have sorted the leaderboard by the Retrieval Average column. This is because RAG is a retrieval\\xa0task and we want to see the best retrieval models at the top. We will ignore columns corresponding to other tasks, and focus on the following columns:Retrieval Average: Represents average Normalized Discounted Cumulative Gain (NDCG) @ 10 across several datasets. NDCG is a common metric to measure the performance of retrieval systems. A higher NDCG indicates a model that is better at ranking relevant items higher in the list of retrieved results.\\xa0Model Size: Size of the model (in GB). It gives an idea of the computational resources required to run the model. While retrieval performance scales with model size, it is important to note that model size also has a direct impact on latency. The latency-performance trade-off becomes especially important in a production setup.\\xa0\\xa0Max Tokens: Number of tokens that can be compressed into a single embedding. You typically don’t want to put more than a single paragraph of text (~100 tokens) into a single embedding. So even models with max tokens of 512 should be more than enough.Embedding Dimensions: Length of the embedding vector. Smaller embeddings offer faster inference and are more storage-efficient, while more dimensions can capture nuanced details and relationships in the data. Ultimately, we want a good trade-off between capturing the complexity of data and operational efficiency.The top 10 models on the leaderboard contain a mix of small vs large and proprietary vs open-source models. Let’s compare some of these to find the best embedding model for our dataset.Before we beginHere are some things to note about our evaluation experiment.DatasetMongoDB’s cosmopedia-wikihow-chunked dataset is available on Hugging Face, which consists of prechunked WikiHow-style articles.Models evaluatedvoyage-lite-02-instruct: A proprietary embedding model from VoyageAItext-embedding-3-large: One of OpenAI’s latest proprietary embedding modelsUAE-Large-V1: A small-ish (335M parameters) open-source embedding modelWe also attempted to evaluate SFR-Embedding-Mistral, currently the #1 model on the MTEB leaderboard, but the hardware below was not sufficient to run this model. This model and other 14+ GB models on the leaderboard will likely require a/multiple GPU(s) with at least 32 GB of total memory, which means higher costs and/or getting into distributed inference. While we haven’t evaluated this model in our experiment, this is already a good data point when thinking about cost and resources.Evaluation metricsWe used the following metrics to evaluate embedding performance:Embedding latency: Time taken to create embeddingsRetrieval quality: Relevance of retrieved documents to the user queryHardware used1 NVIDIA T4 GPU, 16GB MemoryWhere’s the code?Evaluation notebooks for each of the above models are available:voyage-lite-02-instructtext-embedding-3-largeUAE-Large-V1To run a notebook, click on the Open in Colab shield at the top of the notebook. The notebook will open in Google Colaboratory.Click the Connect button on the top right corner to connect to a hosted runtime environment.Once connected, you can also change the runtime type to use the T4 GPUs available for free on Google Colab.Step 1: Install the required librariesThe libraries required for each model differ slightly, but the common ones are as follows:datasets: Python library to get access to datasets available on Hugging Face Hubsentence-transformers: Framework for working with text and image embeddingsnumpy: Python library that provides tools to perform mathematical operations on arrayspandas: Python library for data analysis, exploration, and manipulationtdqm: Python module to show a progress meter for loopsCode Snippet! pip install -qU datasets sentence-transformers numpy pandas tqdmAdditionally for Voyage AI:\\nvoyageai: Python library to interact with OpenAI APIsCode Snippet! pip install -qU voyageaiAdditionally for OpenAI:\\nopenai: Python library to interact with OpenAI APIsCode Snippet! pip install -qU openaiAdditionally for UAE:\\ntransformers: Python library that provides APIs to interact with pre-trained models available on Hugging FaceCode Snippet! pip install -qU transformersStep 2: Setup pre-requisitesOpenAI and Voyage AI models are available via APIs. So you’ll need to obtain API keys and make them available to the respective clients.Code Snippetimport os\\nimport getpassInitialize Voyage AI client:Code Snippetimport voyageai\\nVOYAGE_API_KEY = getpass.getpass(\"Voyage API Key:\")\\nvoyage_client = voyageai.Client(api_key=VOYAGE_API_KEY)Initialize OpenAI client:Code Snippetfrom openai import OpenAI\\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\\nopenai_client = OpenAI()Step 3: Download the evaluation datasetAs mentioned previously, we will use MongoDB’s cosmopedia-wikihow-chunked dataset. The dataset is quite large (1M+ documents). So we will stream it and grab the first 25k records, instead of downloading the entire dataset to disk.Code Snippetfrom datasets import load_dataset\\nimport pandas as pd\\n\\n# Use streaming=True to load the dataset without downloading it fully\\ndata = load_dataset(\"MongoDB/cosmopedia-wikihow-chunked\", split=\"train\", streaming=True)\\n# Get first 25k records from the dataset\\ndata_head = data.take(25000)\\ndf = pd.DataFrame(data_head)\\n\\n# Use this if you want the full dataset\\n# data = load_dataset(\"MongoDB/cosmopedia-wikihow-chunked\", split=\"train\")\\n# df = pd.DataFrame(data)Step 4: Data analysisNow that we have our dataset, let’s perform some simple data analysis and run some sanity checks on our data to ensure that we don’t see any obvious errors:Code Snippet# Ensuring length of dataset is what we expect i.e. 25k\\nlen(df)\\n\\n# Previewing the contents of the data\\ndf.head()\\n\\n# Only keep records where the text field is not null\\ndf = df[df[\"text\"].notna()]\\n\\n# Number of unique documents in the dataset\\ndf.doc_id.nunique()Step 5: Create embeddingsNow, let’s create embedding functions for each of our models.For voyage-lite-02-instruct:Code Snippetdef get_embeddings(docs: List[str], input_type: str, model:str=\"voyage-lite-02-instruct\") -> List[List[float]]:\\n    \"\"\"\\n    Get embeddings using the Voyage AI API.\\n\\n    Args:\\n        docs (List[str]): List of texts to embed\\n        input_type (str): Type of input to embed. Can be \"document\" or \"query\".\\n        model (str, optional): Model name. Defaults to \"voyage-lite-02-instruct\".\\n\\n    Returns:\\n        List[List[float]]: Array of embedddings\\n    \"\"\"\\n    response = voyage_client.embed(docs, model=model, input_type=input_type)\\n    return response.embeddingsThe embedding function above takes a list of texts (docs) and an input_type as arguments and returns a list of embeddings. The input_type can be document or query depending on whether we are embedding a list of documents or user queries. Voyage uses this value to prepend the inputs with special prompts to enhance retrieval quality.For text-embedding-3-large:Code Snippetdef get_embeddings(docs: List[str], model: str=\"text-embedding-3-large\") -> List[List[float]]:\\n    \"\"\"\\n    Generate embeddings using the OpenAI API.\\n\\n    Args:\\n        docs (List[str]): List of texts to embed\\n        model (str, optional): Model name. Defaults to \"text-embedding-3-large\".\\n\\n    Returns:\\n        List[float]: Array of embeddings\\n    \"\"\"\\n    # replace newlines, which can negatively affect performance.\\n    docs = [doc.replace(\"\\\\n\", \" \") for doc in docs]\\n    response = openai_client.embeddings.create(input=docs, model=model)\\n    response = [r.embedding for r in response.data]\\n    return responseThe embedding function for the OpenAI model is similar to the previous one, with some key differences — there is no input_type argument, and the API returns a list of embedding objects, which need to be parsed to get the final list of embeddings. A sample response from the API looks as follows:Code Snippet{\\n  \"data\": [\\n    {\\n      \"embedding\": [\\n        0.018429679796099663,\\n        -0.009457024745643139\\n    .\\n    .\\n    .\\n      ],\\n      \"index\": 0,\\n      \"object\": \"embedding\"\\n    }\\n  ],\\n  \"model\": \"text-embedding-3-large\",\\n  \"object\": \"list\",\\n  \"usage\": {\\n    \"prompt_tokens\": 183,\\n    \"total_tokens\": 183\\n  }\\n}For UAE-large-V1:Code Snippetfrom typing import List\\nfrom transformers import AutoModel, AutoTokenizer\\nimport torch\\n\\n# Instruction to append to user queries, to improve retrieval\\nRETRIEVAL_INSTRUCT = \"Represent this sentence for searching relevant passages:\"\\n\\n# Check if CUDA (GPU support) is available, and set the device accordingly\\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\\n# Load the UAE-Large-V1 model from the Hugging Face \\nmodel = AutoModel.from_pretrained(\\'WhereIsAI/UAE-Large-V1\\').to(device)\\n# Load the tokenizer associated with the UAE-Large-V1 model\\ntokenizer = AutoTokenizer.from_pretrained(\\'WhereIsAI/UAE-Large-V1\\')\\n\\n# Decorator to disable gradient calculations\\n@torch.no_grad()\\ndef get_embeddings(docs: List[str], input_type: str) -> List[List[float]]:\\n    \"\"\"\\n    Get embeddings using the UAE-Large-V1 model.\\n\\n    Args:\\n        docs (List[str]): List of texts to embed\\n        input_type (str): Type of input to embed. Can be \"document\" or \"query\".\\n\\n    Returns:\\n        List[List[float]]: Array of embedddings\\n    \"\"\"\\n    # Prepend retrieval instruction to queries\\n    if input_type == \"query\":\\n        docs = [\"{}{}\".format(RETRIEVAL_INSTRUCT, q) for q in docs]\\n    # Tokenize input texts\\n    inputs = tokenizer(docs, padding=True, truncation=True, return_tensors=\\'pt\\', max_length=512).to(device)\\n    # Pass tokenized inputs to the model, and obtain the last hidden state\\n    last_hidden_state = model(**inputs, return_dict=True).last_hidden_state\\n    # Extract embeddings from the last hidden state\\n    embeddings = last_hidden_state[:, 0]\\n    return embeddings.cpu().numpy()The UAE-Large-V1 model is an open-source model available on Hugging Face Model Hub. First, we will need to download the model and its tokenizer from Hugging Face. We do this using the Auto classes — namely, AutoModel and AutoTokenizer from the Transformers library  — which automatically infers the underlying model architecture, in this case, BERT. Next, we load the model onto the GPU using .to(device) since we have one available.The embedding function for the UAE model, much like the Voyage model, takes a list of texts (docs) and an input_type as arguments and returns a list of embeddings. A special prompt is prepended to queries for better retrieval as well.The input texts are first tokenized, which includes padding (for short sequences) and truncation (for long sequences)  as needed to ensure that the length of inputs to the model is consistent — 512, in this case, defined by the max_length parameter. The pt value for return_tensors indicates that the output of tokenization should be PyTorch tensors.The tokenized texts are then passed to the model for inference and the last hidden layer (last_hidden_state) is extracted. This layer is the model’s final learned representation of the entire input sequence. The final embedding, however, is extracted only from the first token, which is often a special token ([CLS] in BERT) in transformer-based models. This token serves as an aggregate representation of the entire sequence due to the self-attention mechanism in transformers, where the representation of each token in a sequence is influenced by all other tokens. Finally, we move the embeddings back to CPU using .cpu() and convert the PyTorch tensors to numpy arrays using .numpy().Step 6: EvaluationAs mentioned previously, we will evaluate the models based on embedding latency and retrieval quality.Measuring embedding latencyTo measure embedding latency, we will create a local vector store, which is essentially a list of embeddings for the entire dataset. Latency here is defined as the time it takes to create embeddings for the full dataset.Code Snippetfrom tqdm.auto import tqdm\\n\\n# Get all the texts in the dataset\\ntexts = df[\"text\"].tolist()\\n\\n# Number of samples in a single batch\\nbatch_size = 128\\n\\nembeddings = []\\n# Generate embeddings in batches\\nfor i in tqdm(range(0, len(texts), batch_size)):\\n    end = min(len(texts), i+batch_size)\\n    batch = texts[i:end]\\n    # Generate embeddings for current batch\\n    batch_embeddings = get_embeddings(batch)\\n    # Add to the list of embeddings\\n    embeddings.extend(batch_embeddings)We first create a list of all the texts we want to embed and set the batch size. The voyage-lite-02-instruct model has a batch size limit of 128, so we use the same for all models, for consistency. We iterate through the list of texts, grabbing batch_size number of samples in each iteration, getting embeddings for the batch, and adding them to our \"vector store\".The time taken to generate embeddings on our hardware looked as follows:ModelBatch SizeDimensionsTimetext-embedding-3-large12830724m 17svoyage-lite-02-instruct128102411m 14sUAE-large-V1128102419m 50sThe OpenAI model has the lowest latency. However, note that it also has three times the number of embedding dimensions compared to the other two models. OpenAI also charges by tokens used, so both the storage and inference costs of this model can add up over time. While the UAE model is the slowest of the lot (despite running inference on a GPU), there is room for optimizations such as quantization, distillation, etc., since it is open-source.Measuring retrieval qualityTo evaluate retrieval quality, we use a set of questions based on themes seen in our dataset. For real applications, however, you will want to curate a set of \"cannot-miss\" questions — i.e. questions that you would typically expect users to ask from your data. For this tutorial, we will qualitatively evaluate the relevance of retrieved documents as a measure of quality, but we will explore metrics and techniques for quantitative evaluations in a following tutorial.Here are the main themes (generated using ChatGPT) covered by the top three documents retrieved by each model for our queries:😐 denotes documents that we felt weren’t as relevant to the question. Sentences that contributed to this verdict have been highlighted in bold.Query: Give me some tips to improve my mental health.voyage-lite-02-instructtext-embedding-3-largeUAE-large-V1😐 Regularly reassess treatment efficacy and modify plans as needed. Track mood, thoughts, and behaviors; share updates with therapists and support network. Use a multifaceted approach to manage suicidal thoughts, involving resources, skills, and connections.Eat balanced, exercise, sleep well. Cultivate relationships, engage socially, set boundaries. Manage stress with effective coping mechanisms.Prioritizing mental health is essential, not selfish. Practice mindfulness through meditation, journaling, and activities like yoga. Adopt healthy habits for better mood, less anxiety, and improved cognition.Recognize early signs of stress, share concerns, and develop coping mechanisms. Combat isolation by nurturing relationships and engaging in social activities. Set boundaries, communicate openly, and seek professional help for social anxiety.Prioritizing mental health is essential, not selfish. Practice mindfulness through meditation, journaling, and activities like yoga. Adopt healthy habits for better mood, less anxiety, and improved cognition.Eat balanced, exercise regularly, get 7-9 hours of sleep. Cultivate positive relationships, nurture friendships, and seek new social opportunities. Manage stress with effective coping mechanisms.Prioritizing mental health is essential, not selfish. Practice mindfulness through meditation, journaling, and activities like yoga. Adopt healthy habits for better mood, less anxiety, and improved cognition.Acknowledging feelings is a step to address them. Engage in self-care activities to boost mood and health. Make self-care consistent for lasting benefits.😐 Taking care of your mental health is crucial for a fulfilling life, productivity, and strong relationships. Recognize the importance of mental health in all aspects of life. Managing mental health reduces the risk of severe psychological conditions.While the results cover similar themes, the Voyage AI model keys in heavily on seeking professional help, while the UAE model covers slightly more about why taking care of your mental health is important. The OpenAI model is the one that consistently retrieves documents that cover general tips for improving mental health.Query: Give me some tips for writing good code.voyage-lite-02-instructtext-embedding-3-largeUAE-large-V1Strive for clean, maintainable code with consistent conventions and version control. Utilize linters, static analyzers, and document work for quality and collaboration. Embrace best practices like SOLID and TDD to enhance design, scalability, and extensibility.Strive for clean, maintainable code with consistent conventions and version control. Utilize linters, static analyzers, and document work for quality and collaboration. Embrace best practices like SOLID and TDD to enhance design, scalability, and extensibility.Strive for clean, maintainable code with consistent conventions and version control. Utilize linters, static analyzers, and document work for quality and collaboration. Embrace best practices like SOLID and TDD to enhance design, scalability, and extensibility.😐 Code and test core gameplay mechanics like combat and quest systems; debug and refine for stability. Use modular coding, version control, and object-oriented principles for effective game development. Playtest frequently to find and fix bugs, seek feedback, and prioritize significant improvements.😐 Good programming needs dedication, persistence, and patience. Master core concepts, practice diligently, and engage with peers for improvement. Every expert was once a beginner—keep pushing forward.Read programming books for comprehensive coverage and deep insights, choosing beginner-friendly texts with pathways to proficiency. Combine reading with coding to reinforce learning; take notes on critical points and unfamiliar terms. Engage with exercises and challenges in books to apply concepts and enhance skills.😐 Monitor social media and newsletters for current software testing insights. Participate in networks and forums to exchange knowledge with experienced testers. Regularly update your testing tools and methods for enhanced efficiency.Apply learning by working on real projects, starting small and progressing to larger ones. Participate in open-source projects or develop your applications to enhance problem-solving. Master debugging with IDEs, print statements, and understanding common errors for productivity.😐 Programming is key in various industries, offering diverse opportunities. This guide covers programming fundamentals, best practices, and improvement strategies. Choose a programming language based on interests, goals, and resources.All the models seem to struggle a bit with this question. They all retrieve at least one document that is not as relevant to the question. However, it is interesting to note that all the models retrieve the same document as their number one.Query: What are some environment-friendly practices I can incorporate in everyday life?voyage-lite-02-instructtext-embedding-3-largeUAE-large-V1😐 Conserve resources by reducing waste, reusing, and recycling, reflecting Jawa culture\\'s values due to their planet\\'s limited resources. Monitor consumption (e.g., water, electricity), repair goods, and join local environmental efforts. Eco-friendly practices enhance personal and global well-being, aligning with Jawa values.Carry reusable bags for shopping, keeping extras in your car or bag. Choose sustainable alternatives like reusable water bottles and eco-friendly cutlery. Support businesses that minimize packaging and use biodegradable materials.Educate others on eco-friendly practices; lead by example. Host workshops or discussion groups on sustainable living.Embody respect for the planet; every effort counts towards improvement.Learn and follow local recycling rules, rinse containers, and educate others on proper recycling. Opt for green transportation like walking, cycling, or electric vehicles, and check for incentives. Upgrade to energy-efficient options like LED lights, seal drafts, and consider renewable energy sources.Opt for sustainable transportation, energy-efficient appliances, solar panels, and eat less meat to reduce emissions. Conserve water by fixing leaks, taking shorter showers, and using low-flow fixtures. Water conservation protects ecosystems, ensures food security, and reduces infrastructure stress.Carry reusable bags for shopping, keeping extras in your car or bag. Choose sustainable alternatives like reusable water bottles and eco-friendly cutlery. Support businesses that minimize packaging and use biodegradable materials.😐 Consistently implement these steps. Actively contribute to a cleaner, greener world. Support resilience for future generations.Conserve water with low-flow fixtures, fix leaks, and use rainwater for gardening. Compost kitchen scraps to reduce waste and enrich soil, avoid meat and dairy. Shop locally at farmers markets and CSAs to lower emissions and support local economies.Join local tree-planting events and volunteer at community gardens or restoration projects. Integrate native plants into landscaping to support pollinators and remove invasive species. Adopt eco-friendly transportation methods to decrease fossil fuel consumption.We see a similar trend with this query as with the previous two examples — the OpenAI model consistently retrieves documents that provide the most actionable tips, followed by the UAE model. The Voyage model provides more high-level advice.Overall, based on our preliminary evaluation, OpenAI’s text-embedding-3-large model comes out on top. When working with real-world systems, however, a more rigorous evaluation of a larger dataset is recommended. Also, operational costs become an important consideration. More on evaluation coming in Part 2 of this series!ConclusionIn this tutorial, we looked into how to choose the right model to embed data for RAG. The MTEB leaderboard is a good place to start, especially for text embedding models, but evaluating them on your data is important to find the best one for your RAG application. Storage and inference costs, embedding latency, and retrieval quality are all important parameters to consider while evaluating embedding models. The best model is typically one that offers the best trade-off across these dimensions.Now that you have a good understanding of embedding models, here are some resources to get started with building RAG applications using MongoDB:Using Latest OpenAI Embeddings in a RAG System With MongoDBBuilding a RAG System With Google’s Gemma, Hugging Face, and MongoDBHow to Build a RAG System With LlamaIndex, OpenAI, and MongoDBFollow along with these by creating a free MongoDB Atlas cluster and reach out to us in our Generative AI community forums if you have any questions.Top Comments in ForumsThere are no comments on this article yet.Start the ConversationRate this tutorialRelatedQuickstartGetting Started with Deno & MongoDB May 22, 2024 | 12 min readTutorialBeyond Basics: Enhancing Kotlin Ktor API With Vector Search Mar 25, 2024 | 9 min readTutorialNairobi Stock Exchange Web Scraper Apr 02, 2024 | 20 min readTutorialBuilding AI Applications with Microsoft Semantic Kernel and MongoDB Atlas Vector Search Nov 27, 2023 | 8 min readRequest a TutorialTable of ContentsWhat are embeddings and embedding models?What is RAG (briefly)Choosing the right embedding model for your RAG applicationStep 1: Install the required librariesStep 2: Setup pre-requisitesStep 3: Download the evaluation datasetStep 4: Data analysisStep 5: Create embeddingsStep 6: EvaluationConclusionEnglishEnglishPortuguêsEspañol한국어日本語ItalianoDeutschFrançais简体中文© 2024 MongoDB, Inc.AboutCareersInvestor RelationsLegal NoticesPrivacy NoticesSecurity InformationTrust CenterSupportContact UsCustomer PortalAtlas StatusCustomer SupportSocialGitHubStack OverflowLinkedInYouTubeXTwitchFacebook© 2024 MongoDB, Inc.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview the `page_content` attribute of the Document object\n",
    "docs[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'https://www.mongodb.com/developer/products/atlas/choose-embedding-model-rag/',\n",
       " 'title': 'How to Choose the Right Embedding Model for Your LLM Application | MongoDB',\n",
       " 'description': 'In this tutorial, we will see why embeddings are important for RAG, and how to choose the right embedding model for your RAG application.',\n",
       " 'language': 'en'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview the metadata attribute of the Document object\n",
    "docs[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunk up the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import Language, RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "📚 https://python.langchain.com/v0.1/docs/modules/data_connection/document_transformers/split_by_token/#tiktoken\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the `RecursiveCharacterTextSplitter` text splitter with the `cl100k_base` encoding\n",
    "# For text data, you typically want to keep 1-2 paragraphs (~200 tokens) in a single chunk\n",
    "# Chunk overlap of 15-20% of the chunk size is recommended\n",
    "text_splitter = <CODE_BLOCK_3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🦹‍♀️ Try out a different chunking strategy\n",
    "\n",
    "📚 https://python.langchain.com/v0.1/docs/modules/data_connection/document_transformers/code_splitter/#python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the `from_language` method of the `RecursiveCharacterTextSplitter` text splitter\n",
    "# NOTE:\n",
    "    # You are not merging characters into tokens here so the `chunk_size` and `chunk_overlap` should be in terms of characters\n",
    "    # Assume 1 token = ~4 characters and set the values accordingly\n",
    "text_splitter = <CODE_BLOCK_4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "📚 https://api.python.langchain.com/en/latest/character/langchain_text_splitters.character.RecursiveCharacterTextSplitter.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split `docs` using the appropriate method of the `RecursiveCharacterTextSplitter` class\n",
    "# NOTE: `docs` is a list of LangChain documents\n",
    "split_docs = <CODE_BLOCK_5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "240"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that the length of the list of chunked documents is greater than the length of `docs`\n",
    "len(split_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a Python list comprehension to convert each LangChain Document object in `split_docs` to a Python dictionary.\n",
    "# Use the `.dict()` method on each Document object.\n",
    "split_docs = [<CODE_BLOCK_6>]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': None,\n",
       " 'metadata': {'source': 'https://www.mongodb.com/developer/products/atlas/choose-embedding-model-rag/',\n",
       "  'title': 'How to Choose the Right Embedding Model for Your LLM Application | MongoDB',\n",
       "  'description': 'In this tutorial, we will see why embeddings are important for RAG, and how to choose the right embedding model for your RAG application.',\n",
       "  'language': 'en'},\n",
       " 'page_content': 'How to Choose the Right Embedding Model for Your LLM Application | MongoDBBlogAtlas Vector Search voted most loved vector database in 2024 Retool State of AI reportLearn more\\xa0>>Developer Articles & TopicsGeneral InformationDocumentationDeveloper Articles & TopicsCommunity ForumsBlogUniversityProductsPlatformAtlasBuild on a developer data platformPlatform ServicesDatabaseDeploy a multi-cloud databaseSearchDeliver engaging search experiencesVector SearchDesign intelligent apps with GenAIStream ProcessingUnify data in motion and data at restToolsCompassWork with MongoDB data in a GUIIntegrationsIntegrations with third-party servicesRelational MigratorMigrate to MongoDB with confidenceSelf ManagedEnterprise AdvancedRun and manage MongoDB yourselfCommunity EditionDevelop locally with',\n",
       " 'type': 'Document'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview one of the items in split_docs- ensure that it is a Python dictionary\n",
    "split_docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/apoorva.joshi/Documents/ai-rag-lab-notebooks/.venv/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "📚 https://huggingface.co/mixedbread-ai/mxbai-embed-large-v1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Mixedbread AI's `mxbai-embed-large-v1` model using the Sentence Transformers library\n",
    "embedding_model = <CODE_BLOCK_7>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "📚 https://huggingface.co/mixedbread-ai/mxbai-embed-large-v1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function that takes a piece of text (`text`) as input, embeds it using the `embedding_model` instantiated above and returns the embedding as a list\n",
    "# NOTE: An array can be converted to a list using the `tolist()` method\n",
    "def get_embedding(text: str) -> List[float]:\n",
    "    \"\"\"\n",
    "    Generate the embedding for a piece of text.\n",
    "\n",
    "    Args:\n",
    "        text (str): Text to embed.\n",
    "\n",
    "    Returns:\n",
    "        List[float]: Embedding of the text as a list.\n",
    "    \"\"\"\n",
    "    <CODE_BLOCK_8>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_docs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code to add an `embedding` field to each dictionary in `split_docs`\n",
    "# The `embedding` field should correspond to the embedding of the value of the `page_content` field\n",
    "# Use the `get_embedding` function defined above to generate the embedding\n",
    "# NOTE: Append the updated dictionaries to `embedded_docs` initialized above.\n",
    "<CODE_BLOCK_9>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Perform Semantic Search on Your Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "from typing import Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingest documents into MongoDB\n",
    "\n",
    "📚 https://pymongo.readthedocs.io/en/stable/api/pymongo/mongo_client.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a MongoDB Python client\n",
    "mongo_client = <CODE_BLOCK_10>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of the database -- Change if needed or leave as is\n",
    "DB_NAME = \"mongodb_rag_lab\"\n",
    "# Name of the collection -- Change if needed or leave as is\n",
    "COLLECTION_NAME = \"knowledge_base\"\n",
    "# Name of the vector search index -- Change if needed or leave as is\n",
    "ATLAS_VECTOR_SEARCH_INDEX_NAME = \"vector_index\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "📚 https://pymongo.readthedocs.io/en/stable/tutorial.html#getting-a-collection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the collection defined above using the MongoDB client\n",
    "collection = <CODE_BLOCK_11>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "📚 https://pymongo.readthedocs.io/en/stable/api/pymongo/collection.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeleteResult({'n': 202, 'electionId': ObjectId('7fffffff000000000000000c'), 'opTime': {'ts': Timestamp(1720633181, 201), 't': 12}, 'ok': 1.0, '$clusterTime': {'clusterTime': Timestamp(1720633181, 202), 'signature': {'hash': b'\\xc3:,\\xd8XY\\xf4=\\x84?]@\\xd2\\xd2}\\x14Y\\xf3\\xe9E', 'keyId': 7353010953081847814}}, 'operationTime': Timestamp(1720633181, 201)}, acknowledged=True)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bulk delete all existing records from the collection defined above -- should be a one-liner\n",
    "<CODE_BLOCK_12>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "📚 https://pymongo.readthedocs.io/en/stable/api/pymongo/collection.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data ingestion into MongoDB completed\n"
     ]
    }
   ],
   "source": [
    "# Bulk insert `embedded_docs` into the collection defined above -- should be a one-liner\n",
    "<CODE_BLOCK_13>\n",
    "\n",
    "print(\"Data ingestion into MongoDB completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a vector search index\n",
    "\n",
    "Follow the instructions in the documentation to create a Vector Search index in the Atlas UI.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a vector search function\n",
    "\n",
    "📚 https://www.mongodb.com/docs/atlas/atlas-vector-search/vector-search-stage/#fields\n",
    "\n",
    "📚 https://www.mongodb.com/docs/atlas/atlas-vector-search/vector-search-stage/#ann-examples (Refer to the \"Basic Example\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to retrieve relevant documents for a user query using vector search\n",
    "def vector_search(user_query: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Retrieve relevant documents for a user query using vector search.\n",
    "\n",
    "    Args:\n",
    "    user_query (str): The user's query string.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of matching documents.\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate embedding for the `user_query` using the `get_embedding` function defined in Step 2\n",
    "    <CODE_BLOCK_14>\n",
    "\n",
    "    # Define an aggregation pipeline consisting of a $vectorSearch stage, followed by a $project stage\n",
    "    # Set the number of candidates to 150 and only return the top 5 documents from the vector search\n",
    "    # In the $project stage, exclude the `_id` field and include only the `page_content` field and `vectorSearchScore`\n",
    "    # NOTE: Use variables defined previously for the `index`, `queryVector` and `path` fields in the $vectorSearch stage\n",
    "    pipeline = <CODE_BLOCK_15>\n",
    "\n",
    "    # Execute the aggregation `pipeline`` and store the results in `results`\n",
    "    results = <CODE_BLOCK_16>\n",
    "    return list(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run vector search queries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page_content': 'on choosing an embedding model also applies to multimodal models.Benchmarks are a good place to begin but bear in mind that these results are self-reported and have been benchmarked on datasets that might not accurately represent the data you are dealing with. It is also possible that some models may include the MTEB datasets in their training data since they are publicly available. So even if you choose a model based on benchmark results, we recommend evaluating it on your dataset. We will see how to do this later in the tutorial, but first, let’s take a closer look at the leaderboard.Here’s a snapshot of the top 10 models on the leaderboard currently:Let’s look at the Overall tab since it provides a comprehensive summary of each model. However, note that we have sorted the leaderboard',\n",
       "  'score': 0.8976883888244629},\n",
       " {'page_content': 'comes out on top. When working with real-world systems, however, a more rigorous evaluation of a larger dataset is recommended. Also, operational costs become an important consideration. More on evaluation coming in Part 2 of this series!ConclusionIn this tutorial, we looked into how to choose the right model to embed data for RAG. The MTEB leaderboard is a good place to start, especially for text embedding models, but evaluating them on your data is important to find the best one for your RAG application. Storage and inference costs, embedding latency, and retrieval quality are all important parameters to consider while evaluating embedding models. The best model is typically one that offers the best trade-off across these dimensions.Now that you have a good understanding of embedding',\n",
       "  'score': 0.884284496307373},\n",
       " {'page_content': 'ExamplesPodcastsMongoDB TVcloseAtlasplusSign in to follow topicsArticlesCode ExamplesDocumentationexternalEventsNews & AnnouncementsPodcastsQuickstartsTutorialsVideosMongoDB Developer Centerchevron-rightDeveloper Topicschevron-rightProductschevron-rightAtlaschevron-rightTutorialsHow to Choose the Right Chunking Strategy for Your LLM ApplicationApoorva Joshi15 min read • Published Jun 17, 2024 • Updated Jun 17, 2024AIPythonAtlasRate this tutorialIn Part 1 of this series on Retrieval Augmented Generation (RAG), we looked into choosing the right embedding model for your RAG application. While the choice of embedding model is an important consideration to ensure good quality retrieval for RAG, there is one key decision to be made before the embedding stage that can have a significant',\n",
       "  'score': 0.8746219873428345},\n",
       " {'page_content': 'more dimensions can capture nuanced details and relationships in the data. Ultimately, we want a good trade-off between capturing the complexity of data and operational efficiency.The top 10 models on the leaderboard contain a mix of small vs large and proprietary vs open-source models. Let’s compare some of these to find the best embedding model for our dataset.Before we beginHere are some things to note about our evaluation experiment.DatasetMongoDB’s cosmopedia-wikihow-chunked dataset is available on Hugging Face, which consists of prechunked WikiHow-style articles.Models evaluatedvoyage-lite-02-instruct: A proprietary embedding model from VoyageAItext-embedding-3-large: One of OpenAI’s latest proprietary embedding modelsUAE-Large-V1: A small-ish (335M parameters) open-source embedding',\n",
       "  'score': 0.8736425638198853},\n",
       " {'page_content': 'ApplicationPart 2: How to Evaluate Your LLM ApplicationPart 3: How to Choose the Right Chunking Strategy for Your LLM ApplicationPart 4: Improving RAG using metadata extraction and filteringWhat are embeddings and embedding models?An embedding is an array of numbers (a vector) representing a piece of information, such as text, images, audio, video, etc. Together, these numbers capture semantics and other important features of the data. The immediate consequence of doing this is that semantically similar entities map close to each other while dissimilar entities map farther apart in the vector space. For clarity, see the image below for a depiction of a high-dimensional vector space:In the context of natural language processing (NLP), embedding models are algorithms designed to learn and',\n",
       "  'score': 0.8727899789810181}]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_search(\n",
    "    \"What are the important considerations while choosing an embedding model?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page_content': 'of 10 samples for demonstration so the above results cannot be treated as fully conclusive. In reality, you will want to use a larger, more representative evaluation dataset.ConclusionIn this tutorial, we learned how to choose the right chunking strategy for RAG. Breaking up large documents into smaller chunks for RAG results in fewer tokens passed as input to LLMs, and a more targeted context for LLMs to work with. Chunking strategies are composed of three key components — splitting technique, chunk size, and chunk overlap. Picking the right strategy involves experimenting with different combinations of the three components.Now that you have a good understanding of chunking for RAG, take it up as a challenge to evaluate different chunking strategies on datasets that were used in some of',\n",
       "  'score': 0.9246349930763245},\n",
       " {'page_content': 'quality retrieval for RAG, there is one key decision to be made before the embedding stage that can have a significant downstream impact — choosing the right chunking strategy for your data.In this tutorial, we will cover the following:What is chunking and why is it important for RAG?Choosing the right chunking strategy for your RAG applicationEvaluating different chunking methodologies on a datasetRAG — a very quick refresherIn a RAG application, the goal is to get more accurate responses from a large language model (LLM) on subjects that might not be well-represented in its parametric knowledge — for example, your organization’s data. This is typically done by retrieving relevant information from a knowledge base using semantic search. This technique uses an embedding model to create',\n",
       "  'score': 0.9117333889007568},\n",
       " {'page_content': 'helps.Chunking is the process of breaking down large pieces of text into smaller segments or chunks. In the context of RAG, embedding smaller chunks instead of entire documents to create the knowledge base means that given a user query, you only have to retrieve the most relevant document chunks, resulting in fewer input tokens and more targeted context for the LLM to work with.Choosing the right chunking strategy for your RAG applicationThere is no “one size fits all” solution when it comes to choosing a chunking strategy for RAG — it depends on the structure of the documents being used to create the knowledge base and will look different depending on whether you are working with well-formatted text documents or documents with code snippets, tables, images, etc. The three key components',\n",
       "  'score': 0.9106854200363159},\n",
       " {'page_content': 'with well-formatted text documents or documents with code snippets, tables, images, etc. The three key components of a chunking strategy are as follows:Splitting technique: Determines where the chunk boundaries will be placed — based on paragraph boundaries, programming language-specific separators, tokens, or even semantic boundariesChunk size: The maximum number of characters or tokens allowed for each chunkChunk overlap: Number of overlapping characters or tokens between chunks; overlapping chunks can help preserve cross-chunk context; the degree of overlap is typically specified as a percentage of the chunk sizeAs with choosing embedding or completion models for RAG, we recommend picking a few different options for each component; evaluating them on a small, manually curated dataset',\n",
       "  'score': 0.8990657329559326},\n",
       " {'page_content': 'MyFordBenefitsReplySee More on ForumsRate this tutorialRelatedTutorialBeyond Basics: Enhancing Kotlin Ktor API With Vector Search Mar 25, 2024 | 9 min readCode ExampleBlogue Jul 07, 2022 | 1 min readIndustry EventlocationMEDELLÍN, ANTIOQUIA, COLOMBIA | IN-PERSONMongoDB Developer Day Bancolombia 24 Jul 15, 2024 | 2:00 PM - 10:00 PM UTCTutorialWrite A Serverless Function with AWS Lambda and MongoDB Sep 23, 2022 | 15 min readRequest a TutorialTable of ContentsRAG — a very quick refresherWhat is chunking and why is it important for RAG?Choosing the right chunking strategy for your RAG applicationChunking strategiesStep 1: Install required librariesStep 2: Set up pre-requisitesStep 3: Load the datasetStep 4: Define chunking functionsStep 5: Generate the evaluation datasetStep 6: Evaluate',\n",
       "  'score': 0.883987307548523}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_search(\"How to choose a chunking strategy for RAG?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🦹‍♀️ Combine pre-filtering with vector search\n",
    "\n",
    "📚 https://www.mongodb.com/docs/atlas/atlas-vector-search/vector-search-type/#about-the-filter-type\n",
    "\n",
    "📚 https://www.mongodb.com/docs/atlas/atlas-vector-search/vector-search-stage/#ann-examples (Refer to the \"Filter Example\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter for documents where the language is `en`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the vector search index definition to include the `metadata.language` field as a `filter` field\n",
    "<CODE_BLOCK_17>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the $vectorSearch stage of the aggregation pipeline defined previously to include a filter for documents where the `metadata.language` field has the value `en`\n",
    "<CODE_BLOCK_18>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter on documents where the language is `en` and type is `Document`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the vector search index definition to include the `metadata.language` and `type` fields as `filter` fields\n",
    "<CODE_BLOCK_19>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the $vectorSearch stage of the aggregation pipeline defined previously to include a filter for documents where\n",
    "# the `metadata.language` field has the value `en`\n",
    "# AND\n",
    "# the `type` field has the value `Document`\n",
    "<CODE_BLOCK_20>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Build a RAG Application\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate a chat model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fireworks.client import Fireworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the Fireworks AI client and the model string\n",
    "fw_client = Fireworks()\n",
    "model = \"accounts/fireworks/models/llama-v3-8b-instruct\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function to create the chat prompt\n",
    "\n",
    "📚 https://docs.python.org/3/library/stdtypes.html#str.join\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to create the user prompt for our RAG application\n",
    "def create_prompt(user_query: str) -> str:\n",
    "    \"\"\"\n",
    "    Create a chat prompt that includes the user query and retrieved context.\n",
    "\n",
    "    Args:\n",
    "        user_query (str): The user's query string.\n",
    "\n",
    "    Returns:\n",
    "        str: The chat prompt string.\n",
    "    \"\"\"\n",
    "    # Retrieve the most relevant documents for the `user_query` using the `vector_search` function\n",
    "    context = <CODE_BLOCK_21>\n",
    "    # Join the retrieved documents into a single string, where each document is separated by two new lines (\"\\n\\n\")\n",
    "    # NOTE: Extract only the `page_content` field from the documents\n",
    "    context = <CODE_BLOCK_22>\n",
    "    # Prompt consisting of the question and relevant context to answer it\n",
    "    prompt = f\"Answer the question based only on the following context. If the context is empty, say I DON'T KNOW\\n\\nContext:\\n{context}\\n\\nQuestion:{user_query}\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function to answer user queries\n",
    "\n",
    "📚 https://docs.fireworks.ai/guides/querying-text-models#chat-completions-api\n",
    "\n",
    "📚 https://docs.fireworks.ai/api-reference/post-completions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to answer user queries using Fireworks' Chat Completion API\n",
    "def generate_answer(user_query: str) -> None:\n",
    "    \"\"\"\n",
    "    Generate an answer to the user query.\n",
    "\n",
    "    Args:\n",
    "        user_query (str): The user's query string.\n",
    "    \"\"\"\n",
    "    # Use the `create_prompt` function above to fill in the `content` field in the chat message\n",
    "    # Set the `temperature` parameter to 0 to get more deterministic responses\n",
    "    # Print the final answer\n",
    "    <CODE_BLOCK_23>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query the RAG application\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_answer(\n",
    "    \"What are the important considerations while choosing an embedding model?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_answer(\"What did I just ask you?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🦹‍♀️ Return streaming responses\n",
    "\n",
    "📚 https://docs.fireworks.ai/guides/querying-text-models#streaming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to answer user queries in streaming mode using Fireworks' Chat Completion API \n",
    "def generate_answer(user_query: str) -> None:\n",
    "    \"\"\"\n",
    "    Generate an answer to the user query.\n",
    "\n",
    "    Args:\n",
    "        user_query (str): The user's query string.\n",
    "    \"\"\"\n",
    "    # Use the `create_prompt` function above to fill in the `content` field in the chat message\n",
    "    # Set the `temperature` parameter to 0 to get more deterministic responses\n",
    "    # Set the `stream` parameter to True\n",
    "    response = <CODE_BLOCK_24>\n",
    "\n",
    "    # Iterate through the `response` generator and print the results as they are generated\n",
    "    <CODE_BLOCK_25>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the context, the important considerations while choosing an embedding model are:\n",
      "\n",
      "1. Benchmark results: The MTEB leaderboard is a good place to start, but it's essential to evaluate the model on your dataset.\n",
      "2. Storage and inference costs: These costs become an important consideration when working with real-world systems.\n",
      "3. Embedding latency: This is an important parameter to consider while evaluating embedding models.\n",
      "4. Retrieval quality: This is another important parameter to consider while evaluating embedding models.\n",
      "5. Trade-off: The best model is typically one that offers the best trade-off across these dimensions."
     ]
    }
   ],
   "source": [
    "generate_answer(\n",
    "    \"What are the important considerations while choosing an embedding model?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Add memory to the RAG Application\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'session_id_1'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_collection = mongo_client[DB_NAME][\"chat_history\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "📚 https://pymongo.readthedocs.io/en/stable/api/pymongo/collection.html#pymongo.collection.Collection.create_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an index on the key `session_id` for the `history_collection` collection\n",
    "<CODE_BLOCK_26>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function to store chat messages in MongoDB\n",
    "\n",
    "📚 https://docs.python.org/3/library/datetime.html#datetime.datetime.nowhttps://docs.python.org/3/library/datetime.html#datetime.datetime.now\n",
    "\n",
    "📚 https://pymongo.readthedocs.io/en/stable/api/pymongo/collection.html#pymongo.collection.Collection.insert_one\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_chat_message(session_id: str, role: str, content: str) -> None:\n",
    "    \"\"\"\n",
    "    Store a chat message in a MongoDB collection.\n",
    "\n",
    "    Args:\n",
    "        session_id (str): Session ID of the message.\n",
    "        role (str): Role for the message. One of `system`, `user` or `assistant`.\n",
    "        content (str): Content of the message.\n",
    "    \"\"\"\n",
    "    # Create a message object with `session_id`, `role`, `content` and `timestamp` fields\n",
    "    # `timestamp` should be set the current timestamp\n",
    "    message = <CODE_BLOCK_27>\n",
    "    # Insert the `message` into the `history_collection` collection\n",
    "    <CODE_BLOCK_28>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function to retrieve chat history from MongoDB\n",
    "\n",
    "📚 https://pymongo.readthedocs.io/en/stable/api/pymongo/collection.html#pymongo.collection.Collection.find\n",
    "\n",
    "📚 https://pymongo.readthedocs.io/en/stable/api/pymongo/cursor.html#pymongo.cursor.Cursor.sort\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_session_history(session_id: str) -> List:\n",
    "    \"\"\"\n",
    "    Retrieve chat message history for a particular session.\n",
    "\n",
    "    Args:\n",
    "        session_id (str): Session ID to retrieve chat message history for.\n",
    "\n",
    "    Returns:\n",
    "        List: List of chat messages.\n",
    "    \"\"\"\n",
    "    # Query the `history_collection` collection for documents where the \"session_id\" field has the value of the input `session_id`\n",
    "    # Sort the results in increasing order of the values in `timestamp` field\n",
    "    cursor =  <CODE_BLOCK_29>\n",
    "\n",
    "    if cursor:\n",
    "        # Write a list comprehension to iterate through the cursor and extract the `role` and `content` field from each entry\n",
    "        # Then format each entry as: {\"role\": <role_value>, \"content\": <content_value>}\n",
    "        messages = [<CODE_BLOCK_30>]\n",
    "    else:\n",
    "        # If cursor is empty, return an empty list\n",
    "        messages = []\n",
    "\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle chat history in the `generate_answer` function\n",
    "\n",
    "📚 https://docs.python.org/3/tutorial/datastructures.html\n",
    "\n",
    "📚 https://docs.fireworks.ai/guides/querying-text-models#chat-completions-api\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(session_id: str, user_query: str) -> None:\n",
    "    \"\"\"\n",
    "    Generate an answer to the user's query taking chat history into account.\n",
    "\n",
    "    Args:\n",
    "        session_id (str): Session ID to retrieve chat history for.\n",
    "        user_query (str): The user's query string.\n",
    "    \"\"\"\n",
    "    # Initialize list of messages to pass to the chat completion model\n",
    "    messages = []\n",
    "\n",
    "    # Retrieve documents relevant to the user query and convert them to a single string\n",
    "    context = vector_search(user_query)\n",
    "    context = \"\\n\\n\".join([d.get(\"page_content\", \"\") for d in context])\n",
    "    # Create a system prompt containing the retrieved context\n",
    "    system_message = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": f\"Answer the question based only on the following context. If the context is empty, say I DON'T KNOW\\n\\nContext:\\n{context}\",\n",
    "    }\n",
    "    # Append the system prompt to the `messages` list\n",
    "    messages.append(system_message)\n",
    "\n",
    "    # Use the `retrieve_session_history` function to retrieve message history from MongoDB for the session ID `session_id` \n",
    "    # And add all messages in the message history to the `messages` list \n",
    "    <CODE_BLOCK_31>\n",
    "\n",
    "    # Format the user message in the format {\"role\": <role_value>, \"content\": <content_value>}\n",
    "    # The role value for user messages must be \"user\"\n",
    "    # And append the user message to the `messages` list\n",
    "    <CODE_BLOCK_32>\n",
    "\n",
    "    # Call the chat completions API \n",
    "    response = fw_client.chat.completions.create(model=model, messages=messages)\n",
    "\n",
    "    # Extract the answer from the API response\n",
    "    answer = response.choices[0].message.content\n",
    "\n",
    "    # Use the `store_chat_message` function to store the user message and also the generated answer in the message history collection\n",
    "    # The role value for user messages is \"user\", and \"assistant\" for the generated answer\n",
    "    <CODE_BLOCK_33>\n",
    "\n",
    "    print answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_answer(\n",
    "    session_id=\"1\",\n",
    "    user_query=\"What are the important considerations while choosing an embedding model?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_answer(\n",
    "    session_id=\"1\",\n",
    "    user_query=\"What did I just ask you?\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
