{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mongodb-developer/ai-agents-lab-notebooks/blob/main/notebook_template.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Lab Documentation and Solutions](https://img.shields.io/badge/Lab%20Documentation%20and%20Solutions-purple)](https://mongodb-developer.github.io/rag-lab/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Install libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -qU pymongo langchain langchain-community langchain-mongodb bs4 tiktoken sentence_transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Setup prerequisites\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "MONGODB_URI = \"<CODE_BLOCK_1>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Create a knowledge base\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset using LangChain WebBaseLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\n",
    "    [\n",
    "        \"https://www.mongodb.com/developer/products/atlas/choose-embedding-model-rag/\",\n",
    "        \"https://www.mongodb.com/developer/products/atlas/evaluate-llm-applications-rag/\",\n",
    "        \"https://www.mongodb.com/developer/products/atlas/choosing-chunking-strategy-rag/\",\n",
    "        \"https://www.mongodb.com/developer/products/atlas/gemma-mongodb-huggingface-rag/\",\n",
    "    ]\n",
    ")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How to Choose the Right Embedding Model for Your LLM Application | MongoDBBlogAtlas Vector Search voted most loved vector database in 2024 Retool State of AI reportLearn more\\xa0>>Developer Articles & TopicsGeneral InformationDocumentationDeveloper Articles & TopicsCommunity ForumsBlogUniversityProductsPlatformAtlasBuild on a developer data platformPlatform ServicesDatabaseDeploy a multi-cloud databaseSearchDeliver engaging search experiencesVector SearchDesign intelligent apps with GenAIStream ProcessingUnify data in motion and data at restToolsCompassWork with MongoDB data in a GUIIntegrationsIntegrations with third-party servicesRelational MigratorMigrate to MongoDB with confidenceSelf ManagedEnterprise AdvancedRun and manage MongoDB yourselfCommunity EditionDevelop locally with MongoDBBuild with MongoDB AtlasGet started for free in minutesSign UpTest Enterprise AdvancedDevelop with MongoDB on-premisesDownloadTry Community EditionExplore the latest version of MongoDBDownloadResourcesDocumentationAtlas DocumentationGet started using AtlasServer DocumentationLearn to use MongoDBStart With GuidesGet step-by-step guidance for key tasks Tools and ConnectorsLearn how to connect to MongoDBMongoDB DriversUse drivers and libraries for MongoDBAI Resources HubGet help building the next big thing in AI with MongoDBarrow-rightConnectDeveloper CenterExplore a wide range of developer resourcesCommunityJoin a global community of developersCourses and CertificationLearn for free from MongoDBWebinars and EventsFind a webinar or event near youSolutionsUse casesArtificial IntelligenceEdge ComputingInternet of ThingsMobilePaymentsServerless DevelopmentIndustriesFinancial ServicesTelecommunicationsHealthcareRetailPublic SectorManufacturingSolutions LibraryOrganized and tailored solutions to kick-start projectsarrow-rightDeveloper Data PlatformAccelerate innovation at scaleLearn morearrow-rightStartups and AI InnovatorsFor world-changing ideas and AI pioneersLearn morearrow-rightCustomer Case StudiesHear directly from our usersSee Storiesarrow-rightCompanyCareersStart your next adventureBlogRead articles and announcementsNewsroomRead press releases and news storiesPartnersLearn about our partner ecosystemLeadershipMeet our executive teamCompanyLearn more about who we areContact UsReach out to MongoDBLet’s chatarrow-rightInvestorsVisit our investor portalLearn morearrow-rightPricingSupportSign InTry Freemenu-verticalMongoDB Developerchevron-downTopicsLanguagesplusTechnologiesplusProductsplusExpertise LevelsplusAll TopicsDocumentationArticlesTutorialsEventsCode ExamplesPodcastsMongoDB TVMongoDB DeveloperTopicschevron-downDocumentationArticlesTutorialsEventsCode ExamplesPodcastsMongoDB TVcloseAtlasplusSign in to follow topicsArticlesCode ExamplesDocumentationexternalEventsNews & AnnouncementsPodcastsQuickstartsTutorialsVideosMongoDB Developer Centerchevron-rightDeveloper Topicschevron-rightProductschevron-rightAtlaschevron-rightTutorialsHow to Choose the Right Embedding Model for Your LLM ApplicationApoorva Joshi16 min read • Published Jun 17, 2024 • Updated Jun 17, 2024AIPythonAtlasRate this tutorialIf you are building Generative AI (GenAI) applications in 2024, you’ve probably heard the term “embeddings” a few times by now and are seeing new embedding models hit the shelf every week. So why do so many people suddenly care about embeddings, a concept that has existed since the 1950s? And if embeddings are so important and you must use them, how do you choose among the vast number of options out there?This tutorial will cover the following:What are embeddings?Importance of embeddings in RAG applicationsHow to choose the right embedding model for your RAG applicationEvaluating embedding modelsThis tutorial is Part 1 of a multi-part series on Retrieval Augmented Generation (RAG), where we start with the fundamentals of building a RAG application, and work our way to more advanced techniques for RAG. The series will cover the following:Part 1: How to Choose the Right Embedding Model for Your LLM ApplicationPart 2: How to Evaluate Your LLM ApplicationPart 3: How to Choose the Right Chunking Strategy for Your LLM ApplicationPart 4: Improving RAG using metadata extraction and filteringWhat are embeddings and embedding models?An embedding is an array of numbers (a vector) representing a piece of information, such as text, images, audio, video, etc. Together, these numbers capture semantics and other important features of the data. The immediate consequence of doing this is that semantically similar entities map close to each other while dissimilar entities map farther apart in the vector space. For clarity, see the image below for a depiction of a high-dimensional vector space:In the context of natural language processing (NLP), embedding models are algorithms designed to learn and generate embeddings for a given piece of information. In today’s AI applications, embeddings are typically created using large language models (LLMs) that are trained on a massive corpus of data and use cutting-edge algorithms to learn complex semantic relationships in the data.What is RAG (briefly)Retrieval Augmented Generation, as the name suggests, aims to improve the quality of pre-trained LLM generation using data retrieved from a knowledge base. The success of RAG lies in retrieving the most relevant results from the knowledge base. This is where embeddings come into the picture. A RAG pipeline looks something like this:In the above pipeline, we see a common approach used for retrieval in GenAI applications — i.e., semantic search. In this technique, an embedding model is used to create vector representations of the user query and of information in the knowledge base. This way, given a user query and its embedding, we can retrieve the most relevant source documents from the knowledge base based on how similar their embeddings are to the query embedding. The retrieved documents, user query, and any user prompts are then passed as context to an LLM, to generate an answer to the user’s question.Choosing the right embedding model for your RAG applicationAs we have seen above, embeddings are central to RAG. But with so many models out there, how do we choose the best one for our use case?A good place to start when looking for embedding models to use is the MTEB Leaderboard on Hugging Face. It is the most up-to-date list of proprietary and open-source text embedding models, accompanied by statistics on how each model performs on various embedding tasks such as retrieval, summarization, etc.Evaluations of this magnitude for multimodal models are just emerging (see the MME benchmark) so we will only focus on text embedding models for this tutorial. However, all the guidance here on choosing an embedding model also applies to multimodal models.Benchmarks are a good place to begin but bear in mind that these results are self-reported and have been benchmarked on datasets that might not accurately represent the data you are dealing with. It is also possible that some models may include the MTEB datasets in their training data since they are publicly available. So even if you choose a model based on benchmark results, we recommend evaluating it on your dataset. We will see how to do this later in the tutorial, but first, let’s take a closer look at the leaderboard.Here’s a snapshot of the top 10 models on the leaderboard currently:Let’s look at the Overall tab since it provides a comprehensive summary of each model. However, note that we have sorted the leaderboard by the Retrieval Average column. This is because RAG is a retrieval\\xa0task and we want to see the best retrieval models at the top. We will ignore columns corresponding to other tasks, and focus on the following columns:Retrieval Average: Represents average Normalized Discounted Cumulative Gain (NDCG) @ 10 across several datasets. NDCG is a common metric to measure the performance of retrieval systems. A higher NDCG indicates a model that is better at ranking relevant items higher in the list of retrieved results.\\xa0Model Size: Size of the model (in GB). It gives an idea of the computational resources required to run the model. While retrieval performance scales with model size, it is important to note that model size also has a direct impact on latency. The latency-performance trade-off becomes especially important in a production setup.\\xa0\\xa0Max Tokens: Number of tokens that can be compressed into a single embedding. You typically don’t want to put more than a single paragraph of text (~100 tokens) into a single embedding. So even models with max tokens of 512 should be more than enough.Embedding Dimensions: Length of the embedding vector. Smaller embeddings offer faster inference and are more storage-efficient, while more dimensions can capture nuanced details and relationships in the data. Ultimately, we want a good trade-off between capturing the complexity of data and operational efficiency.The top 10 models on the leaderboard contain a mix of small vs large and proprietary vs open-source models. Let’s compare some of these to find the best embedding model for our dataset.Before we beginHere are some things to note about our evaluation experiment.DatasetMongoDB’s cosmopedia-wikihow-chunked dataset is available on Hugging Face, which consists of prechunked WikiHow-style articles.Models evaluatedvoyage-lite-02-instruct: A proprietary embedding model from VoyageAItext-embedding-3-large: One of OpenAI’s latest proprietary embedding modelsUAE-Large-V1: A small-ish (335M parameters) open-source embedding modelWe also attempted to evaluate SFR-Embedding-Mistral, currently the #1 model on the MTEB leaderboard, but the hardware below was not sufficient to run this model. This model and other 14+ GB models on the leaderboard will likely require a/multiple GPU(s) with at least 32 GB of total memory, which means higher costs and/or getting into distributed inference. While we haven’t evaluated this model in our experiment, this is already a good data point when thinking about cost and resources.Evaluation metricsWe used the following metrics to evaluate embedding performance:Embedding latency: Time taken to create embeddingsRetrieval quality: Relevance of retrieved documents to the user queryHardware used1 NVIDIA T4 GPU, 16GB MemoryWhere’s the code?Evaluation notebooks for each of the above models are available:voyage-lite-02-instructtext-embedding-3-largeUAE-Large-V1To run a notebook, click on the Open in Colab shield at the top of the notebook. The notebook will open in Google Colaboratory.Click the Connect button on the top right corner to connect to a hosted runtime environment.Once connected, you can also change the runtime type to use the T4 GPUs available for free on Google Colab.Step 1: Install the required librariesThe libraries required for each model differ slightly, but the common ones are as follows:datasets: Python library to get access to datasets available on Hugging Face Hubsentence-transformers: Framework for working with text and image embeddingsnumpy: Python library that provides tools to perform mathematical operations on arrayspandas: Python library for data analysis, exploration, and manipulationtdqm: Python module to show a progress meter for loopsCode Snippet! pip install -qU datasets sentence-transformers numpy pandas tqdmAdditionally for Voyage AI:\\nvoyageai: Python library to interact with OpenAI APIsCode Snippet! pip install -qU voyageaiAdditionally for OpenAI:\\nopenai: Python library to interact with OpenAI APIsCode Snippet! pip install -qU openaiAdditionally for UAE:\\ntransformers: Python library that provides APIs to interact with pre-trained models available on Hugging FaceCode Snippet! pip install -qU transformersStep 2: Setup pre-requisitesOpenAI and Voyage AI models are available via APIs. So you’ll need to obtain API keys and make them available to the respective clients.Code Snippetimport os\\nimport getpassInitialize Voyage AI client:Code Snippetimport voyageai\\nVOYAGE_API_KEY = getpass.getpass(\"Voyage API Key:\")\\nvoyage_client = voyageai.Client(api_key=VOYAGE_API_KEY)Initialize OpenAI client:Code Snippetfrom openai import OpenAI\\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\\nopenai_client = OpenAI()Step 3: Download the evaluation datasetAs mentioned previously, we will use MongoDB’s cosmopedia-wikihow-chunked dataset. The dataset is quite large (1M+ documents). So we will stream it and grab the first 25k records, instead of downloading the entire dataset to disk.Code Snippetfrom datasets import load_dataset\\nimport pandas as pd\\n\\n# Use streaming=True to load the dataset without downloading it fully\\ndata = load_dataset(\"MongoDB/cosmopedia-wikihow-chunked\", split=\"train\", streaming=True)\\n# Get first 25k records from the dataset\\ndata_head = data.take(25000)\\ndf = pd.DataFrame(data_head)\\n\\n# Use this if you want the full dataset\\n# data = load_dataset(\"MongoDB/cosmopedia-wikihow-chunked\", split=\"train\")\\n# df = pd.DataFrame(data)Step 4: Data analysisNow that we have our dataset, let’s perform some simple data analysis and run some sanity checks on our data to ensure that we don’t see any obvious errors:Code Snippet# Ensuring length of dataset is what we expect i.e. 25k\\nlen(df)\\n\\n# Previewing the contents of the data\\ndf.head()\\n\\n# Only keep records where the text field is not null\\ndf = df[df[\"text\"].notna()]\\n\\n# Number of unique documents in the dataset\\ndf.doc_id.nunique()Step 5: Create embeddingsNow, let’s create embedding functions for each of our models.For voyage-lite-02-instruct:Code Snippetdef get_embeddings(docs: List[str], input_type: str, model:str=\"voyage-lite-02-instruct\") -> List[List[float]]:\\n    \"\"\"\\n    Get embeddings using the Voyage AI API.\\n\\n    Args:\\n        docs (List[str]): List of texts to embed\\n        input_type (str): Type of input to embed. Can be \"document\" or \"query\".\\n        model (str, optional): Model name. Defaults to \"voyage-lite-02-instruct\".\\n\\n    Returns:\\n        List[List[float]]: Array of embedddings\\n    \"\"\"\\n    response = voyage_client.embed(docs, model=model, input_type=input_type)\\n    return response.embeddingsThe embedding function above takes a list of texts (docs) and an input_type as arguments and returns a list of embeddings. The input_type can be document or query depending on whether we are embedding a list of documents or user queries. Voyage uses this value to prepend the inputs with special prompts to enhance retrieval quality.For text-embedding-3-large:Code Snippetdef get_embeddings(docs: List[str], model: str=\"text-embedding-3-large\") -> List[List[float]]:\\n    \"\"\"\\n    Generate embeddings using the OpenAI API.\\n\\n    Args:\\n        docs (List[str]): List of texts to embed\\n        model (str, optional): Model name. Defaults to \"text-embedding-3-large\".\\n\\n    Returns:\\n        List[float]: Array of embeddings\\n    \"\"\"\\n    # replace newlines, which can negatively affect performance.\\n    docs = [doc.replace(\"\\\\n\", \" \") for doc in docs]\\n    response = openai_client.embeddings.create(input=docs, model=model)\\n    response = [r.embedding for r in response.data]\\n    return responseThe embedding function for the OpenAI model is similar to the previous one, with some key differences — there is no input_type argument, and the API returns a list of embedding objects, which need to be parsed to get the final list of embeddings. A sample response from the API looks as follows:Code Snippet{\\n  \"data\": [\\n    {\\n      \"embedding\": [\\n        0.018429679796099663,\\n        -0.009457024745643139\\n    .\\n    .\\n    .\\n      ],\\n      \"index\": 0,\\n      \"object\": \"embedding\"\\n    }\\n  ],\\n  \"model\": \"text-embedding-3-large\",\\n  \"object\": \"list\",\\n  \"usage\": {\\n    \"prompt_tokens\": 183,\\n    \"total_tokens\": 183\\n  }\\n}For UAE-large-V1:Code Snippetfrom typing import List\\nfrom transformers import AutoModel, AutoTokenizer\\nimport torch\\n\\n# Instruction to append to user queries, to improve retrieval\\nRETRIEVAL_INSTRUCT = \"Represent this sentence for searching relevant passages:\"\\n\\n# Check if CUDA (GPU support) is available, and set the device accordingly\\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\\n# Load the UAE-Large-V1 model from the Hugging Face \\nmodel = AutoModel.from_pretrained(\\'WhereIsAI/UAE-Large-V1\\').to(device)\\n# Load the tokenizer associated with the UAE-Large-V1 model\\ntokenizer = AutoTokenizer.from_pretrained(\\'WhereIsAI/UAE-Large-V1\\')\\n\\n# Decorator to disable gradient calculations\\n@torch.no_grad()\\ndef get_embeddings(docs: List[str], input_type: str) -> List[List[float]]:\\n    \"\"\"\\n    Get embeddings using the UAE-Large-V1 model.\\n\\n    Args:\\n        docs (List[str]): List of texts to embed\\n        input_type (str): Type of input to embed. Can be \"document\" or \"query\".\\n\\n    Returns:\\n        List[List[float]]: Array of embedddings\\n    \"\"\"\\n    # Prepend retrieval instruction to queries\\n    if input_type == \"query\":\\n        docs = [\"{}{}\".format(RETRIEVAL_INSTRUCT, q) for q in docs]\\n    # Tokenize input texts\\n    inputs = tokenizer(docs, padding=True, truncation=True, return_tensors=\\'pt\\', max_length=512).to(device)\\n    # Pass tokenized inputs to the model, and obtain the last hidden state\\n    last_hidden_state = model(**inputs, return_dict=True).last_hidden_state\\n    # Extract embeddings from the last hidden state\\n    embeddings = last_hidden_state[:, 0]\\n    return embeddings.cpu().numpy()The UAE-Large-V1 model is an open-source model available on Hugging Face Model Hub. First, we will need to download the model and its tokenizer from Hugging Face. We do this using the Auto classes — namely, AutoModel and AutoTokenizer from the Transformers library  — which automatically infers the underlying model architecture, in this case, BERT. Next, we load the model onto the GPU using .to(device) since we have one available.The embedding function for the UAE model, much like the Voyage model, takes a list of texts (docs) and an input_type as arguments and returns a list of embeddings. A special prompt is prepended to queries for better retrieval as well.The input texts are first tokenized, which includes padding (for short sequences) and truncation (for long sequences)  as needed to ensure that the length of inputs to the model is consistent — 512, in this case, defined by the max_length parameter. The pt value for return_tensors indicates that the output of tokenization should be PyTorch tensors.The tokenized texts are then passed to the model for inference and the last hidden layer (last_hidden_state) is extracted. This layer is the model’s final learned representation of the entire input sequence. The final embedding, however, is extracted only from the first token, which is often a special token ([CLS] in BERT) in transformer-based models. This token serves as an aggregate representation of the entire sequence due to the self-attention mechanism in transformers, where the representation of each token in a sequence is influenced by all other tokens. Finally, we move the embeddings back to CPU using .cpu() and convert the PyTorch tensors to numpy arrays using .numpy().Step 6: EvaluationAs mentioned previously, we will evaluate the models based on embedding latency and retrieval quality.Measuring embedding latencyTo measure embedding latency, we will create a local vector store, which is essentially a list of embeddings for the entire dataset. Latency here is defined as the time it takes to create embeddings for the full dataset.Code Snippetfrom tqdm.auto import tqdm\\n\\n# Get all the texts in the dataset\\ntexts = df[\"text\"].tolist()\\n\\n# Number of samples in a single batch\\nbatch_size = 128\\n\\nembeddings = []\\n# Generate embeddings in batches\\nfor i in tqdm(range(0, len(texts), batch_size)):\\n    end = min(len(texts), i+batch_size)\\n    batch = texts[i:end]\\n    # Generate embeddings for current batch\\n    batch_embeddings = get_embeddings(batch)\\n    # Add to the list of embeddings\\n    embeddings.extend(batch_embeddings)We first create a list of all the texts we want to embed and set the batch size. The voyage-lite-02-instruct model has a batch size limit of 128, so we use the same for all models, for consistency. We iterate through the list of texts, grabbing batch_size number of samples in each iteration, getting embeddings for the batch, and adding them to our \"vector store\".The time taken to generate embeddings on our hardware looked as follows:ModelBatch SizeDimensionsTimetext-embedding-3-large12830724m 17svoyage-lite-02-instruct128102411m 14sUAE-large-V1128102419m 50sThe OpenAI model has the lowest latency. However, note that it also has three times the number of embedding dimensions compared to the other two models. OpenAI also charges by tokens used, so both the storage and inference costs of this model can add up over time. While the UAE model is the slowest of the lot (despite running inference on a GPU), there is room for optimizations such as quantization, distillation, etc., since it is open-source.Measuring retrieval qualityTo evaluate retrieval quality, we use a set of questions based on themes seen in our dataset. For real applications, however, you will want to curate a set of \"cannot-miss\" questions — i.e. questions that you would typically expect users to ask from your data. For this tutorial, we will qualitatively evaluate the relevance of retrieved documents as a measure of quality, but we will explore metrics and techniques for quantitative evaluations in a following tutorial.Here are the main themes (generated using ChatGPT) covered by the top three documents retrieved by each model for our queries:😐 denotes documents that we felt weren’t as relevant to the question. Sentences that contributed to this verdict have been highlighted in bold.Query: Give me some tips to improve my mental health.voyage-lite-02-instructtext-embedding-3-largeUAE-large-V1😐 Regularly reassess treatment efficacy and modify plans as needed. Track mood, thoughts, and behaviors; share updates with therapists and support network. Use a multifaceted approach to manage suicidal thoughts, involving resources, skills, and connections.Eat balanced, exercise, sleep well. Cultivate relationships, engage socially, set boundaries. Manage stress with effective coping mechanisms.Prioritizing mental health is essential, not selfish. Practice mindfulness through meditation, journaling, and activities like yoga. Adopt healthy habits for better mood, less anxiety, and improved cognition.Recognize early signs of stress, share concerns, and develop coping mechanisms. Combat isolation by nurturing relationships and engaging in social activities. Set boundaries, communicate openly, and seek professional help for social anxiety.Prioritizing mental health is essential, not selfish. Practice mindfulness through meditation, journaling, and activities like yoga. Adopt healthy habits for better mood, less anxiety, and improved cognition.Eat balanced, exercise regularly, get 7-9 hours of sleep. Cultivate positive relationships, nurture friendships, and seek new social opportunities. Manage stress with effective coping mechanisms.Prioritizing mental health is essential, not selfish. Practice mindfulness through meditation, journaling, and activities like yoga. Adopt healthy habits for better mood, less anxiety, and improved cognition.Acknowledging feelings is a step to address them. Engage in self-care activities to boost mood and health. Make self-care consistent for lasting benefits.😐 Taking care of your mental health is crucial for a fulfilling life, productivity, and strong relationships. Recognize the importance of mental health in all aspects of life. Managing mental health reduces the risk of severe psychological conditions.While the results cover similar themes, the Voyage AI model keys in heavily on seeking professional help, while the UAE model covers slightly more about why taking care of your mental health is important. The OpenAI model is the one that consistently retrieves documents that cover general tips for improving mental health.Query: Give me some tips for writing good code.voyage-lite-02-instructtext-embedding-3-largeUAE-large-V1Strive for clean, maintainable code with consistent conventions and version control. Utilize linters, static analyzers, and document work for quality and collaboration. Embrace best practices like SOLID and TDD to enhance design, scalability, and extensibility.Strive for clean, maintainable code with consistent conventions and version control. Utilize linters, static analyzers, and document work for quality and collaboration. Embrace best practices like SOLID and TDD to enhance design, scalability, and extensibility.Strive for clean, maintainable code with consistent conventions and version control. Utilize linters, static analyzers, and document work for quality and collaboration. Embrace best practices like SOLID and TDD to enhance design, scalability, and extensibility.😐 Code and test core gameplay mechanics like combat and quest systems; debug and refine for stability. Use modular coding, version control, and object-oriented principles for effective game development. Playtest frequently to find and fix bugs, seek feedback, and prioritize significant improvements.😐 Good programming needs dedication, persistence, and patience. Master core concepts, practice diligently, and engage with peers for improvement. Every expert was once a beginner—keep pushing forward.Read programming books for comprehensive coverage and deep insights, choosing beginner-friendly texts with pathways to proficiency. Combine reading with coding to reinforce learning; take notes on critical points and unfamiliar terms. Engage with exercises and challenges in books to apply concepts and enhance skills.😐 Monitor social media and newsletters for current software testing insights. Participate in networks and forums to exchange knowledge with experienced testers. Regularly update your testing tools and methods for enhanced efficiency.Apply learning by working on real projects, starting small and progressing to larger ones. Participate in open-source projects or develop your applications to enhance problem-solving. Master debugging with IDEs, print statements, and understanding common errors for productivity.😐 Programming is key in various industries, offering diverse opportunities. This guide covers programming fundamentals, best practices, and improvement strategies. Choose a programming language based on interests, goals, and resources.All the models seem to struggle a bit with this question. They all retrieve at least one document that is not as relevant to the question. However, it is interesting to note that all the models retrieve the same document as their number one.Query: What are some environment-friendly practices I can incorporate in everyday life?voyage-lite-02-instructtext-embedding-3-largeUAE-large-V1😐 Conserve resources by reducing waste, reusing, and recycling, reflecting Jawa culture\\'s values due to their planet\\'s limited resources. Monitor consumption (e.g., water, electricity), repair goods, and join local environmental efforts. Eco-friendly practices enhance personal and global well-being, aligning with Jawa values.Carry reusable bags for shopping, keeping extras in your car or bag. Choose sustainable alternatives like reusable water bottles and eco-friendly cutlery. Support businesses that minimize packaging and use biodegradable materials.Educate others on eco-friendly practices; lead by example. Host workshops or discussion groups on sustainable living.Embody respect for the planet; every effort counts towards improvement.Learn and follow local recycling rules, rinse containers, and educate others on proper recycling. Opt for green transportation like walking, cycling, or electric vehicles, and check for incentives. Upgrade to energy-efficient options like LED lights, seal drafts, and consider renewable energy sources.Opt for sustainable transportation, energy-efficient appliances, solar panels, and eat less meat to reduce emissions. Conserve water by fixing leaks, taking shorter showers, and using low-flow fixtures. Water conservation protects ecosystems, ensures food security, and reduces infrastructure stress.Carry reusable bags for shopping, keeping extras in your car or bag. Choose sustainable alternatives like reusable water bottles and eco-friendly cutlery. Support businesses that minimize packaging and use biodegradable materials.😐 Consistently implement these steps. Actively contribute to a cleaner, greener world. Support resilience for future generations.Conserve water with low-flow fixtures, fix leaks, and use rainwater for gardening. Compost kitchen scraps to reduce waste and enrich soil, avoid meat and dairy. Shop locally at farmers markets and CSAs to lower emissions and support local economies.Join local tree-planting events and volunteer at community gardens or restoration projects. Integrate native plants into landscaping to support pollinators and remove invasive species. Adopt eco-friendly transportation methods to decrease fossil fuel consumption.We see a similar trend with this query as with the previous two examples — the OpenAI model consistently retrieves documents that provide the most actionable tips, followed by the UAE model. The Voyage model provides more high-level advice.Overall, based on our preliminary evaluation, OpenAI’s text-embedding-3-large model comes out on top. When working with real-world systems, however, a more rigorous evaluation of a larger dataset is recommended. Also, operational costs become an important consideration. More on evaluation coming in Part 2 of this series!ConclusionIn this tutorial, we looked into how to choose the right model to embed data for RAG. The MTEB leaderboard is a good place to start, especially for text embedding models, but evaluating them on your data is important to find the best one for your RAG application. Storage and inference costs, embedding latency, and retrieval quality are all important parameters to consider while evaluating embedding models. The best model is typically one that offers the best trade-off across these dimensions.Now that you have a good understanding of embedding models, here are some resources to get started with building RAG applications using MongoDB:Using Latest OpenAI Embeddings in a RAG System With MongoDBBuilding a RAG System With Google’s Gemma, Hugging Face, and MongoDBHow to Build a RAG System With LlamaIndex, OpenAI, and MongoDBFollow along with these by creating a free MongoDB Atlas cluster and reach out to us in our Generative AI community forums if you have any questions.Top Comments in ForumsThere are no comments on this article yet.Start the ConversationRate this tutorialRelatedTutorialSearching on Your Location with Atlas Search and Geospatial Operators Feb 03, 2023 | 9 min readCode ExampleGet Started with MongoDB Atlas and AWS CloudFormation Jan 23, 2024 | 3 min readTutorialHow to Evaluate Your LLM Application Jun 24, 2024 | 20 min readTutorialHow to Optimize LLM Applications With Prompt Compression Using LLMLingua and LangChain Jun 18, 2024 | 13 min readRequest a TutorialTable of ContentsWhat are embeddings and embedding models?What is RAG (briefly)Choosing the right embedding model for your RAG applicationStep 1: Install the required librariesStep 2: Setup pre-requisitesStep 3: Download the evaluation datasetStep 4: Data analysisStep 5: Create embeddingsStep 6: EvaluationConclusionEnglishEnglishPortuguêsEspañol한국어日本語ItalianoDeutschFrançais简体中文© 2024 MongoDB, Inc.AboutCareersInvestor RelationsLegal NoticesPrivacy NoticesSecurity InformationTrust CenterSupportContact UsCustomer PortalAtlas StatusCustomer SupportSocialGitHubStack OverflowLinkedInYouTubeXTwitchFacebook© 2024 MongoDB, Inc.'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'https://www.mongodb.com/developer/products/atlas/choose-embedding-model-rag/',\n",
       " 'title': 'How to Choose the Right Embedding Model for Your LLM Application | MongoDB',\n",
       " 'description': 'In this tutorial, we will see why embeddings are important for RAG, and how to choose the right embedding model for your RAG application.',\n",
       " 'language': 'en'}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunk up the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    encoding_name=\"cl100k_base\", chunk_size=200, chunk_overlap=30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_docs = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "202"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(split_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_docs = [doc.dict() for doc in split_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': None,\n",
       " 'metadata': {'source': 'https://www.mongodb.com/developer/products/atlas/choose-embedding-model-rag/',\n",
       "  'title': 'How to Choose the Right Embedding Model for Your LLM Application | MongoDB',\n",
       "  'description': 'In this tutorial, we will see why embeddings are important for RAG, and how to choose the right embedding model for your RAG application.',\n",
       "  'language': 'en'},\n",
       " 'page_content': 'How to Choose the Right Embedding Model for Your LLM Application | MongoDBBlogAtlas Vector Search voted most loved vector database in 2024 Retool State of AI reportLearn more\\xa0>>Developer Articles & TopicsGeneral InformationDocumentationDeveloper Articles & TopicsCommunity ForumsBlogUniversityProductsPlatformAtlasBuild on a developer data platformPlatform ServicesDatabaseDeploy a multi-cloud databaseSearchDeliver engaging search experiencesVector SearchDesign intelligent apps with GenAIStream ProcessingUnify data in motion and data at restToolsCompassWork with MongoDB data in a GUIIntegrationsIntegrations with third-party servicesRelational MigratorMigrate to MongoDB with confidenceSelf ManagedEnterprise AdvancedRun and manage MongoDB yourselfCommunity EditionDevelop locally with MongoDBBuild with MongoDB AtlasGet started for free in minutesSign UpTest Enterprise AdvancedDevelop with MongoDB on-premisesDownloadTry Community EditionExplore the latest version of MongoDBDownloadResourcesDocumentationAtlas DocumentationGet started using AtlasServer DocumentationLearn to use MongoDBStart With GuidesGet step-by-step guidance for key tasks Tools',\n",
       " 'type': 'Document'}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = SentenceTransformer(\"mixedbread-ai/mxbai-embed-large-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text: str):\n",
    "    embedding = embedding_model.encode(text)\n",
    "    return embedding.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_docs = [\n",
    "    {**d, \"embedding\": get_embedding(d[\"page_content\"])} for d in split_docs\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_docs = []\n",
    "for doc in split_docs:\n",
    "    temp = doc.copy()\n",
    "    temp[\"embedding\"] = get_embedding(temp[\"page_content\"])\n",
    "    embedded_docs.append(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingest documents into MongoDB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a MongoDB Python client\n",
    "client = MongoClient(MONGODB_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of the database -- Change if needed or leave as is\n",
    "DB_NAME = \"mongodb_rag_lab\"\n",
    "# Name of the collection -- Change if needed or leave as is\n",
    "COLLECTION_NAME = \"knowledge_base\"\n",
    "# Name of the vector search index -- Change if needed or leave as is\n",
    "ATLAS_VECTOR_SEARCH_INDEX_NAME = \"vector_index\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the collection defined above using the MongoDB client\n",
    "collection = client[DB_NAME][COLLECTION_NAME]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeleteResult({'n': 0, 'electionId': ObjectId('7fffffff000000000000000c'), 'opTime': {'ts': Timestamp(1720477037, 15), 't': 12}, 'ok': 1.0, '$clusterTime': {'clusterTime': Timestamp(1720477037, 15), 'signature': {'hash': b'\\x04\\xff@#\\xf6\\xe7\\x7f\\xcc\\xab|zrR\\x94\\x7f\\xe9,\\x9a\\xe0\\xb6', 'keyId': 7353010953081847814}}, 'operationTime': Timestamp(1720477037, 15)}, acknowledged=True)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bulk delete all existing records from the collection defined above -- should be a one-liner\n",
    "collection.delete_many({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data ingestion into MongoDB completed\n"
     ]
    }
   ],
   "source": [
    "# Bulk insert `records` into the collection defined above -- should be a one-liner\n",
    "collection.insert_many(embedded_docs)\n",
    "\n",
    "print(\"Data ingestion into MongoDB completed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
